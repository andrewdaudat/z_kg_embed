<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Fundamentals of Vector Space Representations – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/translation-based.html" rel="next">
<link href="../content/intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/embedding.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#vector-spaces-definitions-and-properties" id="toc-vector-spaces-definitions-and-properties" class="nav-link active" data-scroll-target="#vector-spaces-definitions-and-properties"><span class="header-section-number">4.1</span> Vector spaces: definitions and properties</a>
  <ul class="collapse">
  <li><a href="#vector-space-properties" id="toc-vector-space-properties" class="nav-link" data-scroll-target="#vector-space-properties"><span class="header-section-number">4.1.1</span> Vector space properties</a></li>
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link" data-scroll-target="#linear-transformations"><span class="header-section-number">4.1.2</span> Linear transformations</a></li>
  </ul></li>
  <li><a href="#complex-vector-spaces" id="toc-complex-vector-spaces" class="nav-link" data-scroll-target="#complex-vector-spaces"><span class="header-section-number">4.2</span> Complex vector spaces</a></li>
  <li><a href="#word-embeddings-a-motivating-example" id="toc-word-embeddings-a-motivating-example" class="nav-link" data-scroll-target="#word-embeddings-a-motivating-example"><span class="header-section-number">4.3</span> Word embeddings: a motivating example</a>
  <ul class="collapse">
  <li><a href="#capturing-semantics-in-vector-space" id="toc-capturing-semantics-in-vector-space" class="nav-link" data-scroll-target="#capturing-semantics-in-vector-space"><span class="header-section-number">4.3.1</span> Capturing semantics in vector space</a></li>
  </ul></li>
  <li><a href="#embedding-entities-and-relations" id="toc-embedding-entities-and-relations" class="nav-link" data-scroll-target="#embedding-entities-and-relations"><span class="header-section-number">4.4</span> Embedding entities and relations</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretations-of-relations" id="toc-geometric-interpretations-of-relations" class="nav-link" data-scroll-target="#geometric-interpretations-of-relations"><span class="header-section-number">4.4.1</span> Geometric interpretations of relations</a></li>
  </ul></li>
  <li><a href="#encoding-relation-patterns" id="toc-encoding-relation-patterns" class="nav-link" data-scroll-target="#encoding-relation-patterns"><span class="header-section-number">4.5</span> Encoding relation patterns</a></li>
  <li><a href="#scoring-functions-and-plausibility" id="toc-scoring-functions-and-plausibility" class="nav-link" data-scroll-target="#scoring-functions-and-plausibility"><span class="header-section-number">4.6</span> Scoring functions and plausibility</a></li>
  <li><a href="#learning-embeddings" id="toc-learning-embeddings" class="nav-link" data-scroll-target="#learning-embeddings"><span class="header-section-number">4.7</span> Learning embeddings</a></li>
  <li><a href="#visualization-and-interpretation" id="toc-visualization-and-interpretation" class="nav-link" data-scroll-target="#visualization-and-interpretation"><span class="header-section-number">4.8</span> Visualization and interpretation</a></li>
  <li><a href="#embedding-spaces-for-knowledge-graphs" id="toc-embedding-spaces-for-knowledge-graphs" class="nav-link" data-scroll-target="#embedding-spaces-for-knowledge-graphs"><span class="header-section-number">4.9</span> Embedding spaces for knowledge graphs</a>
  <ul class="collapse">
  <li><a href="#translational-models-chapter-3" id="toc-translational-models-chapter-3" class="nav-link" data-scroll-target="#translational-models-chapter-3"><span class="header-section-number">4.9.1</span> Translational models (Chapter 3)</a></li>
  <li><a href="#semantic-matching-models-chapter-4" id="toc-semantic-matching-models-chapter-4" class="nav-link" data-scroll-target="#semantic-matching-models-chapter-4"><span class="header-section-number">4.9.2</span> Semantic matching models (Chapter 4)</a></li>
  <li><a href="#complex-and-rotation-based-models-chapter-5" id="toc-complex-and-rotation-based-models-chapter-5" class="nav-link" data-scroll-target="#complex-and-rotation-based-models-chapter-5"><span class="header-section-number">4.9.3</span> Complex and rotation-based models (Chapter 5)</a></li>
  </ul></li>
  <li><a href="#properties-of-good-embeddings" id="toc-properties-of-good-embeddings" class="nav-link" data-scroll-target="#properties-of-good-embeddings"><span class="header-section-number">4.10</span> Properties of good embeddings</a></li>
  <li><a href="#from-word-embeddings-to-knowledge-graph-embeddings" id="toc-from-word-embeddings-to-knowledge-graph-embeddings" class="nav-link" data-scroll-target="#from-word-embeddings-to-knowledge-graph-embeddings"><span class="header-section-number">4.11</span> From word embeddings to knowledge graph embeddings</a></li>
  <li><a href="#embedding-space-dimensions" id="toc-embedding-space-dimensions" class="nav-link" data-scroll-target="#embedding-space-dimensions"><span class="header-section-number">4.12</span> Embedding space dimensions</a></li>
  <li><a href="#manifold-hypothesis-and-embedding-spaces" id="toc-manifold-hypothesis-and-embedding-spaces" class="nav-link" data-scroll-target="#manifold-hypothesis-and-embedding-spaces"><span class="header-section-number">4.13</span> Manifold hypothesis and embedding spaces</a></li>
  <li><a href="#relationship-to-matrix-and-tensor-factorization" id="toc-relationship-to-matrix-and-tensor-factorization" class="nav-link" data-scroll-target="#relationship-to-matrix-and-tensor-factorization"><span class="header-section-number">4.14</span> Relationship to matrix and tensor factorization</a></li>
  <li><a href="#vector-spaces-beyond-euclidean-geometry" id="toc-vector-spaces-beyond-euclidean-geometry" class="nav-link" data-scroll-target="#vector-spaces-beyond-euclidean-geometry"><span class="header-section-number">4.15</span> Vector spaces beyond Euclidean geometry</a></li>
  <li><a href="#understanding-the-embedding-space-through-analogies" id="toc-understanding-the-embedding-space-through-analogies" class="nav-link" data-scroll-target="#understanding-the-embedding-space-through-analogies"><span class="header-section-number">4.16</span> Understanding the embedding space through analogies</a></li>
  <li><a href="#entity-types-and-relation-domains" id="toc-entity-types-and-relation-domains" class="nav-link" data-scroll-target="#entity-types-and-relation-domains"><span class="header-section-number">4.17</span> Entity types and relation domains</a></li>
  <li><a href="#translating-from-symbolic-to-continuous-representations" id="toc-translating-from-symbolic-to-continuous-representations" class="nav-link" data-scroll-target="#translating-from-symbolic-to-continuous-representations"><span class="header-section-number">4.18</span> Translating from symbolic to continuous representations</a>
  <ul class="collapse">
  <li><a href="#benefits-of-continuous-representations" id="toc-benefits-of-continuous-representations" class="nav-link" data-scroll-target="#benefits-of-continuous-representations"><span class="header-section-number">4.18.1</span> Benefits of continuous representations:</a></li>
  <li><a href="#challenges-of-continuous-representations" id="toc-challenges-of-continuous-representations" class="nav-link" data-scroll-target="#challenges-of-continuous-representations"><span class="header-section-number">4.18.2</span> Challenges of continuous representations:</a></li>
  </ul></li>
  <li><a href="#embedding-initialization" id="toc-embedding-initialization" class="nav-link" data-scroll-target="#embedding-initialization"><span class="header-section-number">4.19</span> Embedding initialization</a></li>
  <li><a href="#normalization-and-constraints" id="toc-normalization-and-constraints" class="nav-link" data-scroll-target="#normalization-and-constraints"><span class="header-section-number">4.20</span> Normalization and constraints</a></li>
  <li><a href="#visualization-techniques-for-embeddings" id="toc-visualization-techniques-for-embeddings" class="nav-link" data-scroll-target="#visualization-techniques-for-embeddings"><span class="header-section-number">4.21</span> Visualization techniques for embeddings</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4.22</span> Summary</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">4.23</span> Further reading</a>
  <ul class="collapse">
  <li><a href="#vector-spaces-and-linear-algebra" id="toc-vector-spaces-and-linear-algebra" class="nav-link" data-scroll-target="#vector-spaces-and-linear-algebra"><span class="header-section-number">4.23.1</span> Vector spaces and linear algebra</a></li>
  <li><a href="#embedding-methods" id="toc-embedding-methods" class="nav-link" data-scroll-target="#embedding-methods"><span class="header-section-number">4.23.2</span> Embedding methods</a></li>
  <li><a href="#tensor-factorization-and-knowledge-graph-embeddings" id="toc-tensor-factorization-and-knowledge-graph-embeddings" class="nav-link" data-scroll-target="#tensor-factorization-and-knowledge-graph-embeddings"><span class="header-section-number">4.23.3</span> Tensor factorization and knowledge graph embeddings</a></li>
  <li><a href="#visualization-and-dimensionality-reduction" id="toc-visualization-and-dimensionality-reduction" class="nav-link" data-scroll-target="#visualization-and-dimensionality-reduction"><span class="header-section-number">4.23.4</span> Visualization and dimensionality reduction</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>The concept of representing objects in vector spaces forms the mathematical foundation for knowledge graph embeddings. In this chapter, we’ll explore how abstract entities and relationships can be encoded as points and transformations in geometric spaces. This approach provides computational efficiency, enables similarity-based reasoning, and allows us to harness powerful machine learning techniques for knowledge graph completion.</p>
<p>Vector space representations have revolutionized many fields in artificial intelligence and data science. From word embeddings in natural language processing to user embeddings in recommendation systems, the idea of mapping objects to continuous vector spaces has proven remarkably effective. Knowledge graph embeddings apply this same principle to entities and relations in knowledge graphs.</p>
<p>This chapter builds the mathematical groundwork needed to understand knowledge graph embedding models. We’ll start with basic vector space concepts, explore various embedding approaches, and examine how semantic relationships can be captured geometrically. By the end of this chapter, you’ll understand how vector spaces serve as a powerful framework for representing and reasoning with knowledge.</p>
<section id="vector-spaces-definitions-and-properties" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="vector-spaces-definitions-and-properties"><span class="header-section-number">4.1</span> Vector spaces: definitions and properties</h2>
<p>Let’s begin with the fundamental concept of a vector space:</p>
<div id="def-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1 (Vector space)</strong></span> A <strong>vector space</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> over a field <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math> (typically <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℝ</mi><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℂ</mi><annotation encoding="application/x-tex">\mathbb{C}</annotation></semantics></math>) is a set of elements called vectors, equipped with two operations:</p>
<ol type="1">
<li><strong>Vector addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>+</mi><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">+: V \times V \rightarrow V</annotation></semantics></math></li>
<li><strong>Scalar multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>⋅</mi><mo>:</mo><mi>F</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\cdot: F \times V \rightarrow V</annotation></semantics></math></li>
</ol>
<p>These operations must satisfy several axioms, including associativity and distributivity of addition and multiplication, existence of additive and multiplicative identities, and additive inverses.</p>
</div>
<p>For knowledge graph embeddings, we typically work with real vector spaces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the dimension of the embedding. Each vector has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> real-valued components.</p>
<div id="exm-vector-space" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 (Real vector space)</strong></span> In the vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math>, vectors are triplets of real numbers, such as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v} = (3, -1, 4)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐰</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{w} = (2, 0, 5)</annotation></semantics></math>.</p>
<p>Vector addition: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>+</mo><mi>𝐰</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>9</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v} + \mathbf{w} = (3, -1, 4) + (2, 0, 5) = (5, -1, 9)</annotation></semantics></math> Scalar multiplication: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>⋅</mo><mi>𝐯</mi><mo>=</mo><mn>2</mn><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>6</mn><mo>,</mo><mi>−</mi><mn>2</mn><mo>,</mo><mn>8</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">2 \cdot \mathbf{v} = 2 \cdot (3, -1, 4) = (6, -2, 8)</annotation></semantics></math></p>
</div>
<section id="vector-space-properties" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="vector-space-properties"><span class="header-section-number">4.1.1</span> Vector space properties</h3>
<p>Several properties of vector spaces are particularly relevant for knowledge graph embeddings:</p>
<div id="def-vector-space-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 (Key vector space properties)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Dimensionality</strong>: The number of independent basis vectors needed to span the space, denoted as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>.</li>
<li><strong>Norm</strong>: A function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>⋅</mi><mo stretchy="false" form="postfix">∥</mo><mo>:</mo><mi>V</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\|\cdot\|: V \rightarrow \mathbb{R}</annotation></semantics></math> that assigns a non-negative scalar length to each vector.</li>
<li><strong>Distance</strong>: A function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">d: V \times V \rightarrow \mathbb{R}</annotation></semantics></math> that measures the separation between two vectors.</li>
<li><strong>Inner product</strong>: A function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>⋅</mi><mo>,</mo><mi>⋅</mi><mo stretchy="false" form="postfix">⟩</mo><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>F</mi></mrow><annotation encoding="application/x-tex">\langle \cdot, \cdot \rangle: V \times V \rightarrow F</annotation></semantics></math> that generalizes the dot product.</li>
</ol>
</div>
<section id="norms-and-distances" class="level4">
<h4 class="anchored" data-anchor-id="norms-and-distances">Norms and distances</h4>
<p>Norms measure the “size” or “length” of vectors:</p>
<div id="def-norms" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3 (Common norms)</strong></span> For a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐱</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>d</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x} = (x_1, x_2, \ldots, x_d) \in \mathbb{R}^d</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>L1 norm</strong> (Manhattan norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\|\mathbf{x}\|_1 = \sum_{i=1}^d |x_i|</annotation></semantics></math></li>
<li><strong>L2 norm</strong> (Euclidean norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^d x_i^2}</annotation></semantics></math></li>
<li><strong>L-infinity norm</strong> (Maximum norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>∞</mi></msub><mo>=</mo><msubsup><mo>max</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\|\mathbf{x}\|_\infty = \max_{i=1}^d |x_i|</annotation></semantics></math></li>
</ol>
</div>
<p>Distances measure the separation between vectors:</p>
<div id="def-distances" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4 (Common distance measures)</strong></span> For vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}, \mathbf{y} \in \mathbb{R}^d</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>Euclidean distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">Euclidean</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">d_{\text{Euclidean}}(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}</annotation></semantics></math></li>
<li><strong>Manhattan distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">Manhattan</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">d_{\text{Manhattan}}(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_{i=1}^d |x_i - y_i|</annotation></semantics></math></li>
<li><strong>Cosine distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">cosine</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>𝐱</mi><mo>⋅</mo><mi>𝐲</mi></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo stretchy="false" form="postfix">∥</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">d_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2}</annotation></semantics></math></li>
</ol>
</div>
<div id="exm-distances" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 (Distance calculation example)</strong></span> Consider vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐚</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a} = (3, 4)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐛</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>6</mn><mo>,</mo><mn>8</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{b} = (6, 8)</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^2</annotation></semantics></math>.</p>
<p>Euclidean distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">Euclidean</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>−</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>−</mo><mn>8</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt><mo>=</mo><msqrt><mrow><mn>9</mn><mo>+</mo><mn>16</mn></mrow></msqrt><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d_{\text{Euclidean}}(\mathbf{a}, \mathbf{b}) = \sqrt{(3-6)^2 + (4-8)^2} = \sqrt{9 + 16} = 5</annotation></semantics></math></p>
<p>Manhattan distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">Manhattan</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>3</mn><mo>−</mo><mn>6</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>4</mn><mo>−</mo><mn>8</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>3</mn><mo>+</mo><mn>4</mn><mo>=</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">d_{\text{Manhattan}}(\mathbf{a}, \mathbf{b}) = |3-6| + |4-8| = 3 + 4 = 7</annotation></semantics></math></p>
<p>For cosine distance, we first compute:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐚</mi><mo>⋅</mo><mi>𝐛</mi><mo>=</mo><mn>3</mn><mo>×</mo><mn>6</mn><mo>+</mo><mn>4</mn><mo>×</mo><mn>8</mn><mo>=</mo><mn>18</mn><mo>+</mo><mn>32</mn><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">\mathbf{a} \cdot \mathbf{b} = 3 \times 6 + 4 \times 8 = 18 + 32 = 50</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐚</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msup><mn>3</mn><mn>2</mn></msup><mo>+</mo><msup><mn>4</mn><mn>2</mn></msup></mrow></msqrt><mo>=</mo><msqrt><mrow><mn>9</mn><mo>+</mo><mn>16</mn></mrow></msqrt><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{a}\|_2 = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐛</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msup><mn>6</mn><mn>2</mn></msup><mo>+</mo><msup><mn>8</mn><mn>2</mn></msup></mrow></msqrt><mo>=</mo><msqrt><mrow><mn>36</mn><mo>+</mo><mn>64</mn></mrow></msqrt><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{b}\|_2 = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = 10</annotation></semantics></math></li>
</ul>
<p>Cosine distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">cosine</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mn>50</mn><mrow><mn>5</mn><mo>×</mo><mn>10</mn></mrow></mfrac><mo>=</mo><mn>1</mn><mo>−</mo><mn>1</mn><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d_{\text{cosine}}(\mathbf{a}, \mathbf{b}) = 1 - \frac{50}{5 \times 10} = 1 - 1 = 0</annotation></semantics></math></p>
<p>The cosine distance is 0, indicating that the vectors point in the same direction (they’re collinear), which is indeed the case as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐛</mi><mo>=</mo><mn>2</mn><mi>𝐚</mi></mrow><annotation encoding="application/x-tex">\mathbf{b} = 2\mathbf{a}</annotation></semantics></math>.</p>
</div>
</section>
<section id="inner-products" class="level4">
<h4 class="anchored" data-anchor-id="inner-products">Inner products</h4>
<p>Inner products generalize the notion of dot products and provide a way to measure the alignment between vectors:</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.5 (Inner product)</strong></span> An <strong>inner product</strong> on a vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>⋅</mi><mo>,</mo><mi>⋅</mi><mo stretchy="false" form="postfix">⟩</mo><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>F</mi></mrow><annotation encoding="application/x-tex">\langle \cdot, \cdot \rangle: V \times V \rightarrow F</annotation></semantics></math> that satisfies:</p>
<ol type="1">
<li>Linearity: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>α</mi><mi>𝐱</mi><mo>+</mo><mi>β</mi><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>α</mi><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐳</mi><mo stretchy="false" form="postfix">⟩</mo><mo>+</mo><mi>β</mi><mo stretchy="false" form="prefix">⟨</mo><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle \alpha \mathbf{x} + \beta \mathbf{y}, \mathbf{z} \rangle = \alpha \langle \mathbf{x}, \mathbf{z} \rangle + \beta \langle \mathbf{y}, \mathbf{z} \rangle</annotation></semantics></math></li>
<li>Symmetry: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>𝐲</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle</annotation></semantics></math></li>
<li>Positive definiteness: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">⟩</mo><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\langle \mathbf{x}, \mathbf{x} \rangle \geq 0</annotation></semantics></math>, with equality if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐱</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{x} = \mathbf{0}</annotation></semantics></math></li>
</ol>
<p>For real vector spaces, the standard inner product (dot product) is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>𝐱</mi><mo>⋅</mo><mi>𝐲</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^d x_i y_i</annotation></semantics></math></p>
</div>
<p>Inner products are closely related to norms: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">⟩</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\mathbf{x}\|_2 = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}</annotation></semantics></math></p>
</section>
</section>
<section id="linear-transformations" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="linear-transformations"><span class="header-section-number">4.1.2</span> Linear transformations</h3>
<p>Linear transformations are fundamental for understanding how relations can be modeled in embedding spaces:</p>
<div id="def-linear-transformation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.6 (Linear transformation)</strong></span> A <strong>linear transformation</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>:</mo><mi>V</mi><mo>→</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">T: V \rightarrow W</annotation></semantics></math> between vector spaces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> is a function that preserves vector addition and scalar multiplication:</p>
<ol type="1">
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>+</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(\mathbf{x} + \mathbf{y}) = T(\mathbf{x}) + T(\mathbf{y})</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>α</mi><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(\alpha \mathbf{x}) = \alpha T(\mathbf{x})</annotation></semantics></math></li>
</ol>
<p>For finite-dimensional spaces, a linear transformation can be represented by a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐌</mi><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{x}) = \mathbf{M}\mathbf{x}</annotation></semantics></math>.</p>
</div>
<p>Different types of linear transformations are used in knowledge graph embedding models:</p>
<div id="def-transformation-types" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.7 (Common linear transformations)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Translation</strong>: Addition of a fixed vector: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐱</mi><mo>+</mo><mi>𝐛</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{x}) = \mathbf{x} + \mathbf{b}</annotation></semantics></math></li>
<li><strong>Rotation</strong>: Multiplication by a rotation matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐑</mi><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{x}) = \mathbf{R}\mathbf{x}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐑</mi><mi>T</mi></msup><mi>𝐑</mi><mo>=</mo><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\mathbf{R}^T\mathbf{R} = \mathbf{I}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\det(\mathbf{R}) = 1</annotation></semantics></math></li>
<li><strong>Scaling</strong>: Multiplication by a diagonal matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐃</mi><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{x}) = \mathbf{D}\mathbf{x}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐃</mi><mo>=</mo><mtext mathvariant="normal">diag</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{D} = \text{diag}(d_1, d_2, \ldots, d_n)</annotation></semantics></math></li>
<li><strong>Projection</strong>: Multiplication by a projection matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐏</mi><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{x}) = \mathbf{P}\mathbf{x}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐏</mi><mn>2</mn></msup><mo>=</mo><mi>𝐏</mi></mrow><annotation encoding="application/x-tex">\mathbf{P}^2 = \mathbf{P}</annotation></semantics></math></li>
</ol>
</div>
<div id="exm-transformations" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 (Linear transformation example)</strong></span> Consider a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v} = (3, 2)</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^2</annotation></semantics></math> and the following transformations:</p>
<ol type="1">
<li><p>Translation by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐛</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{b} = (1, -1)</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐯</mi><mo>+</mo><mi>𝐛</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mi>−</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_1(\mathbf{v}) = \mathbf{v} + \mathbf{b} = (3, 2) + (1, -1) = (4, 1)</annotation></semantics></math></p></li>
<li><p>Rotation by 90° counterclockwise, using the rotation matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐑</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{R} = \begin{pmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{pmatrix}</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐑</mi><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_2(\mathbf{v}) = \mathbf{R}\mathbf{v} = \begin{pmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 3 \\ 2 \end{pmatrix} = \begin{pmatrix} -2 \\ 3 \end{pmatrix} = (-2, 3)</annotation></semantics></math></p></li>
<li><p>Scaling by factors 2 and 0.5, using the diagonal matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐃</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{D} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 0.5 \end{pmatrix}</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>3</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐃</mi><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.5</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>6</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_3(\mathbf{v}) = \mathbf{D}\mathbf{v} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 0.5 \end{pmatrix} \begin{pmatrix} 3 \\ 2 \end{pmatrix} = \begin{pmatrix} 6 \\ 1 \end{pmatrix} = (6, 1)</annotation></semantics></math></p></li>
</ol>
</div>
</section>
</section>
<section id="complex-vector-spaces" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="complex-vector-spaces"><span class="header-section-number">4.2</span> Complex vector spaces</h2>
<p>While real vector spaces are the most common foundation for embeddings, complex vector spaces offer additional modeling capabilities:</p>
<div id="def-complex-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.8 (Complex vector space)</strong></span> A <strong>complex vector space</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math> consists of vectors with complex number components. Each component has a real and imaginary part: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">z = a + bi</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">a, b \in \mathbb{R}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><msqrt><mrow><mi>−</mi><mn>1</mn></mrow></msqrt></mrow><annotation encoding="application/x-tex">i = \sqrt{-1}</annotation></semantics></math>.</p>
<p>Key operations in complex spaces include:</p>
<ol type="1">
<li><strong>Complex conjugate</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>z</mi><mo accent="true">¯</mo></mover><mo>=</mo><mi>a</mi><mo>−</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">\overline{z} = a - bi</annotation></semantics></math></li>
<li><strong>Modulus</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>z</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">|z| = \sqrt{a^2 + b^2}</annotation></semantics></math></li>
<li><strong>Hermitian inner product</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mover><msub><mi>y</mi><mi>i</mi></msub><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{i=1}^d x_i \overline{y_i}</annotation></semantics></math></li>
</ol>
</div>
<p>Complex vector spaces are particularly useful for modeling asymmetric relations in knowledge graphs, as we’ll see in later chapters when discussing models like ComplEx and RotatE.</p>
<div id="exm-complex-vector" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4 (Complex vector example)</strong></span> Consider complex vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>i</mi><mo>,</mo><mn>2</mn><mo>−</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{u} = (1+i, 2-i)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>−</mo><mn>2</mn><mi>i</mi><mo>,</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v} = (3-2i, i)</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{C}^2</annotation></semantics></math>.</p>
<p>Vector addition: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>i</mi><mo>,</mo><mn>2</mn><mo>−</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>−</mo><mn>2</mn><mi>i</mi><mo>,</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>−</mo><mi>i</mi><mo>,</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{u} + \mathbf{v} = (1+i, 2-i) + (3-2i, i) = (4-i, 2)</annotation></semantics></math></p>
<p>Hermitian inner product: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mover><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>−</mo><mn>2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">¯</mo></mover><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>−</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mover><mi>i</mi><mo accent="true">¯</mo></mover><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mn>2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>−</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mn>2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>−</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\langle \mathbf{u}, \mathbf{v} \rangle = (1+i) \cdot \overline{(3-2i)} + (2-i) \cdot \overline{i} = (1+i)(3+2i) + (2-i)(-i) = (1+i)(3+2i) + (2-i)(-i)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>3</mn><mi>i</mi><mo>+</mo><mn>2</mn><msup><mi>i</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>2</mn><mi>i</mi><mo>+</mo><msup><mi>i</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mn>5</mn><mi>i</mi><mo>−</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>2</mn><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>3</mn><mo>+</mo><mn>5</mn><mi>i</mi><mo>−</mo><mn>2</mn><mo>−</mo><mn>2</mn><mi>i</mi><mo>−</mo><mn>1</mn><mo>=</mo><mn>0</mn><mo>+</mo><mn>3</mn><mi>i</mi></mrow><annotation encoding="application/x-tex">= (3+2i+3i+2i^2) + (-2i+i^2) = (3+5i-2) + (-2i-1) = 3+5i-2-2i-1 = 0+3i</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>3</mn><mi>i</mi></mrow><annotation encoding="application/x-tex">= 3i</annotation></semantics></math></p>
<p>Note that unlike the standard inner product in real space, the Hermitian inner product can yield complex results.</p>
</div>
</section>
<section id="word-embeddings-a-motivating-example" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="word-embeddings-a-motivating-example"><span class="header-section-number">4.3</span> Word embeddings: a motivating example</h2>
<p>To better understand how vector embeddings can capture semantic relationships, let’s examine word embeddings, which have been instrumental in natural language processing and serve as a conceptual precursor to knowledge graph embeddings.</p>
<div id="def-word-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.9 (Word embedding)</strong></span> A <strong>word embedding</strong> is a mapping from words to vectors in a continuous vector space, where the geometric relationships between vectors capture semantic relationships between words.</p>
</div>
<p>Word embedding models like Word2Vec, GloVe, and FastText learn vector representations of words from large text corpora. These embeddings capture remarkable semantic properties.</p>
<section id="capturing-semantics-in-vector-space" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="capturing-semantics-in-vector-space"><span class="header-section-number">4.3.1</span> Capturing semantics in vector space</h3>
<p>Word embeddings exhibit a fascinating property: semantic relationships between words are reflected in geometric relationships between their vectors.</p>
<div id="exm-word-analogy" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5 (Word analogy in embedding space)</strong></span> In well-trained word embeddings, vector arithmetic can solve analogy problems:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐤</mi><mi>𝐢</mi><mi>𝐧</mi><mi>𝐠</mi></mrow><mo>−</mo><mrow><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>+</mo><mrow><mi>𝐰</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>≈</mo><mrow><mi>𝐪</mi><mi>𝐮</mi><mi>𝐞</mi><mi>𝐞</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{king} - \mathbf{man} + \mathbf{woman} \approx \mathbf{queen}</annotation></semantics></math></p>
<p>This means that the gender relationship (difference between “man” and “woman”) is captured consistently in the embedding space and can be applied to other word pairs.</p>
<p>Similar patterns emerge for other relationships:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐩</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo>−</mo><mrow><mi>𝐟</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐲</mi></mrow><mo>≈</mo><mrow><mi>𝐫</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{paris} - \mathbf{france} + \mathbf{italy} \approx \mathbf{rome}</annotation></semantics></math> (capital-country relationship)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐰</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐤</mi><mi>𝐞</mi><mi>𝐝</mi></mrow><mo>−</mo><mrow><mi>𝐰</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐤</mi></mrow><mo>+</mo><mrow><mi>𝐫</mi><mi>𝐮</mi><mi>𝐧</mi></mrow><mo>≈</mo><mrow><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{walked} - \mathbf{walk} + \mathbf{run} \approx \mathbf{ran}</annotation></semantics></math> (verb tense relationship)</li>
</ul>
</div>
<p>This example illustrates a powerful idea: semantic relationships can be captured as geometric transformations in embedding space. The difference vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐰</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>−</mo><mrow><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{woman} - \mathbf{man}</annotation></semantics></math> represents a gender transformation that can be applied to other words.</p>
<p>Knowledge graph embeddings extend this idea to entities and relations in a knowledge graph. Just as word embeddings capture semantic relationships between words, knowledge graph embeddings capture relationships between entities.</p>
</section>
</section>
<section id="embedding-entities-and-relations" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="embedding-entities-and-relations"><span class="header-section-number">4.4</span> Embedding entities and relations</h2>
<p>Knowledge graph embeddings map both entities and relations to vector space:</p>
<div id="def-entity-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.10 (Entity embedding)</strong></span> An <strong>entity embedding</strong> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><mi>E</mi><mo>→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">f: E \rightarrow \mathbb{R}^d</annotation></semantics></math> (or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math>) that maps each entity in the knowledge graph to a vector in the embedding space.</p>
</div>
<div id="def-relation-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.11 (Relation embedding)</strong></span> A <strong>relation embedding</strong> maps each relation to a parameter in the embedding space. Depending on the model, this could be:</p>
<ol type="1">
<li>A vector: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><mi>R</mi><mo>→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">g: R \rightarrow \mathbb{R}^d</annotation></semantics></math> (or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math>)</li>
<li>A matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><mi>R</mi><mo>→</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">g: R \rightarrow \mathbb{R}^{d \times d}</annotation></semantics></math> (or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{C}^{d \times d}</annotation></semantics></math>)</li>
<li>A tensor: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><mi>R</mi><mo>→</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">g: R \rightarrow \mathbb{R}^{d \times d \times k}</annotation></semantics></math> (or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{C}^{d \times d \times k}</annotation></semantics></math>)</li>
</ol>
</div>
<p>Different knowledge graph embedding models use different representations for relations, as we’ll explore in subsequent chapters.</p>
<section id="geometric-interpretations-of-relations" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="geometric-interpretations-of-relations"><span class="header-section-number">4.4.1</span> Geometric interpretations of relations</h3>
<p>Relations in knowledge graphs can be interpreted geometrically in the embedding space:</p>
<div id="def-relation-geometries" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.12 (Geometric interpretations of relations)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Translation</strong>: Relation as a displacement vector: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></li>
<li><strong>Rotation</strong>: Relation as a rotation operator: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐑</mi><mi>r</mi></msub><mi>𝐡</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{R}_r \mathbf{h} \approx \mathbf{t}</annotation></semantics></math></li>
<li><strong>Linear transformation</strong>: Relation as a matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐡</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_r \mathbf{h} \approx \mathbf{t}</annotation></semantics></math></li>
<li><strong>Bilinear product</strong>: Relation as a weighted interaction: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐡</mi><mi>T</mi></msup><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi><mo>≈</mo><mtext mathvariant="normal">high score</mtext></mrow><annotation encoding="application/x-tex">\mathbf{h}^T \mathbf{M}_r \mathbf{t} \approx \text{high score}</annotation></semantics></math></li>
</ol>
</div>
<div id="exm-relation-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6 (Geometric relation example)</strong></span> Consider a simple knowledge graph about geography:</p>
<ul>
<li>(France, has_capital, Paris)</li>
<li>(Germany, has_capital, Berlin)</li>
<li>(Italy, has_capital, Rome)</li>
</ul>
<p>In a translational model like TransE, these facts might be embedded as:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{France} + \mathbf{has\_capital} \approx \mathbf{Paris}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐆</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐲</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐁</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Germany} + \mathbf{has\_capital} \approx \mathbf{Berlin}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐈</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐲</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐑</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Italy} + \mathbf{has\_capital} \approx \mathbf{Rome}</annotation></semantics></math></li>
</ul>
<p>The relation “has_capital” is represented as a consistent displacement vector that, when added to a country’s embedding, approximates the embedding of its capital city.</p>
</div>
</section>
</section>
<section id="encoding-relation-patterns" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="encoding-relation-patterns"><span class="header-section-number">4.5</span> Encoding relation patterns</h2>
<p>One of the key challenges in knowledge graph embeddings is capturing different relation patterns. Various relation types exhibit different logical properties:</p>
<div id="def-relation-patterns" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.13 (Relation patterns)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Symmetry</strong>: A relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is symmetric if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⟹</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(h, r, t) \implies (t, r, h)</annotation></semantics></math> Example: “is_sibling_of”</p></li>
<li><p><strong>Antisymmetry</strong>: A relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is antisymmetric if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⟹</mo><mo>¬</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(h, r, t) \implies \neg(t, r, h)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>≠</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">h \neq t</annotation></semantics></math> Example: “is_greater_than”</p></li>
<li><p><strong>Inversion</strong>: Relations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>1</mn></msub><annotation encoding="application/x-tex">r_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>2</mn></msub><annotation encoding="application/x-tex">r_2</annotation></semantics></math> are inverses if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⟹</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(h, r_1, t) \implies (t, r_2, h)</annotation></semantics></math> Example: “is_parent_of” and “is_child_of”</p></li>
<li><p><strong>Composition</strong>: Relations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>1</mn></msub><annotation encoding="application/x-tex">r_1</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>2</mn></msub><annotation encoding="application/x-tex">r_2</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>3</mn></msub><annotation encoding="application/x-tex">r_3</annotation></semantics></math> form a composition if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi>e</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∧</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>e</mi><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⟹</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><msub><mi>r</mi><mn>3</mn></msub><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(h, r_1, e) \land (e, r_2, t) \implies (h, r_3, t)</annotation></semantics></math> Example: “is_born_in” and “is_located_in” compose to “has_nationality”</p></li>
<li><p><strong>Transitivity</strong>: A relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is transitive if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>e</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∧</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>e</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⟹</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(h, r, e) \land (e, r, t) \implies (h, r, t)</annotation></semantics></math> Example: “is_ancestor_of”</p></li>
</ol>
</div>
<p>Different embedding models have different capacities to capture these relation patterns. The geometric representation of relations constrains which patterns can be modeled effectively.</p>
<div id="exm-relation-patterns" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7 (Modeling relation patterns)</strong></span> Consider how different geometric interpretations might model symmetry:</p>
<ol type="1">
<li><p><strong>Translation model</strong>: If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math> for a symmetric relation, then we would also need <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐭</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{t} + \mathbf{r} \approx \mathbf{h}</annotation></semantics></math>, which implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{r} \approx \mathbf{0}</annotation></semantics></math> (the zero vector). This suggests that pure translation models struggle with symmetric relations.</p></li>
<li><p><strong>Rotation model</strong>: A 180-degree rotation (represented by multiplication by -1) is symmetrical: if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>𝐡</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">-\mathbf{h} \approx \mathbf{t}</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>𝐭</mi><mo>≈</mo><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">-\mathbf{t} \approx \mathbf{h}</annotation></semantics></math>. This makes rotation-based models potentially better suited for symmetric relations.</p></li>
<li><p><strong>Bilinear model</strong>: If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐡</mi><mi>T</mi></msup><mi>𝐌</mi><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h}^T \mathbf{M} \mathbf{t}</annotation></semantics></math> is high for a valid triple, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math> is symmetric (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐌</mi><mo>=</mo><msup><mi>𝐌</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{M} = \mathbf{M}^T</annotation></semantics></math>), then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐭</mi><mi>T</mi></msup><mi>𝐌</mi><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{t}^T \mathbf{M} \mathbf{h}</annotation></semantics></math> will also be high, naturally capturing symmetry.</p></li>
</ol>
</div>
</section>
<section id="scoring-functions-and-plausibility" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="scoring-functions-and-plausibility"><span class="header-section-number">4.6</span> Scoring functions and plausibility</h2>
<p>Knowledge graph embedding models use scoring functions to assess the plausibility of triples:</p>
<div id="def-scoring-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.14 (Scoring function)</strong></span> A <strong>scoring function</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_r(h, t)</annotation></semantics></math> measures the plausibility of a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>. Depending on the model, higher or lower scores indicate more plausible triples.</p>
<p>Scoring functions can be broadly categorized as:</p>
<ol type="1">
<li><strong>Distance-based</strong>: Lower scores indicate more plausible triples, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><mi>𝐭</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</annotation></semantics></math></li>
<li><strong>Similarity-based</strong>: Higher scores indicate more plausible triples, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐡</mi><mi>T</mi></msup><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">f_r(h, t) = \mathbf{h}^T \mathbf{M}_r \mathbf{t}</annotation></semantics></math></li>
</ol>
</div>
<p>The scoring function defines the geometry of the embedding space and determines which triples are considered plausible. Different models use different scoring functions based on their geometric interpretation of relations.</p>
<div id="exm-scoring" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8 (Scoring function example)</strong></span> Consider a knowledge graph with entities <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Bob</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext><mo>,</mo><mtext mathvariant="normal">Dave</mtext><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">E = \{\text{Alice}, \text{Bob}, \text{Charlie}, \text{Dave}\}</annotation></semantics></math> and relation “knows”. Suppose we have the following true triples:</p>
<ul>
<li>(Alice, knows, Bob)</li>
<li>(Bob, knows, Charlie)</li>
<li>(Charlie, knows, Dave)</li>
</ul>
<p>Using a TransE-like model with a distance-based scoring function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><mi>𝐭</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</annotation></semantics></math>, we might compute scores like:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Bob</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mn>0.1</mn></mrow><annotation encoding="application/x-tex">f_{\text{knows}}(\text{Alice}, \text{Bob}) = -0.1</annotation></semantics></math> (low distance, high plausibility)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mn>0.5</mn></mrow><annotation encoding="application/x-tex">f_{\text{knows}}(\text{Alice}, \text{Charlie}) = -0.5</annotation></semantics></math> (moderate distance, moderate plausibility)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Dave</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mn>1.2</mn></mrow><annotation encoding="application/x-tex">f_{\text{knows}}(\text{Alice}, \text{Dave}) = -1.2</annotation></semantics></math> (high distance, low plausibility)</li>
</ul>
<p>For a query “Who does Alice know?”, we would rank entities by score:</p>
<ol type="1">
<li>Bob (-0.1)</li>
<li>Charlie (-0.5)</li>
<li>Dave (-1.2)</li>
</ol>
<p>This correctly predicts the direct connection (Alice knows Bob) as most plausible, while allowing for the possibility of indirect connections.</p>
</div>
</section>
<section id="learning-embeddings" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="learning-embeddings"><span class="header-section-number">4.7</span> Learning embeddings</h2>
<p>Now that we understand what embeddings are, let’s briefly discuss how they are learned:</p>
<div id="def-embedding-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.15 (Embedding learning)</strong></span> <strong>Embedding learning</strong> is the process of finding entity and relation representations that minimize a loss function over the observed triples in a knowledge graph. The general approach involves:</p>
<ol type="1">
<li>Initialize embeddings randomly</li>
<li>Compute scores for observed (positive) triples and unobserved (negative) triples</li>
<li>Update embeddings to increase the gap between scores of positive and negative triples</li>
<li>Repeat until convergence</li>
</ol>
</div>
<p>The specific learning algorithm depends on the model, but most knowledge graph embedding approaches use variants of stochastic gradient descent with a margin-based or logistic loss function.</p>
<div id="exm-learning" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9 (Embedding learning example)</strong></span> Consider a simple knowledge graph with positive triples:</p>
<ul>
<li>(Alice, knows, Bob)</li>
<li>(Bob, knows, Charlie)</li>
</ul>
<p>And negative (unobserved) triples:</p>
<ul>
<li>(Alice, knows, Charlie)</li>
<li>(Charlie, knows, Alice)</li>
</ul>
<p>Using a margin-based loss with margin <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>, we would update embeddings to ensure: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Bob</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>γ</mi><mo>&lt;</mo><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\text{knows}}(\text{Alice}, \text{Bob}) + \gamma &lt; f_{\text{knows}}(\text{Alice}, \text{Charlie})</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Bob</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>γ</mi><mo>&lt;</mo><msub><mi>f</mi><mtext mathvariant="normal">knows</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Charlie</mtext><mo>,</mo><mtext mathvariant="normal">Alice</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\text{knows}}(\text{Bob}, \text{Charlie}) + \gamma &lt; f_{\text{knows}}(\text{Charlie}, \text{Alice})</annotation></semantics></math></p>
<p>Where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_r(h, t)</annotation></semantics></math> is the scoring function, and we assume lower scores indicate more plausible triples.</p>
</div>
<p>A more detailed discussion of learning algorithms will be presented in Chapter 6.</p>
</section>
<section id="visualization-and-interpretation" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="visualization-and-interpretation"><span class="header-section-number">4.8</span> Visualization and interpretation</h2>
<p>Visualizing embeddings can provide insights into how models capture semantic relationships:</p>
<div id="def-embedding-visualization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.16 (Embedding visualization)</strong></span> <strong>Embedding visualization</strong> techniques project high-dimensional embeddings into 2D or 3D space for visual inspection. Common approaches include:</p>
<ol type="1">
<li><strong>Principal Component Analysis (PCA)</strong>: Linear projection that preserves maximum variance</li>
<li><strong>t-SNE</strong>: Non-linear projection that preserves local distances</li>
<li><strong>UMAP</strong>: Non-linear projection that balances local and global structure</li>
</ol>
</div>
<p>While the full embedding space may have hundreds of dimensions, visualizations can reveal clusters, relationships, and patterns that help us understand what the embeddings have learned.</p>
<div id="exm-visualization" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.10 (Embedding visualization example)</strong></span> Imagine we’ve learned embeddings for a geography knowledge graph containing countries, cities, and continents. A 2D visualization might reveal:</p>
<ol type="1">
<li>Clusters of entities by continent (European countries grouped together, Asian countries grouped together, etc.)</li>
<li>Consistent spatial relationships between countries and their capitals</li>
<li>Hierarchical structures where countries appear “between” their cities and their continent</li>
</ol>
<p>These patterns would indicate that the embedding model has successfully captured geographic relationships.</p>
</div>
</section>
<section id="embedding-spaces-for-knowledge-graphs" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="embedding-spaces-for-knowledge-graphs"><span class="header-section-number">4.9</span> Embedding spaces for knowledge graphs</h2>
<p>Different knowledge graph embedding models use different types of embedding spaces and geometric interpretations. Here’s a preview of what we’ll explore in subsequent chapters:</p>
<section id="translational-models-chapter-3" class="level3" data-number="4.9.1">
<h3 data-number="4.9.1" class="anchored" data-anchor-id="translational-models-chapter-3"><span class="header-section-number">4.9.1</span> Translational models (Chapter 3)</h3>
<p>Translational models like TransE, TransH, and TransR represent relations as translations in embedding space:</p>
<div id="def-translational-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.17 (Translational embedding space)</strong></span> In a <strong>translational embedding space</strong>, entities are points in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math> and relations are displacement vectors. For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model enforces: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></p>
</div>
<div id="exm-translational" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.11 (Translational embedding example)</strong></span> In a geography knowledge graph, the “is_capital_of” relation might be represented as a consistent displacement that, when applied to a capital city’s embedding, approximates the embedding of the corresponding country:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo>+</mo><mrow><mi>𝐢</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi mathvariant="bold">_</mi><mi>𝐨</mi><mi>𝐟</mi></mrow><mo>≈</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Paris} + \mathbf{is\_capital\_of} \approx \mathbf{France}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐁</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐧</mi></mrow><mo>+</mo><mrow><mi>𝐢</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi mathvariant="bold">_</mi><mi>𝐨</mi><mi>𝐟</mi></mrow><mo>≈</mo><mrow><mi>𝐆</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐲</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Berlin} + \mathbf{is\_capital\_of} \approx \mathbf{Germany}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐑</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐢</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi mathvariant="bold">_</mi><mi>𝐨</mi><mi>𝐟</mi></mrow><mo>≈</mo><mrow><mi>𝐈</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐲</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Rome} + \mathbf{is\_capital\_of} \approx \mathbf{Italy}</annotation></semantics></math></p>
</div>
</section>
<section id="semantic-matching-models-chapter-4" class="level3" data-number="4.9.2">
<h3 data-number="4.9.2" class="anchored" data-anchor-id="semantic-matching-models-chapter-4"><span class="header-section-number">4.9.2</span> Semantic matching models (Chapter 4)</h3>
<p>Semantic matching models like RESCAL, DistMult, and ComplEx measure plausibility through similarity functions:</p>
<div id="def-bilinear-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.18 (Bilinear embedding space)</strong></span> In a <strong>bilinear embedding space</strong>, entities are points in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math> and relations are matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{M}_r \in \mathbb{R}^{d \times d}</annotation></semantics></math>. The plausibility of a triple is measured by: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>𝐡</mi><mi>T</mi></msup><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">f_r(h, t) = \mathbf{h}^T \mathbf{M}_r \mathbf{t}</annotation></semantics></math></p>
</div>
<div id="exm-bilinear" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.12 (Bilinear embedding example)</strong></span> In a movie knowledge graph, the “acted_in” relation might be represented as a matrix that measures compatibility between actor and movie embeddings:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="normal">acted_in</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Leonardo DiCaprio</mtext><mo>,</mo><mtext mathvariant="normal">Titanic</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mi>𝐋</mi><mi>𝐞</mi><mi>𝐨</mi><mi>𝐧</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐝</mi><mi>𝐨</mi></mrow><mi>T</mi></msup><msub><mi>𝐌</mi><mtext mathvariant="normal">acted_in</mtext></msub><mrow><mi>𝐓</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐢</mi><mi>𝐜</mi></mrow></mrow><annotation encoding="application/x-tex">f_{\text{acted\_in}}(\text{Leonardo DiCaprio}, \text{Titanic}) = \mathbf{Leonardo}^T \mathbf{M}_{\text{acted\_in}} \mathbf{Titanic}</annotation></semantics></math></p>
<p>Actors and movies with compatible characteristics would yield high scores.</p>
</div>
</section>
<section id="complex-and-rotation-based-models-chapter-5" class="level3" data-number="4.9.3">
<h3 data-number="4.9.3" class="anchored" data-anchor-id="complex-and-rotation-based-models-chapter-5"><span class="header-section-number">4.9.3</span> Complex and rotation-based models (Chapter 5)</h3>
<p>More advanced models like ComplEx and RotatE use complex vector spaces or rotational transformations:</p>
<div id="def-complex-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.19 (Complex embedding space)</strong></span> In a <strong>complex embedding space</strong>, entities are points in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math> and relations can be diagonal matrices or element-wise operations. The use of complex numbers enables modeling asymmetric relations through complex conjugation.</p>
</div>
<div id="exm-complex-embeddings" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.13 (Complex embedding example)</strong></span> In ComplEx, the scoring function is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Re</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="prefix">⟨</mo><mi>𝐡</mi><mo>,</mo><mi>𝐫</mi><mo>,</mo><mover><mi>𝐭</mi><mo accent="true">¯</mo></mover><mo stretchy="false" form="postfix">⟩</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Re</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>h</mi><mi>i</mi></msub><msub><mi>r</mi><mi>i</mi></msub><mover><msub><mi>t</mi><mi>i</mi></msub><mo accent="true">¯</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \text{Re}\left(\sum_{i=1}^d h_i r_i \overline{t_i}\right)</annotation></semantics></math></p>
<p>The asymmetry comes from using the complex conjugate of the tail entity embedding. This allows the model to distinguish between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(t, r, h)</annotation></semantics></math>, which is essential for antisymmetric relations.</p>
</div>
</section>
</section>
<section id="properties-of-good-embeddings" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="properties-of-good-embeddings"><span class="header-section-number">4.10</span> Properties of good embeddings</h2>
<p>What makes an embedding space effective for knowledge graph representation? Several desirable properties have been identified:</p>
<div id="def-embedding-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.20 (Desirable embedding properties)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Expressiveness</strong>: Ability to represent various relation patterns (symmetry, antisymmetry, inversion, composition)</li>
<li><strong>Efficiency</strong>: Computationally efficient storage and operations</li>
<li><strong>Inductive capability</strong>: Generalization to unseen entities or relations</li>
<li><strong>Interpretability</strong>: Meaningful geometric structure that aids understanding</li>
<li><strong>Smoothness</strong>: Similar entities have similar embeddings</li>
<li><strong>Separation</strong>: Different relation patterns are clearly distinguished</li>
</ol>
</div>
<p>Different embedding models prioritize different properties. For example, TransE offers simplicity and efficiency but has limited expressiveness for certain relation patterns. More complex models like Neural Tensor Networks offer greater expressiveness but at the cost of computational efficiency.</p>
</section>
<section id="from-word-embeddings-to-knowledge-graph-embeddings" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="from-word-embeddings-to-knowledge-graph-embeddings"><span class="header-section-number">4.11</span> From word embeddings to knowledge graph embeddings</h2>
<p>Let’s take a step back and connect knowledge graph embeddings to the broader context of representation learning:</p>
<div id="def-representation-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.21 (Representation learning)</strong></span> <strong>Representation learning</strong> is the process of automatically discovering useful representations of data for tasks like classification, prediction, and generation. Vector embeddings are a form of representation learning where objects are mapped to continuous vector spaces.</p>
</div>
<p>Word embeddings were among the first successful applications of representation learning in NLP. Knowledge graph embeddings extend similar principles to structured relational data.</p>
<div id="exm-embedding-evolution" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.14 (Evolution of embedding approaches)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>One-hot encodings</strong>: Sparse, high-dimensional vectors where each dimension corresponds to a discrete item (e.g., a specific word)</li>
<li><strong>Word embeddings</strong>: Dense, low-dimensional vectors learned from text data, capturing semantic relationships</li>
<li><strong>Knowledge graph embeddings</strong>: Vector representations of entities and relations in a knowledge graph, capturing relational patterns</li>
<li><strong>Contextual embeddings</strong>: Dynamic representations that adapt based on context (e.g., BERT embeddings for language)</li>
</ol>
</div>
<p>The progression from simple one-hot encodings to sophisticated embedding models reflects a broader trend in AI: moving from hand-crafted features to learned representations that capture implicit patterns in data.</p>
</section>
<section id="embedding-space-dimensions" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="embedding-space-dimensions"><span class="header-section-number">4.12</span> Embedding space dimensions</h2>
<p>The dimensionality of the embedding space is a crucial hyperparameter that balances expressiveness against computational efficiency and overfitting:</p>
<div id="def-dimensionality" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.22 (Embedding dimensionality)</strong></span> The <strong>dimensionality</strong> of an embedding space refers to the number of components in each entity or relation vector. Higher dimensionality offers:</p>
<ol type="1">
<li>Greater capacity to capture complex patterns</li>
<li>More degrees of freedom for separating entities</li>
<li>Potential to model fine-grained distinctions</li>
</ol>
<p>But comes with drawbacks:</p>
<ol type="1">
<li>Increased computational cost</li>
<li>Higher memory requirements</li>
<li>Greater risk of overfitting</li>
<li>Reduced interpretability</li>
</ol>
</div>
<p>There’s no universal “optimal” dimensionality for knowledge graph embeddings. The appropriate dimension depends on:</p>
<ol type="1">
<li>The size and complexity of the knowledge graph</li>
<li>The diversity of entity and relation types</li>
<li>The specific embedding model being used</li>
<li>Available computational resources</li>
<li>The downstream task requirements</li>
</ol>
<div id="exm-dimensionality" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.15 (Dimensionality selection example)</strong></span> Consider different knowledge graphs and reasonable embedding dimensions:</p>
<ol type="1">
<li><p><strong>Small domain-specific graph</strong> (100 entities, 10 relations):</p>
<ul>
<li>Lower dimension (e.g., d = 20-50) might be sufficient</li>
<li>Prevents overfitting to limited data</li>
</ul></li>
<li><p><strong>Medium-sized general knowledge graph</strong> (10,000 entities, 100 relations):</p>
<ul>
<li>Moderate dimension (e.g., d = 100-200)</li>
<li>Balances expressiveness and computational efficiency</li>
</ul></li>
<li><p><strong>Large-scale knowledge graph</strong> (millions of entities, thousands of relations):</p>
<ul>
<li>Higher dimension (e.g., d = 200-500)</li>
<li>Needed to capture diverse entity and relation types</li>
</ul></li>
</ol>
</div>
<p>In practice, dimensionality is often treated as a hyperparameter and tuned based on validation performance.</p>
</section>
<section id="manifold-hypothesis-and-embedding-spaces" class="level2" data-number="4.13">
<h2 data-number="4.13" class="anchored" data-anchor-id="manifold-hypothesis-and-embedding-spaces"><span class="header-section-number">4.13</span> Manifold hypothesis and embedding spaces</h2>
<p>The effectiveness of embeddings can be partly explained by the manifold hypothesis:</p>
<div id="def-manifold-hypothesis" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.23 (Manifold hypothesis)</strong></span> The <strong>manifold hypothesis</strong> states that real-world high-dimensional data (such as images, text, or knowledge) tends to lie on or near a low-dimensional manifold within the high-dimensional space. This allows for effective dimensionality reduction while preserving important structures.</p>
</div>
<p>For knowledge graphs, this suggests that the semantic relationships between entities can be effectively captured in a relatively low-dimensional space, despite the apparent complexity of real-world knowledge.</p>
<div id="exm-manifold" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.16 (Manifold example)</strong></span> Consider a knowledge graph of animals with attributes like size, habitat, diet, and taxonomy. Although animals could be described along hundreds of dimensions, the meaningful variations might lie primarily along a much lower-dimensional manifold.</p>
<p>For instance, we might find that a 100-dimensional embedding space effectively captures the relationships between animals, even though the full description of each animal’s attributes might require thousands of dimensions.</p>
</div>
<p>The manifold hypothesis helps explain why relatively low-dimensional embeddings (typically 50-500 dimensions) can effectively model large knowledge graphs with millions of entities and relations.</p>
</section>
<section id="relationship-to-matrix-and-tensor-factorization" class="level2" data-number="4.14">
<h2 data-number="4.14" class="anchored" data-anchor-id="relationship-to-matrix-and-tensor-factorization"><span class="header-section-number">4.14</span> Relationship to matrix and tensor factorization</h2>
<p>Knowledge graph embedding methods are closely related to matrix and tensor factorization techniques:</p>
<div id="def-factorization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.24 (Tensor factorization for knowledge graphs)</strong></span> A knowledge graph can be represented as a 3-dimensional tensor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒳</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{X} \in \mathbb{R}^{|E| \times |R| \times |E|}</annotation></semantics></math>, where:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{h,r,t} = 1</annotation></semantics></math> if triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math> exists in the knowledge graph</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{h,r,t} = 0</annotation></semantics></math> otherwise</li>
</ul>
<p><strong>Tensor factorization</strong> decomposes this tensor into lower-dimensional factors, which can be interpreted as entity and relation embeddings.</p>
</div>
<p>Different knowledge graph embedding models correspond to different tensor factorization approaches:</p>
<ol type="1">
<li><strong>RESCAL</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>≈</mo><msup><mi>𝐡</mi><mi>T</mi></msup><msub><mi>𝐖</mi><mi>r</mi></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}_{h,r,t} \approx \mathbf{h}^T \mathbf{W}_r \mathbf{t}</annotation></semantics></math> (Tucker decomposition)</li>
<li><strong>DistMult</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>≈</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>h</mi><mi>i</mi></msub><msub><mi>r</mi><mi>i</mi></msub><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{X}_{h,r,t} \approx \sum_{i=1}^d h_i r_i t_i</annotation></semantics></math> (CP decomposition with diagonal core tensor)</li>
<li><strong>ComplEx</strong>: Complex-valued extension of CP decomposition</li>
</ol>
<p>This connection to tensor factorization provides theoretical grounding for knowledge graph embedding methods.</p>
<div id="exm-factorization" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.17 (Tensor factorization example)</strong></span> Consider a tiny knowledge graph with 3 entities (Alice, Bob, Charlie) and 2 relations (knows, likes):</p>
<p>The tensor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒳</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math> would be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 2 \times 3</annotation></semantics></math>, with entries:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">knows</mtext><mo>,</mo><mtext mathvariant="normal">Bob</mtext></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{\text{Alice},\text{knows},\text{Bob}} = 1</annotation></semantics></math> (Alice knows Bob)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mtext mathvariant="normal">Bob</mtext><mo>,</mo><mtext mathvariant="normal">knows</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{\text{Bob},\text{knows},\text{Charlie}} = 1</annotation></semantics></math> (Bob knows Charlie)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mtext mathvariant="normal">Alice</mtext><mo>,</mo><mtext mathvariant="normal">likes</mtext><mo>,</mo><mtext mathvariant="normal">Charlie</mtext></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{\text{Alice},\text{likes},\text{Charlie}} = 1</annotation></semantics></math> (Alice likes Charlie)</li>
<li>All other entries = 0</li>
</ul>
<p>A rank-2 factorization might yield:</p>
<ul>
<li>Entity embeddings: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐀</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.8</mn><mo>,</mo><mn>0.3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Alice} = (0.8, 0.3)</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐁</mi><mi>𝐨</mi><mi>𝐛</mi></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Bob} = (0.5, 0.7)</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐂</mi><mi>𝐡</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐞</mi></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.2</mn><mo>,</mo><mn>0.9</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Charlie} = (0.2, 0.9)</annotation></semantics></math></li>
<li>Relation matrices: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐖</mi><mtext mathvariant="normal">knows</mtext></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.9</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.8</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{knows}} = \begin{pmatrix} 0.9 &amp; 0.2 \\ 0.2 &amp; 0.8 \end{pmatrix}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐖</mi><mtext mathvariant="normal">likes</mtext></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.7</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.7</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}_{\text{likes}} = \begin{pmatrix} 0.3 &amp; 0.7 \\ 0.7 &amp; 0.4 \end{pmatrix}</annotation></semantics></math></li>
</ul>
<p>These embeddings would approximately reconstruct the original tensor.</p>
</div>
</section>
<section id="vector-spaces-beyond-euclidean-geometry" class="level2" data-number="4.15">
<h2 data-number="4.15" class="anchored" data-anchor-id="vector-spaces-beyond-euclidean-geometry"><span class="header-section-number">4.15</span> Vector spaces beyond Euclidean geometry</h2>
<p>While most knowledge graph embedding models use Euclidean space, other geometric spaces can offer advantages for certain types of knowledge structures:</p>
<div id="def-non-euclidean" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.25 (Non-Euclidean embedding spaces)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Hyperbolic space</strong>: A non-Euclidean geometry with constant negative curvature, well-suited for hierarchical structures</li>
<li><strong>Spherical space</strong>: Embedding on the surface of a hypersphere, useful for clustering and when distances should be bounded</li>
<li><strong>Product spaces</strong>: Combinations of different geometric spaces, capturing different aspects of the data</li>
</ol>
</div>
<div id="exm-hyperbolic" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.18 (Hyperbolic embedding example)</strong></span> Consider a knowledge graph with hierarchical taxonomic relations, such as:</p>
<ul>
<li>(Animal, has_subclass, Mammal)</li>
<li>(Mammal, has_subclass, Canine)</li>
<li>(Canine, has_subclass, Dog)</li>
<li>(Dog, has_subclass, Labrador)</li>
</ul>
<p>In Euclidean space, it’s challenging to maintain consistent distances between all levels of the hierarchy.</p>
<p>Hyperbolic space, however, has an exponentially expanding volume as you move away from the origin. This allows for embedding trees with consistent distances between levels, making it well-suited for taxonomic hierarchies.</p>
</div>
<p>Advanced knowledge graph embedding models sometimes leverage these alternative geometric spaces to better capture specific relation patterns.</p>
</section>
<section id="understanding-the-embedding-space-through-analogies" class="level2" data-number="4.16">
<h2 data-number="4.16" class="anchored" data-anchor-id="understanding-the-embedding-space-through-analogies"><span class="header-section-number">4.16</span> Understanding the embedding space through analogies</h2>
<p>To build intuition about how embedding spaces work, it’s helpful to consider analogies in knowledge graph embeddings, similar to the word analogies we discussed earlier:</p>
<div id="def-kg-analogy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.26 (Knowledge graph analogy)</strong></span> A <strong>knowledge graph analogy</strong> is a relationship pattern of the form “entity A is to entity B as entity C is to entity D,” which can be expressed through vector arithmetic in the embedding space.</p>
</div>
<div id="exm-kg-analogy" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.19 (Knowledge graph analogy example)</strong></span> In a well-trained embedding space for a geography knowledge graph, we might observe:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>−</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo>≈</mo><mrow><mi>𝐆</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐲</mi></mrow><mo>−</mo><mrow><mi>𝐁</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐧</mi></mrow><mo>≈</mo><mrow><mi>𝐈</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐲</mi></mrow><mo>−</mo><mrow><mi>𝐑</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{France} - \mathbf{Paris} \approx \mathbf{Germany} - \mathbf{Berlin} \approx \mathbf{Italy} - \mathbf{Rome}</annotation></semantics></math></p>
<p>This indicates that the “is_capital_of” relationship is captured as a consistent vector offset in the embedding space.</p>
<p>Similarly, for a family relationship knowledge graph:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>−</mo><mrow><mi>𝐁</mi><mi>𝐨</mi><mi>𝐲</mi></mrow><mo>≈</mo><mrow><mi>𝐖</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>−</mo><mrow><mi>𝐆</mi><mi>𝐢</mi><mi>𝐫</mi><mi>𝐥</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Man} - \mathbf{Boy} \approx \mathbf{Woman} - \mathbf{Girl}</annotation></semantics></math></p>
<p>This captures the “adult version of” relationship.</p>
</div>
<p>Analogies provide a way to visualize and understand what the embedding space has learned. When embeddings successfully capture these analogical relationships, it suggests that the model has learned a meaningful semantic structure.</p>
</section>
<section id="entity-types-and-relation-domains" class="level2" data-number="4.17">
<h2 data-number="4.17" class="anchored" data-anchor-id="entity-types-and-relation-domains"><span class="header-section-number">4.17</span> Entity types and relation domains</h2>
<p>Knowledge graphs often include information about entity types and relation domains/ranges:</p>
<div id="def-type-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.27 (Type constraints)</strong></span> <strong>Type constraints</strong> in knowledge graphs specify:</p>
<ol type="1">
<li><strong>Entity types</strong>: The categories that entities belong to (e.g., Person, Place, Organization)</li>
<li><strong>Relation domains</strong>: The types of entities that can appear as the head of a relation</li>
<li><strong>Relation ranges</strong>: The types of entities that can appear as the tail of a relation</li>
</ol>
</div>
<p>In embedding spaces, entity types often manifest as clusters or regions:</p>
<div id="exm-type-clustering" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.20 (Type clustering example)</strong></span> In a well-trained embedding space for a general knowledge graph:</p>
<ul>
<li>Person entities might cluster in one region of the space</li>
<li>Location entities in another region</li>
<li>Organization entities in yet another region</li>
</ul>
<p>This clustering emerges naturally during training because entities of the same type tend to participate in similar relations.</p>
<p>For instance, the relation “was_born_in” typically has a Person as its head entity and a Location as its tail entity. The embedding model learns to place Persons and Locations in different regions of the space to satisfy this pattern.</p>
</div>
<p>Some advanced embedding models explicitly incorporate type information to improve performance.</p>
</section>
<section id="translating-from-symbolic-to-continuous-representations" class="level2" data-number="4.18">
<h2 data-number="4.18" class="anchored" data-anchor-id="translating-from-symbolic-to-continuous-representations"><span class="header-section-number">4.18</span> Translating from symbolic to continuous representations</h2>
<p>Knowledge graph embeddings fundamentally translate between symbolic and continuous representations:</p>
<div id="def-representation-types" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.28 (Representation paradigms)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Symbolic representation</strong>: Discrete, exact, and interpretable (e.g., triples in a knowledge graph)</li>
<li><strong>Continuous representation</strong>: Dense, approximate, and amenable to mathematical operations (e.g., embeddings)</li>
</ol>
</div>
<p>This translation offers both benefits and challenges:</p>
<section id="benefits-of-continuous-representations" class="level3" data-number="4.18.1">
<h3 data-number="4.18.1" class="anchored" data-anchor-id="benefits-of-continuous-representations"><span class="header-section-number">4.18.1</span> Benefits of continuous representations:</h3>
<ol type="1">
<li>Efficient computation through vector operations</li>
<li>Similarity-based generalization to unseen facts</li>
<li>Robust handling of noise and uncertainty</li>
<li>Integration with neural models for downstream tasks</li>
</ol>
</section>
<section id="challenges-of-continuous-representations" class="level3" data-number="4.18.2">
<h3 data-number="4.18.2" class="anchored" data-anchor-id="challenges-of-continuous-representations"><span class="header-section-number">4.18.2</span> Challenges of continuous representations:</h3>
<ol type="1">
<li>Loss of interpretability</li>
<li>Approximation errors</li>
<li>Difficulty in incorporating hard logical constraints</li>
<li>Challenges in representing discrete categories</li>
</ol>
<div id="exm-continuous-symbolic" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.21 (Continuous vs.&nbsp;symbolic example)</strong></span> Consider the fact “Paris is the capital of France.”</p>
<p><strong>Symbolic representation</strong>:</p>
<pre><code>(Paris, is_capital_of, France)</code></pre>
<p>This is exact, discrete, and human-interpretable.</p>
<p><strong>Continuous representation (vectors)</strong>:</p>
<pre><code>Paris = [0.2, 0.7, -0.5, 0.1, ...]
is_capital_of = [0.8, -0.3, 0.2, 0.6, ...]
France = [1.0, 0.4, -0.3, 0.7, ...]</code></pre>
<p>This is approximate and less interpretable but enables mathematical operations like measuring that: <code>||Paris + is_capital_of - France|| = 0.3</code> (low distance indicating a plausible triple)</p>
</div>
<p>The power of knowledge graph embeddings comes from bridging these two representation paradigms, leveraging the strengths of both.</p>
</section>
</section>
<section id="embedding-initialization" class="level2" data-number="4.19">
<h2 data-number="4.19" class="anchored" data-anchor-id="embedding-initialization"><span class="header-section-number">4.19</span> Embedding initialization</h2>
<p>The initialization of embeddings before training can significantly impact the learning process and final results:</p>
<div id="def-initialization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.29 (Embedding initialization)</strong></span> <strong>Embedding initialization</strong> refers to the strategy for setting initial values of entity and relation embeddings before training. Common approaches include:</p>
<ol type="1">
<li><strong>Random uniform initialization</strong>: Sample from uniform distribution, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mfrac><mn>1</mn><msqrt><mi>d</mi></msqrt></mfrac><mo>,</mo><mfrac><mn>1</mn><msqrt><mi>d</mi></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">U(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})</annotation></semantics></math></li>
<li><strong>Normal initialization</strong>: Sample from normal distribution, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(0, \frac{1}{d})</annotation></semantics></math></li>
<li><strong>Xavier/Glorot initialization</strong>: Scaled based on input and output dimensions</li>
<li><strong>Pre-trained initialization</strong>: Start with embeddings from another model or data source</li>
</ol>
</div>
<div id="exm-initialization" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.22 (Initialization impact example)</strong></span> Consider training a TransE model with different initializations:</p>
<ol type="1">
<li><p><strong>Too small initialization</strong> (e.g., uniform[-0.01, 0.01]):</p>
<ul>
<li>Entities start very close together</li>
<li>Gradients may be small</li>
<li>Training progresses slowly</li>
<li>May get stuck in suboptimal solutions</li>
</ul></li>
<li><p><strong>Too large initialization</strong> (e.g., uniform[-10, 10]):</p>
<ul>
<li>Entities start far apart</li>
<li>Initial predictions are very poor</li>
<li>Training may be unstable</li>
<li>Can lead to exploding gradients</li>
</ul></li>
<li><p><strong>Properly scaled initialization</strong> (e.g., uniform[-0.5, 0.5] for d=100):</p>
<ul>
<li>Provides good separation between entities</li>
<li>Allows effective gradient flow</li>
<li>Enables faster convergence</li>
</ul></li>
</ol>
</div>
<p>Proper initialization helps avoid numerical issues during training and can lead to better final embeddings.</p>
</section>
<section id="normalization-and-constraints" class="level2" data-number="4.20">
<h2 data-number="4.20" class="anchored" data-anchor-id="normalization-and-constraints"><span class="header-section-number">4.20</span> Normalization and constraints</h2>
<p>Many knowledge graph embedding models apply normalization or constraints to the embeddings:</p>
<div id="def-normalization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.30 (Embedding normalization)</strong></span> <strong>Embedding normalization</strong> refers to constraints applied to embeddings during training, such as:</p>
<ol type="1">
<li><strong>L2 normalization</strong>: Constraining <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{e}\|_2 = 1</annotation></semantics></math> for all entity embeddings</li>
<li><strong>Maximum norm constraint</strong>: Ensuring <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>≤</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\|\mathbf{e}\|_2 \leq C</annotation></semantics></math> for some constant <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math></li>
<li><strong>Unit modulus constraint</strong>: For complex embeddings, ensuring <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|e_i| = 1</annotation></semantics></math> for each component</li>
</ol>
</div>
<p>These constraints serve several purposes:</p>
<ol type="1">
<li>Preventing the model from “cheating” by scaling embeddings arbitrarily</li>
<li>Improving numerical stability during training</li>
<li>Enforcing specific geometric properties (e.g., rotations in complex space)</li>
<li>Regularizing the model to prevent overfitting</li>
</ol>
<div id="exm-normalization" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.23 (Normalization example)</strong></span> In TransE, a common approach is to normalize entity embeddings to unit L2 norm after each training step:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>←</mo><mfrac><mi>𝐞</mi><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbf{e} \leftarrow \frac{\mathbf{e}}{\|\mathbf{e}\|_2}</annotation></semantics></math> for each entity embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐞</mi><annotation encoding="application/x-tex">\mathbf{e}</annotation></semantics></math></p>
<p>This ensures that all entities lie on the unit hypersphere, preventing the model from simply pushing entities far apart to satisfy the margin-based loss function.</p>
<p>Without this constraint, the model could “cheat” by making <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\mathbf{h}\|</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐭</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\mathbf{t}\|</annotation></semantics></math> very large while keeping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐫</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\mathbf{r}\|</annotation></semantics></math> small, which would satisfy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math> without capturing meaningful semantic relationships.</p>
</div>
<p>Different models use different normalization strategies based on their geometric interpretation of embeddings.</p>
</section>
<section id="visualization-techniques-for-embeddings" class="level2" data-number="4.21">
<h2 data-number="4.21" class="anchored" data-anchor-id="visualization-techniques-for-embeddings"><span class="header-section-number">4.21</span> Visualization techniques for embeddings</h2>
<p>Visualization is a powerful tool for understanding and debugging embedding spaces:</p>
<div id="def-viz-techniques" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.31 (Embedding visualization techniques)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Dimensionality reduction</strong>:</p>
<ul>
<li>Principal Component Analysis (PCA): Linear projection preserving maximum variance</li>
<li>t-SNE: Non-linear projection preserving local distances</li>
<li>UMAP: Non-linear projection balancing local and global structure</li>
</ul></li>
<li><p><strong>Relation-centric visualization</strong>:</p>
<ul>
<li>Plotting head-tail pairs connected by the same relation</li>
<li>Visualizing relation-specific transformations</li>
</ul></li>
<li><p><strong>Query-based visualization</strong>:</p>
<ul>
<li>Showing nearest neighbors to specific entities</li>
<li>Visualizing entities connected through specific relation paths</li>
</ul></li>
</ol>
</div>
<div id="exm-viz" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.24 (Visualization example)</strong></span> Imagine we’ve trained embeddings for a geography knowledge graph. We might create visualizations such as:</p>
<ol type="1">
<li><p><strong>PCA projection</strong> showing countries clustered by continent, with capital cities positioned near their countries</p></li>
<li><p><strong>Relation visualization</strong> showing consistent offsets between countries and their capitals, indicating that the “capital_of” relation has been learned as a consistent translation</p></li>
<li><p><strong>Nearest neighbor visualization</strong> for the query “France,” showing similar entities like “Germany,” “Italy,” and “Spain” nearby in the embedding space</p></li>
</ol>
</div>
<p>Visualizations can reveal patterns, biases, and problems in the embedding space, helping guide refinements to the model.</p>
</section>
<section id="summary" class="level2" data-number="4.22">
<h2 data-number="4.22" class="anchored" data-anchor-id="summary"><span class="header-section-number">4.22</span> Summary</h2>
<p>In this chapter, we’ve explored the fundamental concepts of vector space representations for knowledge graphs:</p>
<ul>
<li>Vector spaces provide a mathematical framework for representing entities and relations</li>
<li>Different geometric interpretations (translations, rotations, bilinear forms) capture different aspects of relations</li>
<li>Embedding spaces can encode semantic relationships as geometric relationships</li>
<li>Properties like dimensionality, normalization, and initialization impact embedding quality</li>
<li>Different relation patterns (symmetry, composition, transitivity) require different modeling approaches</li>
<li>Visualization techniques help understand and debug embedding spaces</li>
</ul>
<p>This mathematical foundation prepares us for the detailed exploration of specific knowledge graph embedding models in the following chapters. We’ll begin in Chapter 3 with translational embedding models, which provide an intuitive geometric interpretation of relations as translations in the embedding space.</p>
</section>
<section id="further-reading" class="level2" data-number="4.23">
<h2 data-number="4.23" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">4.23</span> Further reading</h2>
<section id="vector-spaces-and-linear-algebra" class="level3" data-number="4.23.1">
<h3 data-number="4.23.1" class="anchored" data-anchor-id="vector-spaces-and-linear-algebra"><span class="header-section-number">4.23.1</span> Vector spaces and linear algebra</h3>
<ul>
<li>Strang, G. (2006). Linear Algebra and Its Applications. 4th Edition. Brooks Cole.</li>
<li>Axler, S. (2014). Linear Algebra Done Right. 3rd Edition. Springer.</li>
<li>Roman, S. (2005). Advanced Linear Algebra. 3rd Edition. Springer.</li>
</ul>
</section>
<section id="embedding-methods" class="level3" data-number="4.23.2">
<h3 data-number="4.23.2" class="anchored" data-anchor-id="embedding-methods"><span class="header-section-number">4.23.2</span> Embedding methods</h3>
<ul>
<li>Bengio, Y., Courville, A., &amp; Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1798-1828.</li>
<li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (pp.&nbsp;3111-3119).</li>
<li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (pp.&nbsp;1532-1543).</li>
</ul>
</section>
<section id="tensor-factorization-and-knowledge-graph-embeddings" class="level3" data-number="4.23.3">
<h3 data-number="4.23.3" class="anchored" data-anchor-id="tensor-factorization-and-knowledge-graph-embeddings"><span class="header-section-number">4.23.3</span> Tensor factorization and knowledge graph embeddings</h3>
<ul>
<li>Kolda, T. G., &amp; Bader, B. W. (2009). Tensor decompositions and applications. SIAM Review, 51(3), 455-500.</li>
<li>Nickel, M., Tresp, V., &amp; Kriegel, H. P. (2011). A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (pp.&nbsp;809-816).</li>
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
</ul>
</section>
<section id="visualization-and-dimensionality-reduction" class="level3" data-number="4.23.4">
<h3 data-number="4.23.4" class="anchored" data-anchor-id="visualization-and-dimensionality-reduction"><span class="header-section-number">4.23.4</span> Visualization and dimensionality reduction</h3>
<ul>
<li>Van der Maaten, L., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov), 2579-2605.</li>
<li>McInnes, L., Healy, J., &amp; Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.</li>
<li>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … &amp; Zheng, X. (2016). TensorFlow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (pp.&nbsp;265-283).</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/intro.html" class="pagination-link" aria-label="Introduction to Knowledge Graphs and Representations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/translation-based.html" class="pagination-link" aria-label="Translation-Based Embedding Models">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>