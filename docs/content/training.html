<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Training and Optimization Techniques – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/evaluation.html" rel="next">
<link href="../content/rotation-nn.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/training.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-training-knowledge-graph-embeddings" id="toc-introduction-to-training-knowledge-graph-embeddings" class="nav-link active" data-scroll-target="#introduction-to-training-knowledge-graph-embeddings"><span class="header-section-number">8.1</span> Introduction to Training Knowledge Graph Embeddings</a></li>
  <li><a href="#basic-training-process-for-knowledge-graph-embeddings" id="toc-basic-training-process-for-knowledge-graph-embeddings" class="nav-link" data-scroll-target="#basic-training-process-for-knowledge-graph-embeddings"><span class="header-section-number">8.2</span> Basic Training Process for Knowledge Graph Embeddings</a></li>
  <li><a href="#loss-functions-for-knowledge-graph-embeddings" id="toc-loss-functions-for-knowledge-graph-embeddings" class="nav-link" data-scroll-target="#loss-functions-for-knowledge-graph-embeddings"><span class="header-section-number">8.3</span> Loss Functions for Knowledge Graph Embeddings</a>
  <ul class="collapse">
  <li><a href="#margin-based-ranking-loss" id="toc-margin-based-ranking-loss" class="nav-link" data-scroll-target="#margin-based-ranking-loss"><span class="header-section-number">8.3.1</span> Margin-Based Ranking Loss</a></li>
  <li><a href="#logistic-loss" id="toc-logistic-loss" class="nav-link" data-scroll-target="#logistic-loss"><span class="header-section-number">8.3.2</span> Logistic Loss</a></li>
  <li><a href="#self-adversarial-negative-sampling-loss" id="toc-self-adversarial-negative-sampling-loss" class="nav-link" data-scroll-target="#self-adversarial-negative-sampling-loss"><span class="header-section-number">8.3.3</span> Self-Adversarial Negative Sampling Loss</a></li>
  </ul></li>
  <li><a href="#negative-sampling-strategies" id="toc-negative-sampling-strategies" class="nav-link" data-scroll-target="#negative-sampling-strategies"><span class="header-section-number">8.4</span> Negative Sampling Strategies</a>
  <ul class="collapse">
  <li><a href="#uniform-negative-sampling" id="toc-uniform-negative-sampling" class="nav-link" data-scroll-target="#uniform-negative-sampling"><span class="header-section-number">8.4.1</span> Uniform Negative Sampling</a></li>
  <li><a href="#bernoulli-negative-sampling" id="toc-bernoulli-negative-sampling" class="nav-link" data-scroll-target="#bernoulli-negative-sampling"><span class="header-section-number">8.4.2</span> Bernoulli Negative Sampling</a></li>
  <li><a href="#self-adversarial-negative-sampling" id="toc-self-adversarial-negative-sampling" class="nav-link" data-scroll-target="#self-adversarial-negative-sampling"><span class="header-section-number">8.4.3</span> Self-Adversarial Negative Sampling</a></li>
  </ul></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">8.5</span> Optimization Algorithms</a>
  <ul class="collapse">
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd"><span class="header-section-number">8.5.1</span> Stochastic Gradient Descent (SGD)</a></li>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad"><span class="header-section-number">8.5.2</span> Adagrad</a></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam"><span class="header-section-number">8.5.3</span> Adam</a></li>
  </ul></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques"><span class="header-section-number">8.6</span> Regularization Techniques</a>
  <ul class="collapse">
  <li><a href="#l2-regularization" id="toc-l2-regularization" class="nav-link" data-scroll-target="#l2-regularization"><span class="header-section-number">8.6.1</span> L2 Regularization</a></li>
  <li><a href="#norm-constraints" id="toc-norm-constraints" class="nav-link" data-scroll-target="#norm-constraints"><span class="header-section-number">8.6.2</span> Norm Constraints</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout"><span class="header-section-number">8.6.3</span> Dropout</a></li>
  </ul></li>
  <li><a href="#batch-selection-and-training-strategies" id="toc-batch-selection-and-training-strategies" class="nav-link" data-scroll-target="#batch-selection-and-training-strategies"><span class="header-section-number">8.7</span> Batch Selection and Training Strategies</a>
  <ul class="collapse">
  <li><a href="#mini-batch-training" id="toc-mini-batch-training" class="nav-link" data-scroll-target="#mini-batch-training"><span class="header-section-number">8.7.1</span> Mini-Batch Training</a></li>
  <li><a href="#curriculum-learning" id="toc-curriculum-learning" class="nav-link" data-scroll-target="#curriculum-learning"><span class="header-section-number">8.7.2</span> Curriculum Learning</a></li>
  <li><a href="#self-paced-learning" id="toc-self-paced-learning" class="nav-link" data-scroll-target="#self-paced-learning"><span class="header-section-number">8.7.3</span> Self-Paced Learning</a></li>
  </ul></li>
  <li><a href="#hyperparameter-selection-and-tuning" id="toc-hyperparameter-selection-and-tuning" class="nav-link" data-scroll-target="#hyperparameter-selection-and-tuning"><span class="header-section-number">8.8</span> Hyperparameter Selection and Tuning</a></li>
  <li><a href="#early-stopping-and-convergence-criteria" id="toc-early-stopping-and-convergence-criteria" class="nav-link" data-scroll-target="#early-stopping-and-convergence-criteria"><span class="header-section-number">8.9</span> Early Stopping and Convergence Criteria</a></li>
  <li><a href="#implementation-examples" id="toc-implementation-examples" class="nav-link" data-scroll-target="#implementation-examples"><span class="header-section-number">8.10</span> Implementation Examples</a></li>
  <li><a href="#scalability-considerations" id="toc-scalability-considerations" class="nav-link" data-scroll-target="#scalability-considerations"><span class="header-section-number">8.11</span> Scalability Considerations</a>
  <ul class="collapse">
  <li><a href="#parallel-and-distributed-training" id="toc-parallel-and-distributed-training" class="nav-link" data-scroll-target="#parallel-and-distributed-training"><span class="header-section-number">8.11.1</span> Parallel and Distributed Training</a></li>
  <li><a href="#memory-efficient-implementations" id="toc-memory-efficient-implementations" class="nav-link" data-scroll-target="#memory-efficient-implementations"><span class="header-section-number">8.11.2</span> Memory-Efficient Implementations</a></li>
  </ul></li>
  <li><a href="#best-practices-and-common-pitfalls" id="toc-best-practices-and-common-pitfalls" class="nav-link" data-scroll-target="#best-practices-and-common-pitfalls"><span class="header-section-number">8.12</span> Best Practices and Common Pitfalls</a>
  <ul class="collapse">
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices"><span class="header-section-number">8.12.1</span> Best Practices</a></li>
  <li><a href="#common-pitfalls" id="toc-common-pitfalls" class="nav-link" data-scroll-target="#common-pitfalls"><span class="header-section-number">8.12.2</span> Common Pitfalls</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">8.13</span> Conclusion</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">8.14</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="introduction-to-training-knowledge-graph-embeddings" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="introduction-to-training-knowledge-graph-embeddings"><span class="header-section-number">8.1</span> Introduction to Training Knowledge Graph Embeddings</h2>
<p>Knowledge graph embeddings (KGEs) have proven to be powerful tools for knowledge graph completion and link prediction. However, their performance heavily depends not only on the model architecture but also on how they are trained. In this chapter, we will explore the techniques, strategies, and best practices for effectively training knowledge graph embedding models.</p>
<p>Training a KGE model involves several critical components: designing an appropriate loss function, implementing effective negative sampling strategies, selecting suitable optimization algorithms, and tuning hyperparameters. Each of these components can significantly impact model performance, sometimes even more than the choice of model architecture itself.</p>
<p>Let’s begin by understanding the general training procedure for knowledge graph embeddings before diving into the specific techniques that make training more effective and efficient.</p>
</section>
<section id="basic-training-process-for-knowledge-graph-embeddings" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="basic-training-process-for-knowledge-graph-embeddings"><span class="header-section-number">8.2</span> Basic Training Process for Knowledge Graph Embeddings</h2>
<p>The training process for KGE models typically follows these general steps:</p>
<ol type="1">
<li><strong>Initialization</strong>: Initialize entity and relation embeddings randomly</li>
<li><strong>Batch Creation</strong>: Sample batches of positive triples from the training set</li>
<li><strong>Negative Sampling</strong>: Generate negative samples for each positive triple</li>
<li><strong>Scoring</strong>: Compute scores for both positive and negative triples</li>
<li><strong>Loss Computation</strong>: Calculate the loss based on the scores</li>
<li><strong>Optimization</strong>: Update embeddings using gradient descent</li>
<li><strong>Iteration</strong>: Repeat steps 2-6 until convergence or a maximum number of epochs</li>
</ol>
<p>This process aims to learn embeddings where valid triples receive high scores (or low distances/energies, depending on the model), while invalid triples receive low scores (or high distances/energies).</p>
<div id="def-kge-training-objective" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.1 (Knowledge Graph Embedding Training Objective)</strong></span> The general objective of training a knowledge graph embedding model is to learn entity and relation representations such that:</p>
<p><span class="math display">\[\text{score}(h, r, t) &gt; \text{score}(h', r, t')\]</span></p>
<p>for any valid triple <span class="math inline">\((h, r, t)\)</span> and any invalid triple <span class="math inline">\((h', r, t')\)</span>, where <span class="math inline">\(h', t'\)</span> are corrupted entities.</p>
</div>
<p>Now, let’s examine the key components of this training process in detail.</p>
</section>
<section id="loss-functions-for-knowledge-graph-embeddings" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="loss-functions-for-knowledge-graph-embeddings"><span class="header-section-number">8.3</span> Loss Functions for Knowledge Graph Embeddings</h2>
<p>The loss function is a critical component that guides the learning process. It quantifies how well the current embeddings are performing and provides the signal for updating them. Several loss functions have been proposed for training knowledge graph embeddings.</p>
<section id="margin-based-ranking-loss" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="margin-based-ranking-loss"><span class="header-section-number">8.3.1</span> Margin-Based Ranking Loss</h3>
<p>The margin-based ranking loss is one of the most widely used loss functions for KGE models. It was first introduced with TransE and has since been adopted by many other models.</p>
<div id="def-margin-ranking-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.2 (Margin-Based Ranking Loss)</strong></span> Given a positive triple <span class="math inline">\((h, r, t)\)</span> and a negative triple <span class="math inline">\((h', r, t')\)</span>, the margin-based ranking loss is defined as:</p>
<p><span class="math display">\[L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma + f_r(h, t) - f_r(h', t')]_+\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(S\)</span> is the set of valid triples</li>
<li><span class="math inline">\(S'_{(h,r,t)}\)</span> is the set of corrupted triples generated from <span class="math inline">\((h,r,t)\)</span></li>
<li><span class="math inline">\(f_r(h, t)\)</span> is the score function for triple <span class="math inline">\((h, r, t)\)</span></li>
<li><span class="math inline">\(\gamma\)</span> is the margin hyperparameter</li>
<li><span class="math inline">\([x]_+ = \max(0, x)\)</span> denotes the positive part of <span class="math inline">\(x\)</span></li>
</ul>
</div>
<p>The intuition behind this loss function is to ensure that the score of a valid triple is at least <span class="math inline">\(\gamma\)</span> higher than the score of any corrupted triple. When this condition is met, the loss becomes zero.</p>
<div id="exm-margin-loss-transe" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1 (Margin Loss with TransE)</strong></span> In TransE, the score function is <span class="math inline">\(f_r(h, t) = -\|h + r - t\|\)</span> (negative distance, so higher is better). With margin <span class="math inline">\(\gamma = 1\)</span>, if we have:</p>
<ul>
<li>Positive triple: <span class="math inline">\((Alice, friendOf, Bob)\)</span> with score <span class="math inline">\(-0.2\)</span></li>
<li>Negative triple: <span class="math inline">\((Alice, friendOf, Charlie)\)</span> with score <span class="math inline">\(-1.5\)</span></li>
</ul>
<p>The loss would be: <span class="math inline">\([1 + (-0.2) - (-1.5)]_+ = [1 - 0.2 + 1.5]_+ = [2.3]_+ = 2.3\)</span></p>
<p>This positive loss indicates that the model needs to adjust embeddings to increase the difference between scores.</p>
</div>
</section>
<section id="logistic-loss" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="logistic-loss"><span class="header-section-number">8.3.2</span> Logistic Loss</h3>
<p>Another common approach is to use the logistic loss (or binary cross-entropy), which interprets the problem as binary classification of triples.</p>
<div id="def-logistic-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.3 (Logistic Loss)</strong></span> The logistic loss for knowledge graph embeddings is defined as:</p>
<p><span class="math display">\[L = \sum_{(h,r,t) \in S \cup S'} y_{(h,r,t)} \log \sigma(f_r(h, t)) + (1 - y_{(h,r,t)}) \log (1 - \sigma(f_r(h, t)))\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(S\)</span> is the set of valid triples with <span class="math inline">\(y_{(h,r,t)} = 1\)</span></li>
<li><span class="math inline">\(S'\)</span> is the set of corrupted triples with <span class="math inline">\(y_{(h,r,t)} = 0\)</span></li>
<li><span class="math inline">\(\sigma\)</span> is the sigmoid function <span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span></li>
<li><span class="math inline">\(f_r(h, t)\)</span> is the score function</li>
</ul>
</div>
<p>This loss is particularly suitable for models like DistMult and ComplEx, where the score can be interpreted as a measure of triple plausibility.</p>
<div id="exm-logistic-loss-complex" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2 (Logistic Loss with ComplEx)</strong></span> For ComplEx, with a positive triple <span class="math inline">\((Alice, friendOf, Bob)\)</span> with score <span class="math inline">\(2.1\)</span> and a negative triple <span class="math inline">\((Alice, friendOf, Charlie)\)</span> with score <span class="math inline">\(-1.3\)</span>:</p>
<p>For the positive triple: <span class="math inline">\(-\log \sigma(2.1) = -\log(0.891) = 0.115\)</span></p>
<p>For the negative triple: <span class="math inline">\(-\log(1 - \sigma(-1.3)) = -\log(0.785) = 0.242\)</span></p>
<p>The total loss would be the sum: <span class="math inline">\(0.115 + 0.242 = 0.357\)</span></p>
</div>
</section>
<section id="self-adversarial-negative-sampling-loss" class="level3" data-number="8.3.3">
<h3 data-number="8.3.3" class="anchored" data-anchor-id="self-adversarial-negative-sampling-loss"><span class="header-section-number">8.3.3</span> Self-Adversarial Negative Sampling Loss</h3>
<p>A more advanced loss function was introduced with RotatE, called self-adversarial negative sampling loss. This loss weighs negative samples based on their current scores, giving more importance to difficult negative samples.</p>
<div id="def-self-adversarial-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.4 (Self-Adversarial Negative Sampling Loss)</strong></span> The self-adversarial negative sampling loss is defined as:</p>
<p><span class="math display">\[L = -\log \sigma(\gamma - f_r(h, t)) - \sum_{i=1}^{n} p(h'_i, r, t'_i) \log \sigma(f_r(h'_i, t'_i) - \gamma)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\gamma\)</span> is a fixed margin</li>
<li><span class="math inline">\(f_r(h, t)\)</span> is the score for the positive triple</li>
<li><span class="math inline">\((h'_i, r, t'_i)\)</span> is the <span class="math inline">\(i\)</span>-th negative sample</li>
<li><span class="math inline">\(p(h'_i, r, t'_i) = \frac{\exp \alpha f_r(h'_i, t'_i)}{\sum_{j} \exp \alpha f_r(h'_j, t'_j)}\)</span> is the adversarial weight</li>
<li><span class="math inline">\(\alpha\)</span> is a temperature hyperparameter</li>
</ul>
</div>
<p>The self-adversarial loss automatically adjusts the weights of negative samples, focusing the learning on harder negatives that are most likely to improve the model.</p>
<div id="exm-self-adversarial-loss" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.3 (Self-Adversarial Loss with RotatE)</strong></span> Consider a positive triple and three negative samples with the following scores:</p>
<ul>
<li>Positive triple: <span class="math inline">\((Alice, friendOf, Bob)\)</span> with score <span class="math inline">\(-0.3\)</span></li>
<li>Negative triple 1: <span class="math inline">\((Alice, friendOf, Charlie)\)</span> with score <span class="math inline">\(-0.8\)</span></li>
<li>Negative triple 2: <span class="math inline">\((Alice, friendOf, David)\)</span> with score <span class="math inline">\(-1.5\)</span></li>
<li>Negative triple 3: <span class="math inline">\((Alice, friendOf, Eve)\)</span> with score <span class="math inline">\(-0.5\)</span></li>
</ul>
<p>With temperature <span class="math inline">\(\alpha = 1\)</span>, the weights would be: <span class="math inline">\(p_1 = \frac{e^{-0.8}}{e^{-0.8} + e^{-1.5} + e^{-0.5}} = \frac{0.449}{0.449 + 0.223 + 0.607} = 0.351\)</span> <span class="math inline">\(p_2 = \frac{e^{-1.5}}{0.449 + 0.223 + 0.607} = 0.174\)</span> <span class="math inline">\(p_3 = \frac{e^{-0.5}}{0.449 + 0.223 + 0.607} = 0.475\)</span></p>
<p>The negative with the highest score (Eve) gets the highest weight, indicating it’s the most difficult negative and should contribute more to the loss.</p>
</div>
</section>
</section>
<section id="negative-sampling-strategies" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="negative-sampling-strategies"><span class="header-section-number">8.4</span> Negative Sampling Strategies</h2>
<p>Negative sampling is essential for training KGE models because knowledge graphs typically only contain positive examples (valid triples). To learn discriminative embeddings, we need to generate negative examples (invalid triples).</p>
<section id="uniform-negative-sampling" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="uniform-negative-sampling"><span class="header-section-number">8.4.1</span> Uniform Negative Sampling</h3>
<p>The simplest strategy is uniform negative sampling, where corrupted triples are generated by randomly replacing either the head or tail entity with another entity from the knowledge graph.</p>
<div id="def-uniform-negative-sampling" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.5 (Uniform Negative Sampling)</strong></span> For each positive triple <span class="math inline">\((h, r, t)\)</span>, generate a negative triple by:</p>
<ol type="1">
<li>Randomly deciding whether to corrupt the head or tail (with equal probability)</li>
<li>Randomly selecting an entity <span class="math inline">\(e \in E \setminus \{h\}\)</span> (if corrupting head) or <span class="math inline">\(e \in E \setminus \{t\}\)</span> (if corrupting tail)</li>
<li>Creating the negative triple as <span class="math inline">\((e, r, t)\)</span> or <span class="math inline">\((h, r, e)\)</span></li>
</ol>
</div>
<p>While simple to implement, uniform sampling doesn’t account for the structural properties of relations, which can lead to inefficient training.</p>
</section>
<section id="bernoulli-negative-sampling" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="bernoulli-negative-sampling"><span class="header-section-number">8.4.2</span> Bernoulli Negative Sampling</h3>
<p>Bernoulli negative sampling improves upon uniform sampling by considering the relationship types. It recognizes that some relations are predominantly one-to-many, many-to-one, or many-to-many.</p>
<div id="def-bernoulli-negative-sampling" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.6 (Bernoulli Negative Sampling)</strong></span> For each positive triple <span class="math inline">\((h, r, t)\)</span>, generate a negative triple by:</p>
<ol type="1">
<li>Computing probabilities <span class="math inline">\(p_h\)</span> and <span class="math inline">\(p_t\)</span> for corrupting head or tail based on relation properties</li>
<li>Sampling head or tail for corruption based on probabilities <span class="math inline">\(p_h\)</span> and <span class="math inline">\(p_t\)</span></li>
<li>Randomly selecting a replacement entity</li>
<li>Creating the negative triple accordingly</li>
</ol>
<p>The probability of corrupting the head is calculated as: <span class="math display">\[p_h = \frac{tph}{tph + hpt}\]</span></p>
<p>where <span class="math inline">\(tph\)</span> is the average number of tails per head for relation <span class="math inline">\(r\)</span>, and <span class="math inline">\(hpt\)</span> is the average number of heads per tail.</p>
</div>
<p>This approach is particularly beneficial for models like TransE that struggle with certain relation types.</p>
<div id="exm-bernoulli-sampling" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.4 (Bernoulli Sampling Example)</strong></span> Consider the relation <span class="math inline">\(parentOf\)</span>:</p>
<ul>
<li>Average tails per head (<span class="math inline">\(tph\)</span>) = 2.5 (people have ~2.5 children on average)</li>
<li>Average heads per tail (<span class="math inline">\(hpt\)</span>) = 2 (people have 2 parents)</li>
</ul>
<p>The probability of corrupting the head would be: <span class="math inline">\(p_h = \frac{2.5}{2.5 + 2} = 0.556\)</span></p>
<p>So for a triple <span class="math inline">\((Bob, parentOf, Alice)\)</span>, we would corrupt the head with probability 0.556 and the tail with probability 0.444, reflecting the nature of the relation.</p>
</div>
</section>
<section id="self-adversarial-negative-sampling" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3" class="anchored" data-anchor-id="self-adversarial-negative-sampling"><span class="header-section-number">8.4.3</span> Self-Adversarial Negative Sampling</h3>
<p>Self-adversarial negative sampling not only generates negative samples but also assigns weights to them based on their current scores.</p>
<div id="def-self-adversarial-sampling" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.7 (Self-Adversarial Negative Sampling)</strong></span> &nbsp;</p>
<ol type="1">
<li>Generate multiple negative samples for each positive triple</li>
<li>Calculate the score for each negative sample using the current model</li>
<li>Compute weights for each negative sample based on their scores: <span class="math display">\[p(h', r, t') = \frac{\exp(\alpha f_r(h', t'))}{\sum_{j}\exp(\alpha f_r(h'_j, t'_j))}\]</span></li>
<li>Use these weights when computing the loss function</li>
</ol>
</div>
<p>This strategy focuses training on difficult negative samples that are most informative for learning, leading to faster convergence and better performance.</p>
</section>
</section>
<section id="optimization-algorithms" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="optimization-algorithms"><span class="header-section-number">8.5</span> Optimization Algorithms</h2>
<p>After defining the loss function and negative sampling strategy, we need to optimize the embeddings using gradient-based methods. Several optimization algorithms have been employed for training KGE models.</p>
<section id="stochastic-gradient-descent-sgd" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="stochastic-gradient-descent-sgd"><span class="header-section-number">8.5.1</span> Stochastic Gradient Descent (SGD)</h3>
<p>The most basic approach is stochastic gradient descent, which updates embeddings using the gradient of the loss function.</p>
<div id="def-sgd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.8 (Stochastic Gradient Descent (SGD))</strong></span> The update rule for SGD is: <span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\theta\)</span> represents the model parameters (entity and relation embeddings)</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate</li>
<li><span class="math inline">\(\nabla_\theta L(\theta_t)\)</span> is the gradient of the loss with respect to parameters</li>
</ul>
</div>
<p>While simple, SGD often requires careful tuning of the learning rate and may converge slowly.</p>
</section>
<section id="adagrad" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="adagrad"><span class="header-section-number">8.5.2</span> Adagrad</h3>
<p>Adagrad adapts the learning rate for each parameter based on the historical gradients, which is beneficial for sparse data like knowledge graphs.</p>
<div id="def-adagrad" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.9 (Adagrad)</strong></span> The update rule for Adagrad is: <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta L(\theta_t)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(G_t\)</span> is the sum of squares of past gradients</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant to prevent division by zero</li>
</ul>
</div>
<p>Adagrad automatically decreases the learning rate for frequently updated parameters, which helps when some entities or relations appear more frequently than others.</p>
</section>
<section id="adam" class="level3" data-number="8.5.3">
<h3 data-number="8.5.3" class="anchored" data-anchor-id="adam"><span class="header-section-number">8.5.3</span> Adam</h3>
<p>Adam combines the benefits of adaptive learning rates with momentum, making it one of the most popular optimization algorithms for neural networks, including KGE models.</p>
<div id="def-adam" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.10 (Adam Optimizer)</strong></span> The update rules for Adam are: <span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta L(\theta_t)\]</span> <span class="math display">\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta L(\theta_t))^2\]</span> <span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}\]</span> <span class="math display">\[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are the first and second moment estimates</li>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are decay rates for the moments</li>
<li><span class="math inline">\(\hat{m}_t\)</span> and <span class="math inline">\(\hat{v}_t\)</span> are bias-corrected moment estimates</li>
</ul>
</div>
<p>Adam often achieves faster convergence and better performance than SGD or Adagrad for KGE models, especially when training deep neural network-based approaches.</p>
</section>
</section>
<section id="regularization-techniques" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="regularization-techniques"><span class="header-section-number">8.6</span> Regularization Techniques</h2>
<p>Regularization helps prevent overfitting and ensures that the learned embeddings generalize well to unseen data.</p>
<section id="l2-regularization" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="l2-regularization"><span class="header-section-number">8.6.1</span> L2 Regularization</h3>
<p>L2 regularization (weight decay) is a common technique that penalizes large embedding values.</p>
<div id="def-l2-regularization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.11 (L2 Regularization)</strong></span> The L2 regularization term added to the loss function is: <span class="math display">\[\lambda \sum_{\theta \in \Theta} \|\theta\|_2^2\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the regularization strength</li>
<li><span class="math inline">\(\Theta\)</span> is the set of all model parameters</li>
</ul>
</div>
<p>L2 regularization encourages smoother embedding spaces and helps prevent overfitting.</p>
</section>
<section id="norm-constraints" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="norm-constraints"><span class="header-section-number">8.6.2</span> Norm Constraints</h3>
<p>Many KGE models constrain embeddings to lie on a unit sphere or within a bounded region.</p>
<div id="def-norm-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.12 (Norm Constraints)</strong></span> After each update step, normalize the embeddings: <span class="math display">\[e = \frac{e}{\|e\|}\]</span></p>
<p>for entity embeddings <span class="math inline">\(e\)</span> to enforce unit norm.</p>
</div>
<p>This constraint prevents the model from artificially decreasing the loss by increasing the magnitude of the embeddings, which is particularly important for distance-based models like TransE.</p>
<div id="exm-norm-constraint" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.5 (Norm Constraint Example)</strong></span> Consider an entity embedding <span class="math inline">\(e = [0.5, 1.2, -0.8]\)</span> after an update step.</p>
<p>Its norm is <span class="math inline">\(\|e\| = \sqrt{0.5^2 + 1.2^2 + (-0.8)^2} = \sqrt{2.13} = 1.46\)</span></p>
<p>After normalization, the embedding becomes: <span class="math inline">\(e' = \frac{e}{\|e\|} = \frac{[0.5, 1.2, -0.8]}{1.46} = [0.34, 0.82, -0.55]\)</span></p>
<p>The normalized embedding has unit norm: <span class="math inline">\(\|e'\| = 1\)</span></p>
</div>
</section>
<section id="dropout" class="level3" data-number="8.6.3">
<h3 data-number="8.6.3" class="anchored" data-anchor-id="dropout"><span class="header-section-number">8.6.3</span> Dropout</h3>
<p>For neural network-based KGE models, dropout is an effective regularization technique.</p>
<div id="def-dropout" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.13 (Dropout)</strong></span> During training, randomly set a fraction <span class="math inline">\(p\)</span> of the input units to 0 at each update. During testing, scale the outputs by <span class="math inline">\(1-p\)</span> to maintain the same expected sum.</p>
</div>
<p>Dropout helps neural network-based KGE models like ConvE and CompGCN generalize better by preventing co-adaptation of features.</p>
</section>
</section>
<section id="batch-selection-and-training-strategies" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="batch-selection-and-training-strategies"><span class="header-section-number">8.7</span> Batch Selection and Training Strategies</h2>
<p>The way batches are created and processed during training can significantly impact model performance and convergence speed.</p>
<section id="mini-batch-training" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="mini-batch-training"><span class="header-section-number">8.7.1</span> Mini-Batch Training</h3>
<p>Mini-batch training processes subsets of the training data at each step, balancing computational efficiency and update quality.</p>
<div id="def-mini-batch-training" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.14 (Mini-Batch Training)</strong></span> &nbsp;</p>
<ol type="1">
<li>Divide the training set into batches of size <span class="math inline">\(B\)</span></li>
<li>For each batch: a. Generate negative samples b. Compute loss and gradients c.&nbsp;Update model parameters</li>
<li>One complete pass through all batches constitutes an epoch</li>
</ol>
</div>
<p>Mini-batch training is essential for large knowledge graphs where processing all triples at once would be computationally infeasible.</p>
</section>
<section id="curriculum-learning" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="curriculum-learning"><span class="header-section-number">8.7.2</span> Curriculum Learning</h3>
<p>Curriculum learning arranges training examples from easy to hard, mimicking human learning processes.</p>
<div id="def-curriculum-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.15 (Curriculum Learning for KGEs)</strong></span> &nbsp;</p>
<ol type="1">
<li>Assign difficulty scores to training triples based on factors like:
<ul>
<li>Frequency of entities and relations</li>
<li>Graph connectivity</li>
<li>Embedding uncertainty</li>
</ul></li>
<li>Start training with easier triples</li>
<li>Gradually incorporate more difficult triples as training progresses</li>
</ol>
</div>
<p>This approach can lead to faster convergence and better final performance, especially for complex models.</p>
</section>
<section id="self-paced-learning" class="level3" data-number="8.7.3">
<h3 data-number="8.7.3" class="anchored" data-anchor-id="self-paced-learning"><span class="header-section-number">8.7.3</span> Self-Paced Learning</h3>
<p>Self-paced learning extends curriculum learning by dynamically determining the difficulty of examples based on the current model state.</p>
<div id="def-self-paced-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.16 (Self-Paced Learning for KGEs)</strong></span> &nbsp;</p>
<ol type="1">
<li>Compute loss for all training triples using current model</li>
<li>Select a subset of triples with lower loss (easier examples)</li>
<li>Train on these selected triples</li>
<li>Gradually increase the proportion of harder triples</li>
</ol>
</div>
<p>Self-paced learning adapts the curriculum to the model’s learning progress, potentially leading to more effective training.</p>
</section>
</section>
<section id="hyperparameter-selection-and-tuning" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="hyperparameter-selection-and-tuning"><span class="header-section-number">8.8</span> Hyperparameter Selection and Tuning</h2>
<p>Proper hyperparameter selection is crucial for achieving good performance with KGE models. Key hyperparameters include:</p>
<ol type="1">
<li><strong>Embedding dimension</strong>: Higher dimensions can capture more complex relationships but require more data and computational resources</li>
<li><strong>Learning rate</strong>: Controls the step size during optimization</li>
<li><strong>Batch size</strong>: Affects optimization stability and training speed</li>
<li><strong>Negative samples per positive</strong>: Impacts training efficiency and model performance</li>
<li><strong>Margin</strong>: For margin-based loss functions</li>
<li><strong>Regularization strength</strong>: Controls the impact of regularization terms</li>
<li><strong>Model-specific parameters</strong>: Such as relation-specific parameters in TransH, TransR, etc.</li>
</ol>
<div id="def-hyperparameter-tuning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.17 (Hyperparameter Tuning)</strong></span> Common strategies for hyperparameter tuning include:</p>
<ol type="1">
<li><strong>Grid Search</strong>: Systematically evaluating combinations of hyperparameters</li>
<li><strong>Random Search</strong>: Randomly sampling hyperparameter combinations</li>
<li><strong>Bayesian Optimization</strong>: Using probabilistic models to guide hyperparameter search</li>
</ol>
</div>
<p>For knowledge graph embeddings, performance on a validation set (typically measured by Mean Reciprocal Rank or Hits@k) is used to select the best hyperparameters.</p>
<div id="exm-hyperparameter-tuning" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.6 (Hyperparameter Tuning Example)</strong></span> For TransE on a medium-sized knowledge graph, a typical hyperparameter search might explore:</p>
<ul>
<li>Embedding dimensions: <span class="math inline">\(\{50, 100, 200\}\)</span></li>
<li>Learning rates: <span class="math inline">\(\{0.001, 0.01, 0.1\}\)</span></li>
<li>Margin values: <span class="math inline">\(\{1, 2, 5, 10\}\)</span></li>
<li>L1 vs.&nbsp;L2 norm for distance</li>
<li>Batch sizes: <span class="math inline">\(\{128, 256, 512\}\)</span></li>
<li>Negative samples per positive: <span class="math inline">\(\{1, 5, 10\}\)</span></li>
</ul>
<p>Each combination would be trained and evaluated on a validation set, with the best-performing configuration selected for final training.</p>
</div>
</section>
<section id="early-stopping-and-convergence-criteria" class="level2" data-number="8.9">
<h2 data-number="8.9" class="anchored" data-anchor-id="early-stopping-and-convergence-criteria"><span class="header-section-number">8.9</span> Early Stopping and Convergence Criteria</h2>
<p>Determining when to stop training is important to prevent overfitting while ensuring the model has learned effectively.</p>
<div id="def-early-stopping" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.18 (Early Stopping)</strong></span> &nbsp;</p>
<ol type="1">
<li>Evaluate the model on a validation set after each epoch or at regular intervals</li>
<li>Keep track of the model’s performance on the validation set</li>
<li>If performance doesn’t improve for <span class="math inline">\(N\)</span> consecutive evaluations, stop training</li>
<li>Return the model parameters that achieved the best validation performance</li>
</ol>
</div>
<p>Common validation metrics for KGE models include Mean Reciprocal Rank (MRR) and Hits@k.</p>
</section>
<section id="implementation-examples" class="level2" data-number="8.10">
<h2 data-number="8.10" class="anchored" data-anchor-id="implementation-examples"><span class="header-section-number">8.10</span> Implementation Examples</h2>
<p>Let’s look at a pseudo-code implementation of the training loop for a knowledge graph embedding model using margin-based ranking loss.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize entity and relation embeddings</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>entities <span class="op">=</span> initialize_embeddings(num_entities, embedding_dim)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>relations <span class="op">=</span> initialize_embeddings(num_relations, embedding_dim)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(max_epochs):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shuffle training triples</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    shuffled_triples <span class="op">=</span> shuffle(training_triples)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process mini-batches</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> create_batches(shuffled_triples, batch_size):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (h, r, t) <span class="kw">in</span> batch:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate negative samples</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            negative_samples <span class="op">=</span> generate_negatives(h, r, t, num_negatives)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute positive score</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            pos_score <span class="op">=</span> score_function(entities[h], relations[r], entities[t])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute loss for each negative sample</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (h_neg, r_neg, t_neg) <span class="kw">in</span> negative_samples:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                neg_score <span class="op">=</span> score_function(entities[h_neg], relations[r_neg], entities[t_neg])</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                sample_loss <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, margin <span class="op">+</span> pos_score <span class="op">-</span> neg_score)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                batch_loss <span class="op">+=</span> sample_loss</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gradients and update embeddings</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> compute_gradients(batch_loss)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        update_parameters(entities, relations, gradients, learning_rate)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply constraints (e.g., normalization)</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        normalize_embeddings(entities)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate on validation set</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    validation_metrics <span class="op">=</span> evaluate(validation_triples, entities, relations)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Early stopping check</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> early_stopping_criterion_met(validation_metrics):</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This pseudo-code illustrates the main components of the training process: batch creation, negative sampling, loss computation, parameter updates, and evaluation.</p>
</section>
<section id="scalability-considerations" class="level2" data-number="8.11">
<h2 data-number="8.11" class="anchored" data-anchor-id="scalability-considerations"><span class="header-section-number">8.11</span> Scalability Considerations</h2>
<p>Training KGE models on large knowledge graphs presents scalability challenges that require special consideration.</p>
<section id="parallel-and-distributed-training" class="level3" data-number="8.11.1">
<h3 data-number="8.11.1" class="anchored" data-anchor-id="parallel-and-distributed-training"><span class="header-section-number">8.11.1</span> Parallel and Distributed Training</h3>
<p>For very large knowledge graphs, training can be parallelized across multiple GPUs or machines.</p>
<div id="def-parallel-training" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.19 (Parallel Training for KGEs)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Data Parallelism</strong>: Distribute batches across multiple processors</li>
<li><strong>Model Parallelism</strong>: Partition the embedding matrices across multiple devices</li>
<li><strong>Distributed Training</strong>: Use parameter servers or all-reduce algorithms to synchronize updates</li>
</ol>
</div>
<p>Frameworks like DGL-KE and PyTorch-BigGraph implement distributed training strategies specifically for knowledge graph embeddings.</p>
</section>
<section id="memory-efficient-implementations" class="level3" data-number="8.11.2">
<h3 data-number="8.11.2" class="anchored" data-anchor-id="memory-efficient-implementations"><span class="header-section-number">8.11.2</span> Memory-Efficient Implementations</h3>
<p>Memory consumption is often the bottleneck when training KGE models on large graphs. Several techniques can reduce memory requirements:</p>
<div id="def-memory-efficient-techniques" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.20 (Memory-Efficient Training Techniques)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Sparse Embeddings</strong>: Only materialize embeddings needed for the current batch</li>
<li><strong>Mixed-Precision Training</strong>: Use lower precision (e.g., 16-bit floats) for embeddings</li>
<li><strong>Gradient Accumulation</strong>: Update parameters after multiple forward-backward passes</li>
<li><strong>Embedding Sharing</strong>: Use shared embeddings for similar entities or relations</li>
</ol>
</div>
<p>These techniques can significantly increase the scale of knowledge graphs that can be processed on given hardware.</p>
</section>
</section>
<section id="best-practices-and-common-pitfalls" class="level2" data-number="8.12">
<h2 data-number="8.12" class="anchored" data-anchor-id="best-practices-and-common-pitfalls"><span class="header-section-number">8.12</span> Best Practices and Common Pitfalls</h2>
<p>Based on experience from the research community, here are some best practices and common pitfalls to avoid when training KGE models:</p>
<section id="best-practices" class="level3" data-number="8.12.1">
<h3 data-number="8.12.1" class="anchored" data-anchor-id="best-practices"><span class="header-section-number">8.12.1</span> Best Practices</h3>
<ol type="1">
<li><strong>Start Simple</strong>: Begin with simpler models like TransE or DistMult before trying more complex architectures</li>
<li><strong>Validation Protocol</strong>: Use a consistent validation protocol for hyperparameter tuning</li>
<li><strong>Multiple Runs</strong>: Average results over multiple runs with different random seeds</li>
<li><strong>Learning Rate Scheduling</strong>: Gradually decrease the learning rate during training</li>
<li><strong>Batch Composition</strong>: Ensure batches contain diverse relation types</li>
<li><strong>Proper Evaluation</strong>: Use filtered metrics that exclude other valid triples</li>
</ol>
</section>
<section id="common-pitfalls" class="level3" data-number="8.12.2">
<h3 data-number="8.12.2" class="anchored" data-anchor-id="common-pitfalls"><span class="header-section-number">8.12.2</span> Common Pitfalls</h3>
<ol type="1">
<li><strong>Inappropriate Negative Sampling</strong>: Using the same strategy for all relation types</li>
<li><strong>Overlooking Regularization</strong>: Neglecting regularization can lead to overfitting</li>
<li><strong>Inconsistent Evaluation</strong>: Using different protocols when comparing models</li>
<li><strong>Premature Stopping</strong>: Stopping training too early before convergence</li>
<li><strong>Ignoring Relation Properties</strong>: Not considering the characteristics of different relations</li>
<li><strong>Overtuning on Test</strong>: Inadvertently tuning hyperparameters based on test set performance</li>
</ol>
</section>
</section>
<section id="conclusion" class="level2" data-number="8.13">
<h2 data-number="8.13" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">8.13</span> Conclusion</h2>
<p>Effective training of knowledge graph embedding models requires careful consideration of multiple factors: loss functions, negative sampling strategies, optimization algorithms, regularization techniques, and hyperparameter tuning. These aspects often have as much impact on the final performance as the model architecture itself.</p>
<p>The field continues to evolve, with new training techniques being developed to improve efficiency, scalability, and performance. As knowledge graphs grow in size and importance, these training considerations become increasingly crucial for practical applications.</p>
<p>In the next chapter, we will discuss how to properly evaluate knowledge graph embedding models, including metrics, protocols, datasets, and common pitfalls in evaluation.</p>
</section>
<section id="further-reading" class="level2" data-number="8.14">
<h2 data-number="8.14" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">8.14</span> Further Reading</h2>
<ol type="1">
<li>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. NIPS.</li>
<li>Sun, Z., Deng, Z. H., Nie, J. Y., &amp; Tang, J. (2019). RotatE: Knowledge graph embedding by relational rotation in complex space. ICLR.</li>
<li>Ruffinelli, D., Broscheit, S., &amp; Gemulla, R. (2020). You CAN teach an old dog new tricks! On training knowledge graph embeddings. ICLR.</li>
<li>Trouillon, T., Dance, C. R., Gaussier, É., Welbl, J., Riedel, S., &amp; Bouchard, G. (2017). Knowledge graph completion via complex tensor factorization. JMLR.</li>
<li>Wang, Y., Gemulla, R., &amp; Li, H. (2018). On multi-relational link prediction with bilinear models. AAAI.</li>
<li>Zheng, D., Song, X., Ma, C., Tan, Z., Ye, Z., Dong, J., Xiong, H., Zhang, Z., &amp; Karypis, G. (2020). DGL-KE: Training knowledge graph embeddings at scale. SIGIR.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/rotation-nn.html" class="pagination-link" aria-label="Advanced Models: Rotations and Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/evaluation.html" class="pagination-link" aria-label="Evaluation Methodologies and Benchmarks">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>