<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Semantic Matching Models – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/rotation-nn.html" rel="next">
<link href="../content/translation-based.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/semantic-matching.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-semantic-matching-principle" id="toc-the-semantic-matching-principle" class="nav-link active" data-scroll-target="#the-semantic-matching-principle"><span class="header-section-number">6.1</span> The semantic matching principle</a></li>
  <li><a href="#rescal-a-tensor-factorization-approach" id="toc-rescal-a-tensor-factorization-approach" class="nav-link" data-scroll-target="#rescal-a-tensor-factorization-approach"><span class="header-section-number">6.2</span> RESCAL: a tensor factorization approach</a>
  <ul class="collapse">
  <li><a href="#tensor-factorization-interpretation" id="toc-tensor-factorization-interpretation" class="nav-link" data-scroll-target="#tensor-factorization-interpretation"><span class="header-section-number">6.2.1</span> Tensor factorization interpretation</a></li>
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation"><span class="header-section-number">6.2.2</span> Geometric interpretation</a></li>
  <li><a href="#learning-algorithm" id="toc-learning-algorithm" class="nav-link" data-scroll-target="#learning-algorithm"><span class="header-section-number">6.2.3</span> Learning algorithm</a></li>
  <li><a href="#strengths-and-limitations-of-rescal" id="toc-strengths-and-limitations-of-rescal" class="nav-link" data-scroll-target="#strengths-and-limitations-of-rescal"><span class="header-section-number">6.2.4</span> Strengths and limitations of RESCAL</a></li>
  </ul></li>
  <li><a href="#distmult-a-simplified-bilinear-model" id="toc-distmult-a-simplified-bilinear-model" class="nav-link" data-scroll-target="#distmult-a-simplified-bilinear-model"><span class="header-section-number">6.3</span> DistMult: a simplified bilinear model</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-1" id="toc-geometric-interpretation-1" class="nav-link" data-scroll-target="#geometric-interpretation-1"><span class="header-section-number">6.3.1</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-distmult" id="toc-strengths-and-limitations-of-distmult" class="nav-link" data-scroll-target="#strengths-and-limitations-of-distmult"><span class="header-section-number">6.3.2</span> Strengths and limitations of DistMult</a></li>
  </ul></li>
  <li><a href="#canonical-polyadic-cp-decomposition" id="toc-canonical-polyadic-cp-decomposition" class="nav-link" data-scroll-target="#canonical-polyadic-cp-decomposition"><span class="header-section-number">6.4</span> Canonical Polyadic (CP) decomposition</a></li>
  <li><a href="#complex-complex-embeddings-for-asymmetric-relations" id="toc-complex-complex-embeddings-for-asymmetric-relations" class="nav-link" data-scroll-target="#complex-complex-embeddings-for-asymmetric-relations"><span class="header-section-number">6.5</span> ComplEx: complex embeddings for asymmetric relations</a>
  <ul class="collapse">
  <li><a href="#complex-number-representation" id="toc-complex-number-representation" class="nav-link" data-scroll-target="#complex-number-representation"><span class="header-section-number">6.5.1</span> Complex number representation</a></li>
  <li><a href="#geometric-interpretation-2" id="toc-geometric-interpretation-2" class="nav-link" data-scroll-target="#geometric-interpretation-2"><span class="header-section-number">6.5.2</span> Geometric interpretation</a></li>
  <li><a href="#connection-to-distmult" id="toc-connection-to-distmult" class="nav-link" data-scroll-target="#connection-to-distmult"><span class="header-section-number">6.5.3</span> Connection to DistMult</a></li>
  <li><a href="#strengths-and-limitations-of-complex" id="toc-strengths-and-limitations-of-complex" class="nav-link" data-scroll-target="#strengths-and-limitations-of-complex"><span class="header-section-number">6.5.4</span> Strengths and limitations of ComplEx</a></li>
  </ul></li>
  <li><a href="#hole-holographic-embeddings" id="toc-hole-holographic-embeddings" class="nav-link" data-scroll-target="#hole-holographic-embeddings"><span class="header-section-number">6.6</span> HolE: holographic embeddings</a>
  <ul class="collapse">
  <li><a href="#circular-correlation" id="toc-circular-correlation" class="nav-link" data-scroll-target="#circular-correlation"><span class="header-section-number">6.6.1</span> Circular correlation</a></li>
  <li><a href="#connection-to-complex" id="toc-connection-to-complex" class="nav-link" data-scroll-target="#connection-to-complex"><span class="header-section-number">6.6.2</span> Connection to ComplEx</a></li>
  <li><a href="#strengths-and-limitations-of-hole" id="toc-strengths-and-limitations-of-hole" class="nav-link" data-scroll-target="#strengths-and-limitations-of-hole"><span class="header-section-number">6.6.3</span> Strengths and limitations of HolE</a></li>
  </ul></li>
  <li><a href="#simple-a-canonical-tensor-decomposition-approach" id="toc-simple-a-canonical-tensor-decomposition-approach" class="nav-link" data-scroll-target="#simple-a-canonical-tensor-decomposition-approach"><span class="header-section-number">6.7</span> SimplE: a canonical tensor decomposition approach</a>
  <ul class="collapse">
  <li><a href="#parameter-sharing-and-inversion-relations" id="toc-parameter-sharing-and-inversion-relations" class="nav-link" data-scroll-target="#parameter-sharing-and-inversion-relations"><span class="header-section-number">6.7.1</span> Parameter sharing and inversion relations</a></li>
  <li><a href="#strengths-and-limitations-of-simple" id="toc-strengths-and-limitations-of-simple" class="nav-link" data-scroll-target="#strengths-and-limitations-of-simple"><span class="header-section-number">6.7.2</span> Strengths and limitations of SimplE</a></li>
  </ul></li>
  <li><a href="#tucker-a-tensor-decomposition-model" id="toc-tucker-a-tensor-decomposition-model" class="nav-link" data-scroll-target="#tucker-a-tensor-decomposition-model"><span class="header-section-number">6.8</span> TuckER: a tensor decomposition model</a>
  <ul class="collapse">
  <li><a href="#tucker-decomposition" id="toc-tucker-decomposition" class="nav-link" data-scroll-target="#tucker-decomposition"><span class="header-section-number">6.8.1</span> Tucker decomposition</a></li>
  <li><a href="#connections-to-other-models" id="toc-connections-to-other-models" class="nav-link" data-scroll-target="#connections-to-other-models"><span class="header-section-number">6.8.2</span> Connections to other models</a></li>
  <li><a href="#strengths-and-limitations-of-tucker" id="toc-strengths-and-limitations-of-tucker" class="nav-link" data-scroll-target="#strengths-and-limitations-of-tucker"><span class="header-section-number">6.8.3</span> Strengths and limitations of TuckER</a></li>
  </ul></li>
  <li><a href="#quate-quaternion-embeddings" id="toc-quate-quaternion-embeddings" class="nav-link" data-scroll-target="#quate-quaternion-embeddings"><span class="header-section-number">6.9</span> QuatE: quaternion embeddings</a>
  <ul class="collapse">
  <li><a href="#quaternion-algebra" id="toc-quaternion-algebra" class="nav-link" data-scroll-target="#quaternion-algebra"><span class="header-section-number">6.9.1</span> Quaternion algebra</a></li>
  <li><a href="#geometric-interpretation-3" id="toc-geometric-interpretation-3" class="nav-link" data-scroll-target="#geometric-interpretation-3"><span class="header-section-number">6.9.2</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-quate" id="toc-strengths-and-limitations-of-quate" class="nav-link" data-scroll-target="#strengths-and-limitations-of-quate"><span class="header-section-number">6.9.3</span> Strengths and limitations of QuatE</a></li>
  </ul></li>
  <li><a href="#extensions-and-variations" id="toc-extensions-and-variations" class="nav-link" data-scroll-target="#extensions-and-variations"><span class="header-section-number">6.10</span> Extensions and variations</a>
  <ul class="collapse">
  <li><a href="#regularization-approaches" id="toc-regularization-approaches" class="nav-link" data-scroll-target="#regularization-approaches"><span class="header-section-number">6.10.1</span> Regularization approaches</a></li>
  <li><a href="#multi-modal-extensions" id="toc-multi-modal-extensions" class="nav-link" data-scroll-target="#multi-modal-extensions"><span class="header-section-number">6.10.2</span> Multi-modal extensions</a></li>
  <li><a href="#type-aware-models" id="toc-type-aware-models" class="nav-link" data-scroll-target="#type-aware-models"><span class="header-section-number">6.10.3</span> Type-aware models</a></li>
  </ul></li>
  <li><a href="#relation-patterns-and-model-capabilities" id="toc-relation-patterns-and-model-capabilities" class="nav-link" data-scroll-target="#relation-patterns-and-model-capabilities"><span class="header-section-number">6.11</span> Relation patterns and model capabilities</a></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations"><span class="header-section-number">6.12</span> Implementation considerations</a>
  <ul class="collapse">
  <li><a href="#training-objectives" id="toc-training-objectives" class="nav-link" data-scroll-target="#training-objectives"><span class="header-section-number">6.12.1</span> Training objectives</a></li>
  <li><a href="#initialization-strategies" id="toc-initialization-strategies" class="nav-link" data-scroll-target="#initialization-strategies"><span class="header-section-number">6.12.2</span> Initialization strategies</a></li>
  <li><a href="#regularization-and-constraints" id="toc-regularization-and-constraints" class="nav-link" data-scroll-target="#regularization-and-constraints"><span class="header-section-number">6.12.3</span> Regularization and constraints</a></li>
  </ul></li>
  <li><a href="#performance-analysis" id="toc-performance-analysis" class="nav-link" data-scroll-target="#performance-analysis"><span class="header-section-number">6.13</span> Performance analysis</a></li>
  <li><a href="#applications-of-semantic-matching-models" id="toc-applications-of-semantic-matching-models" class="nav-link" data-scroll-target="#applications-of-semantic-matching-models"><span class="header-section-number">6.14</span> Applications of semantic matching models</a>
  <ul class="collapse">
  <li><a href="#question-answering" id="toc-question-answering" class="nav-link" data-scroll-target="#question-answering"><span class="header-section-number">6.14.1</span> Question answering</a></li>
  <li><a href="#recommendation-systems" id="toc-recommendation-systems" class="nav-link" data-scroll-target="#recommendation-systems"><span class="header-section-number">6.14.2</span> Recommendation systems</a></li>
  <li><a href="#information-extraction" id="toc-information-extraction" class="nav-link" data-scroll-target="#information-extraction"><span class="header-section-number">6.14.3</span> Information extraction</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions"><span class="header-section-number">6.15</span> Future directions</a>
  <ul class="collapse">
  <li><a href="#neural-architecture-integration" id="toc-neural-architecture-integration" class="nav-link" data-scroll-target="#neural-architecture-integration"><span class="header-section-number">6.15.1</span> Neural architecture integration</a></li>
  <li><a href="#pre-trained-language-model-integration" id="toc-pre-trained-language-model-integration" class="nav-link" data-scroll-target="#pre-trained-language-model-integration"><span class="header-section-number">6.15.2</span> Pre-trained language model integration</a></li>
  <li><a href="#inductive-learning" id="toc-inductive-learning" class="nav-link" data-scroll-target="#inductive-learning"><span class="header-section-number">6.15.3</span> Inductive learning</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6.16</span> Summary</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">6.17</span> Further reading</a>
  <ul class="collapse">
  <li><a href="#original-papers" id="toc-original-papers" class="nav-link" data-scroll-target="#original-papers"><span class="header-section-number">6.17.1</span> Original papers</a></li>
  <li><a href="#surveys-and-comparative-analyses" id="toc-surveys-and-comparative-analyses" class="nav-link" data-scroll-target="#surveys-and-comparative-analyses"><span class="header-section-number">6.17.2</span> Surveys and comparative analyses</a></li>
  <li><a href="#mathematical-foundations" id="toc-mathematical-foundations" class="nav-link" data-scroll-target="#mathematical-foundations"><span class="header-section-number">6.17.3</span> Mathematical foundations</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>In the previous chapter, we explored translation-based models that interpret relations as geometric displacements in the embedding space. While these models provide an intuitive geometric interpretation, they have limitations in modeling certain relation patterns. Semantic matching models take a different approach, focusing on measuring the plausibility of facts through similarity functions that capture the semantic relationships between entities.</p>
<p>Semantic matching models represent a diverse family of knowledge graph embedding approaches that evaluate the compatibility between entities based on their representations and relation-specific parameters. Instead of enforcing a rigid geometric constraint like translation, these models learn more flexible similarity functions that can capture complex relation patterns. This paradigm includes influential models like RESCAL, DistMult, ComplEx, and others that have achieved state-of-the-art performance on knowledge graph completion tasks.</p>
<p>In this chapter, we’ll delve into the foundations of semantic matching models, exploring their mathematical formulations, geometric interpretations, and practical applications. We’ll examine how these models evolved to address the limitations of translation-based approaches and how they capture different relation patterns. By the end of this chapter, you’ll understand the principles behind semantic matching models and their strengths and limitations compared to translation-based approaches.</p>
<section id="the-semantic-matching-principle" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="the-semantic-matching-principle"><span class="header-section-number">6.1</span> The semantic matching principle</h2>
<p>Unlike translation-based models that enforce a specific geometric relationship between head and tail entities, semantic matching models focus on measuring the compatibility or similarity between entities in the context of a specific relation:</p>
<div id="def-semantic-matching" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 (Semantic matching principle)</strong></span> In <strong>semantic matching models</strong>, the plausibility of a triple <span class="math inline">\((h, r, t)\)</span> is measured using a scoring function that evaluates the compatibility between the head entity <span class="math inline">\(h\)</span> and tail entity <span class="math inline">\(t\)</span> in the context of relation <span class="math inline">\(r\)</span>. This compatibility is typically expressed as a similarity or matching function:</p>
<p><span class="math display">\[f_r(h, t) = sim_r(\mathbf{h}, \mathbf{t})\]</span></p>
<p>where <span class="math inline">\(sim_r\)</span> is a relation-specific similarity function, and <span class="math inline">\(\mathbf{h}, \mathbf{t}\)</span> are the embedding vectors of entities <span class="math inline">\(h\)</span> and <span class="math inline">\(t\)</span>, respectively.</p>
<p>Higher scores indicate more plausible triples.</p>
</div>
<p>The key differences between semantic matching models and translation-based models are:</p>
<ol type="1">
<li><strong>Objective</strong>: Semantic matching models focus on matching or similarity rather than geometric transformations</li>
<li><strong>Direction</strong>: Many semantic matching models are inherently symmetric, though asymmetric variants exist</li>
<li><strong>Interpretation</strong>: The scoring function measures compatibility rather than distance</li>
</ol>
<div id="exm-semantic-matching" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 (Semantic matching example)</strong></span> Consider a knowledge graph about movies, with entities like “The Godfather,” “Francis Ford Coppola,” and “Crime” and relations like “directed_by” and <code>"has_genre"</code></p>
<p>In a semantic matching model:</p>
<ul>
<li>The plausibility of (The_Godfather, directed_by, Francis_Ford_Coppola) would be measured by how well the embeddings of “The Godfather” and “Francis Ford Coppola” match in the context of the “directed_by” relation.</li>
<li>Similarly, the plausibility of (The_Godfather, has_genre, Crime) would be measured by how well the embeddings of “The Godfather” and “Crime” match in the context of the <code>"has_genre"</code> relation.</li>
</ul>
<p>These matchings are learned from data rather than enforced through a geometric constraint like translation.</p>
</div>
</section>
<section id="rescal-a-tensor-factorization-approach" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="rescal-a-tensor-factorization-approach"><span class="header-section-number">6.2</span> RESCAL: a tensor factorization approach</h2>
<p>RESCAL, introduced by Nickel et al.&nbsp;(2011), was one of the first semantic matching models for knowledge graph embeddings. It approaches the problem from a tensor factorization perspective, where the knowledge graph is represented as a third-order tensor.</p>
<div id="def-rescal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 (RESCAL model)</strong></span> In the <strong>RESCAL</strong> model, entities are represented as vectors <span class="math inline">\(\mathbf{e} \in \mathbb{R}^d\)</span>, and each relation <span class="math inline">\(r\)</span> is represented as a matrix <span class="math inline">\(\mathbf{W}_r \in \mathbb{R}^{d \times d}\)</span>.</p>
<p>The scoring function is a bilinear form: <span class="math display">\[f_r(h, t) = \mathbf{h}^T \mathbf{W}_r \mathbf{t}\]</span></p>
<p>This can be viewed as a tensor decomposition of the knowledge graph tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{|E| \times |R| \times |E|}\)</span>, where <span class="math inline">\(\mathcal{X}_{hrt} = 1\)</span> if the triple <span class="math inline">\((h, r, t)\)</span> exists, and 0 otherwise.</p>
</div>
<section id="tensor-factorization-interpretation" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="tensor-factorization-interpretation"><span class="header-section-number">6.2.1</span> Tensor factorization interpretation</h3>
<p>RESCAL can be viewed as a tensor factorization approach:</p>
<div id="def-tensor-factorization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 (Tensor factorization)</strong></span> A <strong>tensor factorization</strong> decomposes a tensor into lower-dimensional factors. For a third-order tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I \times J \times K}\)</span>, the RESCAL decomposition is:</p>
<p><span class="math display">\[\mathcal{X}_{ijk} \approx \sum_{f=1}^{F} a_{if} b_{jf} c_{kf}\]</span></p>
<p>In the context of knowledge graphs, this becomes: <span class="math display">\[\mathcal{X}_{hrt} \approx \mathbf{h}^T \mathbf{W}_r \mathbf{t}\]</span></p>
<p>where the entity embedding matrix <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{|E| \times d}\)</span> contains the entity embeddings, and the relation matrices <span class="math inline">\(\mathbf{W}_r \in \mathbb{R}^{d \times d}\)</span> capture relation-specific interactions.</p>
</div>
</section>
<section id="geometric-interpretation" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="geometric-interpretation"><span class="header-section-number">6.2.2</span> Geometric interpretation</h3>
<p>The RESCAL model has a rich geometric interpretation:</p>
<div id="def-rescal-geometry" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4 (RESCAL geometric interpretation)</strong></span> In RESCAL, the relation matrix <span class="math inline">\(\mathbf{W}_r\)</span> can be decomposed using eigendecomposition: <span class="math display">\[\mathbf{W}_r = \mathbf{V}_r \mathbf{\Lambda}_r \mathbf{V}_r^{-1}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Lambda}_r\)</span> is a diagonal matrix of eigenvalues and <span class="math inline">\(\mathbf{V}_r\)</span> contains the eigenvectors.</p>
<p>This decomposition reveals that <span class="math inline">\(\mathbf{W}_r\)</span> represents a linear transformation in the embedding space, which can include:</p>
<ol type="1">
<li><strong>Scaling</strong>: Changing the magnitude along certain directions</li>
<li><strong>Rotation</strong>: Changing the orientation</li>
<li><strong>Reflection</strong>: Flipping across hyperplanes</li>
<li><strong>Shearing</strong>: Deforming the space along specific directions</li>
</ol>
</div>
<div id="exm-rescal-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 (RESCAL geometric example)</strong></span> Consider a relation <code>"is_parent_of"</code> with a relation matrix <span class="math inline">\(\mathbf{W}_{\text{is\_parent\_of}}\)</span>. This matrix might encode:</p>
<ol type="1">
<li><strong>Directionality</strong>: Parents and children are in different regions of the embedding space</li>
<li><strong>Age correlation</strong>: The transformation captures the age difference between parents and children</li>
<li><strong>Genetic similarity</strong>: The transformation preserves certain dimensions that represent genetic traits</li>
</ol>
<p>When we compute <span class="math inline">\(\mathbf{h}^T \mathbf{W}_{\text{is\_parent\_of}} \mathbf{t}\)</span>, we’re measuring how well the head entity <span class="math inline">\(h\)</span> matches the pattern of being a parent to the tail entity <span class="math inline">\(t\)</span>, based on these learned transformations.</p>
</div>
</section>
<section id="learning-algorithm" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="learning-algorithm"><span class="header-section-number">6.2.3</span> Learning algorithm</h3>
<p>RESCAL is typically trained using alternating least squares (ALS) or stochastic gradient descent (SGD) methods:</p>
<div id="def-rescal-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.5 (RESCAL learning)</strong></span> <strong>Alternating Least Squares (ALS) approach</strong>:</p>
<ol type="1">
<li>Fix entity embeddings <span class="math inline">\(\mathbf{E}\)</span> and optimize relation matrices <span class="math inline">\(\mathbf{W}_r\)</span></li>
<li>Fix relation matrices <span class="math inline">\(\mathbf{W}_r\)</span> and optimize entity embeddings <span class="math inline">\(\mathbf{E}\)</span></li>
<li>Repeat until convergence</li>
</ol>
<p><strong>Gradient-based approach</strong>:</p>
<ol type="1">
<li>Initialize entity embeddings <span class="math inline">\(\mathbf{E}\)</span> and relation matrices <span class="math inline">\(\mathbf{W}_r\)</span></li>
<li>Define a loss function, typically margin-based ranking loss: <span class="math display">\[L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma - f_r(h, t) + f_r(h', t')]_+\]</span></li>
<li>Update parameters using gradient descent</li>
<li>Apply regularization to prevent overfitting: <span class="math display">\[L_{\text{reg}} = L + \lambda_{\mathbf{E}} \|\mathbf{E}\|_F^2 + \lambda_{\mathbf{W}} \sum_r \|\mathbf{W}_r\|_F^2\]</span> where <span class="math inline">\(\|\cdot\|_F\)</span> denotes the Frobenius norm</li>
</ol>
</div>
</section>
<section id="strengths-and-limitations-of-rescal" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="strengths-and-limitations-of-rescal"><span class="header-section-number">6.2.4</span> Strengths and limitations of RESCAL</h3>
<p>RESCAL has several strengths:</p>
<ol type="1">
<li><strong>Expressiveness</strong>: The bilinear form can capture complex interaction patterns between entities</li>
<li><strong>Interpretability</strong>: The relation matrices have a clear geometric interpretation as linear transformations</li>
<li><strong>Flexibility</strong>: RESCAL can model various relation patterns, including symmetry and antisymmetry</li>
</ol>
<p>However, RESCAL also has important limitations:</p>
<ol type="1">
<li><strong>Quadratic complexity</strong>: With <span class="math inline">\(O(|E|d + |R|d^2)\)</span> parameters, RESCAL is computationally expensive for large knowledge graphs or high embedding dimensions</li>
<li><strong>Overfitting risk</strong>: The high number of parameters can lead to overfitting, especially for relations with few training examples</li>
<li><strong>Training difficulty</strong>: The optimization problem is more complex than for simpler models like TransE</li>
</ol>
<div id="exm-rescal-complexity" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 (RESCAL complexity example)</strong></span> Consider a knowledge graph with 100,000 entities, 1,000 relations, and an embedding dimension of 100:</p>
<ul>
<li>Entity embeddings: 100,000 × 100 = 10,000,000 parameters</li>
<li>Relation matrices: 1,000 × 100 × 100 = 10,000,000 parameters</li>
<li>Total: 20,000,000 parameters</li>
</ul>
<p>This high parameter count can lead to overfitting and computational challenges, especially for large knowledge graphs.</p>
</div>
</section>
</section>
<section id="distmult-a-simplified-bilinear-model" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="distmult-a-simplified-bilinear-model"><span class="header-section-number">6.3</span> DistMult: a simplified bilinear model</h2>
<p>DistMult, proposed by Yang et al.&nbsp;(2015), simplifies RESCAL by restricting the relation matrices to be diagonal, significantly reducing the number of parameters while maintaining reasonable expressiveness.</p>
<div id="def-distmult" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.6 (DistMult model)</strong></span> In the <strong>DistMult</strong> model, entities are represented as vectors <span class="math inline">\(\mathbf{e} \in \mathbb{R}^d\)</span>, and each relation <span class="math inline">\(r\)</span> is represented as a vector <span class="math inline">\(\mathbf{r} \in \mathbb{R}^d\)</span> that corresponds to the diagonal of a diagonal matrix.</p>
<p>The scoring function is: <span class="math display">\[f_r(h, t) = \mathbf{h}^T \text{diag}(\mathbf{r}) \mathbf{t} = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i\]</span></p>
<p>where <span class="math inline">\(\text{diag}(\mathbf{r})\)</span> is a diagonal matrix with the elements of <span class="math inline">\(\mathbf{r}\)</span> on its diagonal.</p>
</div>
<section id="geometric-interpretation-1" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="geometric-interpretation-1"><span class="header-section-number">6.3.1</span> Geometric interpretation</h3>
<p>DistMult has a simpler geometric interpretation than RESCAL:</p>
<div id="def-distmult-geometry" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.7 (DistMult geometric interpretation)</strong></span> In DistMult, the diagonal relation matrix <span class="math inline">\(\text{diag}(\mathbf{r})\)</span> represents a dimension-wise scaling of the embedding space. Each component <span class="math inline">\(r_i\)</span> determines the importance of dimension <span class="math inline">\(i\)</span> for relation <span class="math inline">\(r\)</span>.</p>
<p>The scoring function can be viewed as a weighted element-wise product: <span class="math display">\[f_r(h, t) = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i = \langle \mathbf{h} \odot \mathbf{r}, \mathbf{t} \rangle\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes the element-wise (Hadamard) product, and <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span> is the dot product.</p>
</div>
<div id="exm-distmult-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 (DistMult geometric example)</strong></span> Consider a relation <code>"has_profession"</code> with relation vector <span class="math inline">\(\mathbf{r}_{\text{has\_profession}} = [0.9, 0.1, 0.7, 0.2, ...]\)</span>.</p>
<p>The high values (0.9, 0.7) indicate dimensions that are important for the <code>"has_profession"</code> relation, while low values (0.1, 0.2) indicate dimensions that are less relevant.</p>
<p>When computing the score of (Person, has_profession, Doctor), the model emphasizes dimensions where Person and Doctor have similar values and where the relation vector has high values.</p>
</div>
</section>
<section id="strengths-and-limitations-of-distmult" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="strengths-and-limitations-of-distmult"><span class="header-section-number">6.3.2</span> Strengths and limitations of DistMult</h3>
<p>DistMult offers several advantages over RESCAL:</p>
<ol type="1">
<li><strong>Parameter efficiency</strong>: With <span class="math inline">\(O(|E|d + |R|d)\)</span> parameters, DistMult is much more efficient than RESCAL</li>
<li><strong>Simplicity</strong>: The simpler model is easier to train and less prone to overfitting</li>
<li><strong>Interpretability</strong>: The relation vectors directly indicate the importance of each dimension for a relation</li>
</ol>
<p>However, DistMult has a critical limitation:</p>
<ol type="1">
<li><strong>Symmetry constraint</strong>: The scoring function is symmetric: <span class="math inline">\(f_r(h, t) = f_r(t, h)\)</span>, which means DistMult cannot distinguish between triples <span class="math inline">\((h, r, t)\)</span> and <span class="math inline">\((t, r, h)\)</span>. This is problematic for antisymmetric relations, which are common in knowledge graphs.</li>
</ol>
<div id="exm-distmult-symmetry" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 (DistMult symmetry limitation example)</strong></span> Consider the relation <code>"is_capital_of"</code> and the triple (Paris, is_capital_of, France).</p>
<p>Since DistMult’s scoring function is symmetric, it assigns the same score to:</p>
<ul>
<li>(Paris, is_capital_of, France)</li>
<li>(France, is_capital_of, Paris)</li>
</ul>
<p>However, while the first triple is correct, the second is incorrect. DistMult cannot distinguish between these cases because of its inherent symmetry.</p>
</div>
</section>
</section>
<section id="canonical-polyadic-cp-decomposition" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="canonical-polyadic-cp-decomposition"><span class="header-section-number">6.4</span> Canonical Polyadic (CP) decomposition</h2>
<p>The Canonical Polyadic (CP) decomposition is another tensor factorization approach that has been applied to knowledge graph embedding.</p>
<div id="def-cp-decomposition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.8 (CP decomposition model)</strong></span> In the <strong>CP decomposition</strong> model, each entity and relation is represented by multiple vectors:</p>
<ol type="1">
<li>A head entity <span class="math inline">\(h\)</span> is represented by <span class="math inline">\(\mathbf{h}_{\text{head}} \in \mathbb{R}^d\)</span></li>
<li>A tail entity <span class="math inline">\(t\)</span> is represented by <span class="math inline">\(\mathbf{t}_{\text{tail}} \in \mathbb{R}^d\)</span></li>
<li>A relation <span class="math inline">\(r\)</span> is represented by <span class="math inline">\(\mathbf{r} \in \mathbb{R}^d\)</span></li>
</ol>
<p>The scoring function is: <span class="math display">\[f_r(h, t) = \sum_{i=1}^d \mathbf{h}_{\text{head},i} \mathbf{r}_i \mathbf{t}_{\text{tail},i}\]</span></p>
<p>This can be viewed as a tensor factorization where the knowledge graph tensor <span class="math inline">\(\mathcal{X}\)</span> is decomposed into three factor matrices.</p>
</div>
<p>The key difference between CP decomposition and DistMult is that CP uses different representations for entities depending on whether they appear as head or tail, which allows it to model asymmetric relations.</p>
<div id="exm-cp-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 (CP decomposition example)</strong></span> Consider the relation <code>"is_capital_of"</code> and the triples:</p>
<ul>
<li>(Paris, is_capital_of, France)</li>
<li>(Berlin, is_capital_of, Germany)</li>
</ul>
<p>In CP decomposition, the entities would have separate head and tail embeddings:</p>
<ul>
<li><span class="math inline">\(\mathbf{Paris}_{\text{head}}\)</span> and <span class="math inline">\(\mathbf{Paris}_{\text{tail}}\)</span></li>
<li><span class="math inline">\(\mathbf{France}_{\text{head}}\)</span> and <span class="math inline">\(\mathbf{France}_{\text{tail}}\)</span></li>
<li><span class="math inline">\(\mathbf{Berlin}_{\text{head}}\)</span> and <span class="math inline">\(\mathbf{Berlin}_{\text{tail}}\)</span></li>
<li><span class="math inline">\(\mathbf{Germany}_{\text{head}}\)</span> and <span class="math inline">\(\mathbf{Germany}_{\text{tail}}\)</span></li>
</ul>
<p>The scores would be computed as: <span class="math inline">\(f_{\text{is\_capital\_of}}(\text{Paris}, \text{France}) = \sum_{i=1}^d \mathbf{Paris}_{\text{head},i} \cdot \mathbf{is\_capital\_of}_i \cdot \mathbf{France}_{\text{tail},i}\)</span></p>
<p><span class="math inline">\(f_{\text{is\_capital\_of}}(\text{France}, \text{Paris}) = \sum_{i=1}^d \mathbf{France}_{\text{head},i} \cdot \mathbf{is\_capital\_of}_i \cdot \mathbf{Paris}_{\text{tail},i}\)</span></p>
<p>Since the head and tail embeddings are different, these scores can be different, allowing the model to capture asymmetric relations.</p>
</div>
</section>
<section id="complex-complex-embeddings-for-asymmetric-relations" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="complex-complex-embeddings-for-asymmetric-relations"><span class="header-section-number">6.5</span> ComplEx: complex embeddings for asymmetric relations</h2>
<p>ComplEx, proposed by Trouillon et al.&nbsp;(2016), addresses the symmetry limitation of DistMult by using complex-valued embeddings.</p>
<div id="def-complex" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.9 (ComplEx model)</strong></span> In the <strong>ComplEx</strong> model, entities and relations are embedded in complex space: <span class="math inline">\(\mathbf{e}, \mathbf{r} \in \mathbb{C}^d\)</span>.</p>
<p>The scoring function is: <span class="math display">\[f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \text{Re}\left( \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \overline{\mathbf{t}_i} \right)\]</span></p>
<p>where <span class="math inline">\(\overline{\mathbf{t}}\)</span> denotes the complex conjugate of <span class="math inline">\(\mathbf{t}\)</span>, and <span class="math inline">\(\text{Re}(z)\)</span> is the real part of the complex number <span class="math inline">\(z\)</span>.</p>
</div>
<section id="complex-number-representation" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="complex-number-representation"><span class="header-section-number">6.5.1</span> Complex number representation</h3>
<p>In ComplEx, each embedding component is a complex number with real and imaginary parts:</p>
<div id="def-complex-numbers" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.10 (Complex numbers)</strong></span> A <strong>complex number</strong> <span class="math inline">\(z = a + bi\)</span> consists of:</p>
<ul>
<li>A real part <span class="math inline">\(a \in \mathbb{R}\)</span></li>
<li>An imaginary part <span class="math inline">\(b \in \mathbb{R}\)</span></li>
<li>The imaginary unit <span class="math inline">\(i = \sqrt{-1}\)</span></li>
</ul>
<p>Key operations include:</p>
<ol type="1">
<li><strong>Complex conjugate</strong>: <span class="math inline">\(\overline{z} = a - bi\)</span></li>
<li><strong>Modulus</strong>: <span class="math inline">\(|z| = \sqrt{a^2 + b^2}\)</span></li>
<li><strong>Multiplication</strong>: <span class="math inline">\((a + bi)(c + di) = (ac - bd) + (ad + bc)i\)</span></li>
</ol>
</div>
</section>
<section id="geometric-interpretation-2" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="geometric-interpretation-2"><span class="header-section-number">6.5.2</span> Geometric interpretation</h3>
<p>ComplEx has a rich geometric interpretation in terms of rotations and reflections in the complex plane:</p>
<div id="def-complex-geometry" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.11 (ComplEx geometric interpretation)</strong></span> In ComplEx, the scoring function can be rewritten as: <span class="math display">\[f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \langle \text{Re}(\mathbf{h} \odot \mathbf{r}), \text{Re}(\mathbf{t}) \rangle + \langle \text{Im}(\mathbf{h} \odot \mathbf{r}), \text{Im}(\mathbf{t}) \rangle\]</span></p>
<p>This reveals that ComplEx can be viewed as two separate bilinear forms in the real and imaginary parts, allowing it to model both symmetric and antisymmetric components of relations.</p>
<p>The asymmetry in ComplEx comes from the use of the complex conjugate of the tail entity, which introduces an asymmetry in the scoring function: <span class="math inline">\(f_r(h, t) \neq f_r(t, h)\)</span> in general.</p>
</div>
<div id="exm-complex-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.7 (ComplEx geometric example)</strong></span> Consider a relation <code>"is_part_of"</code> with a complex embedding <span class="math inline">\(\mathbf{r}_{\text{is\_part\_of}} = [0.7 + 0.3i, 0.1 - 0.9i, ...]\)</span>.</p>
<p>For symmetric parts of the relation, the imaginary components would be close to zero, while for antisymmetric parts, the imaginary components would have larger magnitudes.</p>
<p>When computing the score of <code>(Hand, is_part_of, Body)</code>, the complex conjugate of the tail entity introduces asymmetry:</p>
<p><span class="math display">\[
f_{\text{is_part_of}}(\text{Hand}, \text{Body}) = \text{Re}(\langle \mathbf{Hand}, \mathbf{is_part_of}, \overline{\mathbf{Body}} \rangle)
\]</span></p>
<p>This score would be different from:</p>
<p><span class="math display">\[
f_{\text{is\_part\_of}}(\text{Body}, \text{Hand}) = \text{Re}(\langle \mathbf{Body}, \mathbf{is\_part\_of}, \overline{\mathbf{Hand}} \rangle)
\]</span></p>
<p>allowing the model to distinguish between these two cases.</p>
</div>
</section>
<section id="connection-to-distmult" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="connection-to-distmult"><span class="header-section-number">6.5.3</span> Connection to DistMult</h3>
<p>ComplEx can be viewed as a generalization of DistMult that addresses its symmetry limitation:</p>
<div id="def-complex-distmult" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.12 (ComplEx as an extension of DistMult)</strong></span> If all embeddings in ComplEx are restricted to have only real parts (zero imaginary components), then: <span class="math display">\[f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \langle \mathbf{h}, \mathbf{r}, \mathbf{t} \rangle = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i\]</span></p>
<p>This is exactly the DistMult scoring function. Therefore, DistMult is a special case of ComplEx when all embeddings are real-valued.</p>
<p>The addition of imaginary components in ComplEx allows it to model asymmetric relations, addressing DistMult’s key limitation.</p>
</div>
</section>
<section id="strengths-and-limitations-of-complex" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="strengths-and-limitations-of-complex"><span class="header-section-number">6.5.4</span> Strengths and limitations of ComplEx</h3>
<p>ComplEx offers several advantages:</p>
<ol type="1">
<li><strong>Expressiveness for asymmetric relations</strong>: ComplEx can model both symmetric and antisymmetric relations effectively</li>
<li><strong>Parameter efficiency</strong>: With <span class="math inline">\(O(2|E|d + 2|R|d)\)</span> parameters (considering real and imaginary parts separately), ComplEx is still relatively efficient</li>
<li><strong>Strong empirical performance</strong>: ComplEx achieves state-of-the-art results on various knowledge graph completion benchmarks</li>
</ol>
<p>However, ComplEx also has some limitations:</p>
<ol type="1">
<li><strong>Complex-valued computations</strong>: Working with complex numbers adds some computational overhead</li>
<li><strong>Interpretability</strong>: The complex-valued embeddings can be less intuitive to interpret than real-valued ones</li>
<li><strong>Limited expressiveness for certain relation patterns</strong>: While ComplEx handles symmetry and antisymmetry well, it may struggle with other relation patterns like composition</li>
</ol>
</section>
</section>
<section id="hole-holographic-embeddings" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="hole-holographic-embeddings"><span class="header-section-number">6.6</span> HolE: holographic embeddings</h2>
<p>Holographic Embeddings (HolE), introduced by Nickel et al.&nbsp;(2016), use circular correlation to capture interactions between entities, inspired by holographic models of associative memory.</p>
<div id="def-hole" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.13 (HolE model)</strong></span> In the <strong>HolE</strong> model, entities and relations are represented as real-valued vectors <span class="math inline">\(\mathbf{e}, \mathbf{r} \in \mathbb{R}^d\)</span>.</p>
<p>The scoring function uses circular correlation: <span class="math display">\[f_r(h, t) = \mathbf{r}^T (\mathbf{h} \star \mathbf{t}) = \sum_{i=1}^d \mathbf{r}_i [\mathbf{h} \star \mathbf{t}]_i\]</span></p>
<p>where <span class="math inline">\(\star\)</span> is the circular correlation operation defined as: <span class="math display">\[[\mathbf{h} \star \mathbf{t}]_k = \sum_{i=0}^{d-1} \mathbf{h}_i \mathbf{t}_{(k+i) \mod d}\]</span></p>
</div>
<section id="circular-correlation" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="circular-correlation"><span class="header-section-number">6.6.1</span> Circular correlation</h3>
<p>Circular correlation is a key operation in HolE:</p>
<div id="def-circular-correlation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.14 (Circular correlation)</strong></span> The <strong>circular correlation</strong> between vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> of dimension <span class="math inline">\(d\)</span> is defined as: <span class="math display">\[[\mathbf{a} \star \mathbf{b}]_k = \sum_{i=0}^{d-1} \mathbf{a}_i \mathbf{b}_{(k+i) \mod d}\]</span></p>
<p>This operation can be efficiently computed using the Fast Fourier Transform (FFT): <span class="math display">\[\mathbf{a} \star \mathbf{b} = \mathcal{F}^{-1}( \overline{\mathcal{F}(\mathbf{a})} \odot \mathcal{F}(\mathbf{b}))\]</span></p>
<p>where <span class="math inline">\(\mathcal{F}\)</span> is the Fourier transform, <span class="math inline">\(\mathcal{F}^{-1}\)</span> is the inverse Fourier transform, <span class="math inline">\(\overline{\mathcal{F}(\mathbf{a})}\)</span> is the complex conjugate of <span class="math inline">\(\mathcal{F}(\mathbf{a})\)</span>, and <span class="math inline">\(\odot\)</span> is the element-wise product.</p>
</div>
</section>
<section id="connection-to-complex" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="connection-to-complex"><span class="header-section-number">6.6.2</span> Connection to ComplEx</h3>
<p>Interestingly, HolE and ComplEx have been shown to be mathematically equivalent under certain conditions:</p>
<div id="def-hole-complex" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.15 (HolE-ComplEx equivalence)</strong></span> Hayashi and Shimbo (2017) proved that HolE and ComplEx are mathematically equivalent when:</p>
<ol type="1">
<li>The embedding dimension in HolE is <span class="math inline">\(d\)</span></li>
<li>The embedding dimension in ComplEx is <span class="math inline">\(d/2\)</span></li>
<li>Appropriate constraints are placed on the embeddings</li>
</ol>
<p>This means that despite their different formulations and motivations, HolE and ComplEx have the same expressive power.</p>
</div>
<div id="exm-hole" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.8 (HolE example)</strong></span> Consider entity embeddings <span class="math inline">\(\mathbf{h} = [0.5, 0.8, 0.1, 0.3]\)</span> and <span class="math inline">\(\mathbf{t} = [0.2, 0.7, 0.4, 0.6]\)</span>, and relation embedding <span class="math inline">\(\mathbf{r} = [0.9, 0.2, 0.5, 0.3]\)</span>.</p>
<p>The circular correlation <span class="math inline">\(\mathbf{h} \star \mathbf{t}\)</span> would be computed as:</p>
<ul>
<li><span class="math inline">\([\mathbf{h} \star \mathbf{t}]_0 = 0.5 \cdot 0.2 + 0.8 \cdot 0.7 + 0.1 \cdot 0.4 + 0.3 \cdot 0.6 = 0.1 + 0.56 + 0.04 + 0.18 = 0.88\)</span></li>
<li><span class="math inline">\([\mathbf{h} \star \mathbf{t}]_1 = 0.5 \cdot 0.6 + 0.8 \cdot 0.2 + 0.1 \cdot 0.7 + 0.3 \cdot 0.4 = 0.3 + 0.16 + 0.07 + 0.12 = 0.65\)</span></li>
<li><span class="math inline">\([\mathbf{h} \star \mathbf{t}]_2 = 0.5 \cdot 0.4 + 0.8 \cdot 0.6 + 0.1 \cdot 0.2 + 0.3 \cdot 0.7 = 0.2 + 0.48 + 0.02 + 0.21 = 0.91\)</span></li>
<li><span class="math inline">\([\mathbf{h} \star \mathbf{t}]_3 = 0.5 \cdot 0.7 + 0.8 \cdot 0.4 + 0.1 \cdot 0.6 + 0.3 \cdot 0.2 = 0.35 + 0.32 + 0.06 + 0.06 = 0.79\)</span></li>
</ul>
<p>The final score would be: <span class="math inline">\(f_r(h, t) = \mathbf{r}^T (\mathbf{h} \star \mathbf{t}) = 0.9 \cdot 0.88 + 0.2 \cdot 0.65 + 0.5 \cdot 0.91 + 0.3 \cdot 0.79 = 0.792 + 0.13 + 0.455 + 0.237 = 1.614\)</span></p>
</div>
</section>
<section id="strengths-and-limitations-of-hole" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="strengths-and-limitations-of-hole"><span class="header-section-number">6.6.3</span> Strengths and limitations of HolE</h3>
<p>HolE offers several advantages:</p>
<ol type="1">
<li><strong>Asymmetric scoring function</strong>: HolE can model both symmetric and antisymmetric relations</li>
<li><strong>Parameter efficiency</strong>: HolE has the same parameter complexity as DistMult: <span class="math inline">\(O(|E|d + |R|d)\)</span></li>
<li><strong>Compositional representations</strong>: The circular correlation operation captures compositional aspects of entity relationships</li>
</ol>
<p>However, HolE also has limitations:</p>
<ol type="1">
<li><strong>Computational complexity</strong>: Computing circular correlation is more expensive than simple operations like dot products</li>
<li><strong>Interpretability</strong>: The circular correlation operation is less intuitive than other scoring functions</li>
<li><strong>Equivalence to ComplEx</strong>: Given the mathematical equivalence to ComplEx, HolE doesn’t offer additional expressiveness</li>
</ol>
</section>
</section>
<section id="simple-a-canonical-tensor-decomposition-approach" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="simple-a-canonical-tensor-decomposition-approach"><span class="header-section-number">6.7</span> SimplE: a canonical tensor decomposition approach</h2>
<p>SimplE, proposed by Kazemi and Poole (2018), is based on Canonical Polyadic (CP) decomposition but addresses its limitations through parameter sharing and an enhanced scoring function.</p>
<div id="def-simple" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.16 (SimplE model)</strong></span> In the <strong>SimplE</strong> model, each entity <span class="math inline">\(e\)</span> has two embeddings:</p>
<ol type="1">
<li>A head embedding <span class="math inline">\(\mathbf{e}_h \in \mathbb{R}^d\)</span></li>
<li>A tail embedding <span class="math inline">\(\mathbf{e}_t \in \mathbb{R}^d\)</span></li>
</ol>
<p>Each relation <span class="math inline">\(r\)</span> also has two embeddings:</p>
<ol type="1">
<li>A forward embedding <span class="math inline">\(\mathbf{r} \in \mathbb{R}^d\)</span></li>
<li>A reverse embedding <span class="math inline">\(\mathbf{r}^{-1} \in \mathbb{R}^d\)</span></li>
</ol>
<p>The scoring function is the average of two CP-based scores: <span class="math display">\[f_r(h, t) = \frac{1}{2} \left( \sum_{i=1}^d \mathbf{h}_h[i] \cdot \mathbf{r}[i] \cdot \mathbf{t}_t[i] + \sum_{i=1}^d \mathbf{t}_h[i] \cdot \mathbf{r}^{-1}[i] \cdot \mathbf{h}_t[i] \right)\]</span></p>
</div>
<section id="parameter-sharing-and-inversion-relations" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="parameter-sharing-and-inversion-relations"><span class="header-section-number">6.7.1</span> Parameter sharing and inversion relations</h3>
<p>A key innovation in SimplE is its parameter sharing mechanism and the explicit modeling of inverse relations:</p>
<div id="def-simple-inversion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.17 (SimplE parameter sharing and inversion)</strong></span> SimplE leverages two key ideas:</p>
<ol type="1">
<li><strong>Parameter sharing</strong>: An entity’s head embedding in one relation can be related to its tail embedding in another relation</li>
<li><strong>Explicit inverse relations</strong>: Each relation <span class="math inline">\(r\)</span> has an explicit inverse relation <span class="math inline">\(r^{-1}\)</span></li>
</ol>
<p>For a triple <span class="math inline">\((h, r, t)\)</span>, the model computes both:</p>
<ul>
<li>Forward score: <span class="math inline">\(\sum_{i=1}^d \mathbf{h}_h[i] \cdot \mathbf{r}[i] \cdot \mathbf{t}_t[i]\)</span></li>
<li>Inverse score: <span class="math inline">\(\sum_{i=1}^d \mathbf{t}_h[i] \cdot \mathbf{r}^{-1}[i] \cdot \mathbf{h}_t[i]\)</span></li>
</ul>
<p>The final score is the average of these two scores, encouraging consistency between forward and inverse relations.</p>
</div>
<div id="exm-simple" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.9 (SimplE example)</strong></span> Consider the triple (France, has_capital, Paris) and its inverse (Paris, is_capital_of, France).</p>
<p>SimplE would compute:</p>
<ol type="1">
<li>Forward score: <span class="math inline">\(\sum_{i=1}^d \mathbf{France}_h[i] \cdot \mathbf{has\_capital}[i] \cdot \mathbf{Paris}_t[i]\)</span></li>
<li>Inverse score: <span class="math inline">\(\sum_{i=1}^d \mathbf{Paris}_h[i] \cdot \mathbf{is\_capital\_of}[i] \cdot \mathbf{France}_t[i]\)</span></li>
</ol>
<p>By averaging these scores, SimplE encourages consistency between the forward and inverse relations, while still allowing for asymmetric scoring due to the use of different head and tail embeddings.</p>
</div>
</section>
<section id="strengths-and-limitations-of-simple" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="strengths-and-limitations-of-simple"><span class="header-section-number">6.7.2</span> Strengths and limitations of SimplE</h3>
<p>SimplE offers several advantages:</p>
<ol type="1">
<li><strong>Expressiveness</strong>: SimplE can model various relation patterns, including symmetry, antisymmetry, and inversion</li>
<li><strong>Interpretability</strong>: The model has a clear interpretation in terms of forward and inverse relations</li>
<li><strong>Parameter efficiency</strong>: SimplE has <span class="math inline">\(O(2|E|d + 2|R|d)\)</span> parameters, which is reasonable given its expressiveness</li>
</ol>
<p>However, SimplE also has limitations:</p>
<ol type="1">
<li><strong>Additional parameters</strong>: SimplE requires more parameters than models like DistMult or TransE, potentially leading to increased overfitting risk</li>
<li><strong>Training complexity</strong>: The need to maintain consistency between forward and inverse relations can complicate the training process</li>
<li><strong>Composition modeling</strong>: Like other CP-based models, SimplE may struggle with modeling compositional patterns in relations</li>
</ol>
</section>
</section>
<section id="tucker-a-tensor-decomposition-model" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="tucker-a-tensor-decomposition-model"><span class="header-section-number">6.8</span> TuckER: a tensor decomposition model</h2>
<p>TuckER, introduced by Balažević et al.&nbsp;(2019), is based on Tucker decomposition, a more general form of tensor factorization than CP decomposition.</p>
<div id="def-tucker" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.18 (TuckER model)</strong></span> In the <strong>TuckER</strong> model:</p>
<ol type="1">
<li>Entities are represented as vectors <span class="math inline">\(\mathbf{e} \in \mathbb{R}^{d_e}\)</span></li>
<li>Relations are represented as vectors <span class="math inline">\(\mathbf{r} \in \mathbb{R}^{d_r}\)</span></li>
<li>A core tensor <span class="math inline">\(\mathcal{W} \in \mathbb{R}^{d_e \times d_r \times d_e}\)</span> captures interactions between entities and relations</li>
</ol>
<p>The scoring function is: <span class="math display">\[f_r(h, t) = \mathcal{W} \times_1 \mathbf{h} \times_2 \mathbf{r} \times_3 \mathbf{t}\]</span></p>
<p>where <span class="math inline">\(\times_n\)</span> denotes the n-mode product of a tensor with a vector.</p>
<p>This can be rewritten as: <span class="math display">\[f_r(h, t) = \sum_{i=1}^{d_e} \sum_{j=1}^{d_r} \sum_{k=1}^{d_e} \mathcal{W}_{ijk} \cdot \mathbf{h}_i \cdot \mathbf{r}_j \cdot \mathbf{t}_k\]</span></p>
</div>
<section id="tucker-decomposition" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="tucker-decomposition"><span class="header-section-number">6.8.1</span> Tucker decomposition</h3>
<p>TuckER is based on the Tucker decomposition of tensors:</p>
<div id="def-tucker-decomposition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.19 (Tucker decomposition)</strong></span> The <strong>Tucker decomposition</strong> factorizes a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I \times J \times K}\)</span> into:</p>
<ol type="1">
<li>A core tensor <span class="math inline">\(\mathcal{G} \in \mathbb{R}^{P \times Q \times R}\)</span></li>
<li>Factor matrices <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{I \times P}\)</span>, <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{J \times Q}\)</span>, and <span class="math inline">\(\mathbf{C} \in \mathbb{R}^{K \times R}\)</span></li>
</ol>
<p>The decomposition is: <span class="math display">\[\mathcal{X}_{ijk} \approx \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R \mathcal{G}_{pqr} \cdot \mathbf{A}_{ip} \cdot \mathbf{B}_{jq} \cdot \mathbf{C}_{kr}\]</span></p>
<p>In the context of knowledge graphs, this becomes: <span class="math display">\[\mathcal{X}_{hrt} \approx \sum_{i=1}^{d_e} \sum_{j=1}^{d_r} \sum_{k=1}^{d_e} \mathcal{W}_{ijk} \cdot \mathbf{E}_{hi} \cdot \mathbf{R}_{rj} \cdot \mathbf{E}_{tk}\]</span></p>
<p>where <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{|E| \times d_e}\)</span> contains entity embeddings and <span class="math inline">\(\mathbf{R} \in \mathbb{R}^{|R| \times d_r}\)</span> contains relation embeddings.</p>
</div>
</section>
<section id="connections-to-other-models" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="connections-to-other-models"><span class="header-section-number">6.8.2</span> Connections to other models</h3>
<p>TuckER provides a unifying framework for several semantic matching models:</p>
<div id="def-tucker-connections" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.20 (TuckER connections to other models)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>RESCAL</strong> is a special case of TuckER where <span class="math inline">\(d_r = d_e^2\)</span> and the core tensor is reshaped into a set of matrices</li>
<li><strong>DistMult</strong> is a special case of TuckER where <span class="math inline">\(d_r = d_e\)</span> and the core tensor is diagonal (non-zero elements only when <span class="math inline">\(i = j = k\)</span>)</li>
<li><strong>ComplEx</strong> is a special case of TuckER with complex-valued embeddings and specific constraints on the core tensor</li>
<li><strong>SimplE</strong> can be viewed as a special case of TuckER with a specific parameterization of the core tensor</li>
</ol>
</div>
<div id="exm-tucker" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.10 (TuckER example)</strong></span> Consider entity embeddings <span class="math inline">\(\mathbf{h} \in \mathbb{R}^3\)</span> and <span class="math inline">\(\mathbf{t} \in \mathbb{R}^3\)</span>, relation embedding <span class="math inline">\(\mathbf{r} \in \mathbb{R}^2\)</span>, and a core tensor <span class="math inline">\(\mathcal{W} \in \mathbb{R}^{3 \times 2 \times 3}\)</span>.</p>
<p>The score would be computed as: <span class="math display">\[f_r(h, t) = \sum_{i=1}^3 \sum_{j=1}^2 \sum_{k=1}^3 \mathcal{W}_{ijk} \cdot \mathbf{h}_i \cdot \mathbf{r}_j \cdot \mathbf{t}_k\]</span></p>
<p>This allows for complex interactions between different dimensions of the entity and relation embeddings, captured by the core tensor <span class="math inline">\(\mathcal{W}\)</span>.</p>
</div>
</section>
<section id="strengths-and-limitations-of-tucker" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="strengths-and-limitations-of-tucker"><span class="header-section-number">6.8.3</span> Strengths and limitations of TuckER</h3>
<p>TuckER offers several advantages:</p>
<ol type="1">
<li><strong>Expressiveness</strong>: As a generalization of several successful models, TuckER can model various relation patterns</li>
<li><strong>Theoretical unification</strong>: TuckER provides a unifying framework for understanding different semantic matching models</li>
<li><strong>Strong empirical performance</strong>: TuckER achieves state-of-the-art results on various benchmarks</li>
</ol>
<p>However, TuckER also has limitations:</p>
<ol type="1">
<li><strong>Parameter complexity</strong>: The core tensor introduces <span class="math inline">\(O(d_e^2 \cdot d_r)\)</span> parameters, which can be substantial for large embedding dimensions</li>
<li><strong>Training complexity</strong>: The optimization of the core tensor can be challenging</li>
<li><strong>Interpretability</strong>: The core tensor’s role is less intuitive than simpler scoring functions</li>
</ol>
</section>
</section>
<section id="quate-quaternion-embeddings" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="quate-quaternion-embeddings"><span class="header-section-number">6.9</span> QuatE: quaternion embeddings</h2>
<p>QuatE, proposed by Zhang et al.&nbsp;(2019), extends complex-valued embeddings to quaternions, which offer even greater expressiveness.</p>
<div id="def-quate" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.21 (QuatE model)</strong></span> In the <strong>QuatE</strong> model, entities and relations are represented as quaternions: <span class="math display">\[\mathbf{e} = a + b\mathbf{i} + c\mathbf{j} + d\mathbf{k}\]</span> where <span class="math inline">\(a, b, c, d \in \mathbb{R}\)</span> are real numbers, and <span class="math inline">\(\mathbf{i}, \mathbf{j}, \mathbf{k}\)</span> are imaginary units with: <span class="math display">\[\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{i}\mathbf{j}\mathbf{k} = -1\]</span></p>
<p>The scoring function is: <span class="math display">\[f_r(h, t) = \langle \mathbf{h} \otimes \mathbf{r}, \mathbf{t} \rangle\]</span> where <span class="math inline">\(\otimes\)</span> is quaternion multiplication and <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span> is the quaternion inner product.</p>
</div>
<section id="quaternion-algebra" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="quaternion-algebra"><span class="header-section-number">6.9.1</span> Quaternion algebra</h3>
<p>Quaternions extend complex numbers to four dimensions, providing additional expressiveness:</p>
<div id="def-quaternions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.22 (Quaternion algebra)</strong></span> A <strong>quaternion</strong> <span class="math inline">\(q = a + b\mathbf{i} + c\mathbf{j} + d\mathbf{k}\)</span> consists of:</p>
<ul>
<li>A scalar (real) part <span class="math inline">\(a \in \mathbb{R}\)</span></li>
<li>Three imaginary parts <span class="math inline">\(b, c, d \in \mathbb{R}\)</span> with imaginary units <span class="math inline">\(\mathbf{i}, \mathbf{j}, \mathbf{k}\)</span></li>
</ul>
<p>Key operations include:</p>
<ol type="1">
<li><strong>Conjugate</strong>: <span class="math inline">\(\overline{q} = a - b\mathbf{i} - c\mathbf{j} - d\mathbf{k}\)</span></li>
<li><strong>Norm</strong>: <span class="math inline">\(|q| = \sqrt{a^2 + b^2 + c^2 + d^2}\)</span></li>
<li><strong>Multiplication</strong>: <span class="math inline">\((a_1 + b_1\mathbf{i} + c_1\mathbf{j} + d_1\mathbf{k})(a_2 + b_2\mathbf{i} + c_2\mathbf{j} + d_2\mathbf{k}) = a_3 + b_3\mathbf{i} + c_3\mathbf{j} + d_3\mathbf{k}\)</span> where:
<ul>
<li><span class="math inline">\(a_3 = a_1a_2 - b_1b_2 - c_1c_2 - d_1d_2\)</span></li>
<li><span class="math inline">\(b_3 = a_1b_2 + b_1a_2 + c_1d_2 - d_1c_2\)</span></li>
<li><span class="math inline">\(c_3 = a_1c_2 - b_1d_2 + c_1a_2 + d_1b_2\)</span></li>
<li><span class="math inline">\(d_3 = a_1d_2 + b_1c_2 - c_1b_2 + d_1a_2\)</span></li>
</ul></li>
<li><strong>Inner product</strong>: <span class="math inline">\(\langle q_1, q_2 \rangle = a_1a_2 + b_1b_2 + c_1c_2 + d_1d_2\)</span></li>
</ol>
</div>
</section>
<section id="geometric-interpretation-3" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="geometric-interpretation-3"><span class="header-section-number">6.9.2</span> Geometric interpretation</h3>
<p>Quaternions have a rich geometric interpretation related to 3D rotations:</p>
<div id="def-quate-geometry" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.23 (QuatE geometric interpretation)</strong></span> In QuatE:</p>
<ol type="1">
<li>Quaternion multiplication represents rotation in 4D space</li>
<li>Pure imaginary quaternions (where the real part is zero) can represent points in 3D space</li>
<li>Unit quaternions (with norm 1) can represent 3D rotations</li>
</ol>
<p>The scoring function measures how well the rotation of the head entity by the relation quaternion aligns with the tail entity.</p>
</div>
<div id="exm-quate" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.11 (QuatE example)</strong></span> Consider quaternion embeddings:</p>
<ul>
<li>Head entity: <span class="math inline">\(\mathbf{h} = 0.5 + 0.5\mathbf{i} + 0.5\mathbf{j} + 0.5\mathbf{k}\)</span></li>
<li>Relation: <span class="math inline">\(\mathbf{r} = 0.0 + 1.0\mathbf{i} + 0.0\mathbf{j} + 0.0\mathbf{k}\)</span> (a rotation around the i-axis)</li>
<li>Tail entity: <span class="math inline">\(\mathbf{t} = 0.5 + 0.5\mathbf{i} - 0.5\mathbf{j} - 0.5\mathbf{k}\)</span></li>
</ul>
<p>The quaternion product <span class="math inline">\(\mathbf{h} \otimes \mathbf{r}\)</span> would represent the rotation of <span class="math inline">\(\mathbf{h}\)</span> by <span class="math inline">\(\mathbf{r}\)</span>, and the inner product with <span class="math inline">\(\mathbf{t}\)</span> would measure how well this rotated vector aligns with the tail entity.</p>
<p>This allows QuatE to model complex relation patterns through rotations in 4D space.</p>
</div>
</section>
<section id="strengths-and-limitations-of-quate" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="strengths-and-limitations-of-quate"><span class="header-section-number">6.9.3</span> Strengths and limitations of QuatE</h3>
<p>QuatE offers several advantages:</p>
<ol type="1">
<li><strong>Enhanced expressiveness</strong>: Quaternions provide additional degrees of freedom compared to complex numbers</li>
<li><strong>Rotation modeling</strong>: QuatE naturally models rotations, which can capture various relation patterns</li>
<li><strong>Strong empirical performance</strong>: QuatE achieves state-of-the-art results on several benchmarks</li>
</ol>
<p>However, QuatE also has limitations:</p>
<ol type="1">
<li><strong>Increased complexity</strong>: Quaternion operations are more complex than real or complex operations</li>
<li><strong>Additional parameters</strong>: With four components per dimension, QuatE has more parameters than real-valued models</li>
<li><strong>Interpretability</strong>: Quaternion algebra and 4D rotations can be less intuitive to understand</li>
</ol>
</section>
</section>
<section id="extensions-and-variations" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="extensions-and-variations"><span class="header-section-number">6.10</span> Extensions and variations</h2>
<p>Several extensions and variations of semantic matching models have been proposed to address specific challenges or incorporate additional information:</p>
<section id="regularization-approaches" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="regularization-approaches"><span class="header-section-number">6.10.1</span> Regularization approaches</h3>
<p>Various regularization techniques have been proposed to improve the training of semantic matching models:</p>
<div id="def-regularization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.24 (Regularization techniques)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Nuclear 3-norm regularization</strong> (Lacroix et al., 2018): Applies tensor nuclear norm regularization to improve generalization</li>
<li><strong>Adversarial regularization</strong> (Minervini et al., 2017): Adds adversarial perturbations to embeddings during training to improve robustness</li>
<li><strong>Orthogonality constraints</strong> (Sun et al., 2019): Enforces orthogonality between entity and relation embeddings to prevent overfitting</li>
</ol>
</div>
</section>
<section id="multi-modal-extensions" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="multi-modal-extensions"><span class="header-section-number">6.10.2</span> Multi-modal extensions</h3>
<p>Several models incorporate additional modalities beyond the graph structure:</p>
<div id="def-multimodal-extensions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.25 (Multi-modal extensions)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>DKRL</strong> (Xie et al., 2016): Incorporates textual descriptions of entities using convolutional neural networks</li>
<li><strong>IKRL</strong> (Xie et al., 2017): Integrates image information for visual entities</li>
<li><strong>KG-BERT</strong> (Yao et al., 2019): Leverages pre-trained language models to encode entities and relations</li>
</ol>
</div>
</section>
<section id="type-aware-models" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="type-aware-models"><span class="header-section-number">6.10.3</span> Type-aware models</h3>
<p>Some models explicitly incorporate entity type information:</p>
<div id="def-type-aware" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.26 (Type-aware models)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>TKRL</strong> (Xie et al., 2016): Incorporates hierarchical type information for entities</li>
<li><strong>TypeComplex</strong> (Jain et al., 2018): Extends ComplEx with entity type constraints</li>
<li><strong>TRESCAL</strong> (Chang et al., 2014): Incorporates type information into the RESCAL model</li>
</ol>
</div>
<div id="exm-type-aware" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.12 (Type-aware example)</strong></span> Consider the entity “Paris” with type “City” and “France” with type “Country”.</p>
<p>A type-aware model would:</p>
<ol type="1">
<li>Learn embeddings for types (City, Country)</li>
<li>Incorporate type compatibility for relations (e.g., <code>"is_capital_of"</code> connects City to Country)</li>
<li>Use type information to constrain predictions (only predict entities of the correct type)</li>
</ol>
<p>This can significantly improve prediction accuracy by reducing the search space to type-compatible entities.</p>
</div>
</section>
</section>
<section id="relation-patterns-and-model-capabilities" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="relation-patterns-and-model-capabilities"><span class="header-section-number">6.11</span> Relation patterns and model capabilities</h2>
<p>Different semantic matching models have different capabilities for modeling relation patterns:</p>
<p>Model capabilities for relation patterns</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Relation Pattern</th>
<th>RESCAL</th>
<th>DistMult</th>
<th>ComplEx</th>
<th>HolE</th>
<th>SimplE</th>
<th>TuckER</th>
<th>QuatE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Symmetry</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Antisymmetry</td>
<td>Strong</td>
<td>Weak</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="odd">
<td>Inversion</td>
<td>Medium</td>
<td>Weak</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Composition</td>
<td>Strong</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="odd">
<td>Hierarchy</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
</tr>
</tbody>
</table>
<p>Understanding these capabilities helps in selecting the appropriate model for specific knowledge graphs based on the prevalent relation patterns.</p>
<div id="exm-model-selection" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.13 (Model selection example)</strong></span> Consider knowledge graphs with different characteristics:</p>
<ol type="1">
<li><p><strong>Biomedical knowledge graph</strong> (many hierarchical relationships like <code>"is_a"</code> and “part_of”):</p>
<ul>
<li>TuckER or QuatE might be most appropriate due to their strong hierarchical modeling capabilities</li>
</ul></li>
<li><p><strong>Social network knowledge graph</strong> (many symmetric relationships like <code>"is_friend_of"</code>):</p>
<ul>
<li>Even simpler models like DistMult might perform well since they naturally handle symmetry</li>
</ul></li>
<li><p><strong>General knowledge graph</strong> (diverse relation patterns including symmetry, antisymmetry, and composition):</p>
<ul>
<li>ComplEx or SimplE might offer a good balance of expressiveness and efficiency</li>
</ul></li>
</ol>
</div>
</section>
<section id="implementation-considerations" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="implementation-considerations"><span class="header-section-number">6.12</span> Implementation considerations</h2>
<p>When implementing semantic matching models, several practical considerations are important:</p>
<section id="training-objectives" class="level3" data-number="6.12.1">
<h3 data-number="6.12.1" class="anchored" data-anchor-id="training-objectives"><span class="header-section-number">6.12.1</span> Training objectives</h3>
<p>Various training objectives can be used for semantic matching models:</p>
<div id="def-training-objectives" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.27 (Training objectives)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Margin-based ranking loss</strong>: <span class="math display">\[L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma + f_r(h, t) - f_r(h', t')]_+\]</span></p></li>
<li><p><strong>Binary cross-entropy loss</strong>: <span class="math display">\[L = \sum_{(h,r,t) \in S \cup S'} y_{hrt} \log \sigma(f_r(h, t)) + (1-y_{hrt}) \log (1-\sigma(f_r(h, t)))\]</span> where <span class="math inline">\(y_{hrt} = 1\)</span> for positive triples and <span class="math inline">\(y_{hrt} = 0\)</span> for negative triples</p></li>
<li><p><strong>Self-adversarial negative sampling</strong> (Sun et al., 2019): <span class="math display">\[L = -\log \sigma(\gamma - f_r(h, t)) - \sum_{(h',r,t') \in S'_{(h,r,t)}} p(h',r,t') \log \sigma(f_r(h', t') - \gamma)\]</span> where <span class="math inline">\(p(h',r,t')\)</span> is based on the current model scores</p></li>
</ol>
</div>
<p>The choice of training objective can significantly impact model performance.</p>
</section>
<section id="initialization-strategies" class="level3" data-number="6.12.2">
<h3 data-number="6.12.2" class="anchored" data-anchor-id="initialization-strategies"><span class="header-section-number">6.12.2</span> Initialization strategies</h3>
<p>Proper initialization is crucial for effective training:</p>
<div id="def-initialization-strategies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.28 (Initialization strategies for semantic matching models)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Uniform initialization</strong>: Sample from a uniform distribution, e.g., <span class="math inline">\(U(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})\)</span></li>
<li><strong>Xavier/Glorot initialization</strong>: Scale based on input and output dimensions</li>
<li><strong>Complex initialization</strong>: For complex-valued embeddings, initialize real and imaginary parts separately</li>
<li><strong>Quaternion initialization</strong>: For quaternion embeddings, initialize each component with appropriate scaling</li>
</ol>
</div>
<p>For semantic matching models, initialization can be particularly important due to the bilinear nature of many scoring functions.</p>
</section>
<section id="regularization-and-constraints" class="level3" data-number="6.12.3">
<h3 data-number="6.12.3" class="anchored" data-anchor-id="regularization-and-constraints"><span class="header-section-number">6.12.3</span> Regularization and constraints</h3>
<p>Various regularization methods and constraints are commonly used:</p>
<div id="def-regularization-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.29 (Regularization and constraints)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>L2 regularization</strong>: Add <span class="math inline">\(\lambda(\|\mathbf{E}\|_F^2 + \|\mathbf{R}\|_F^2)\)</span> to the loss function</li>
<li><strong>Nuclear 3-norm regularization</strong>: Regularize based on the tensor nuclear norm</li>
<li><strong>Dropout</strong>: Apply dropout to entity and relation embeddings during training</li>
<li><strong>Max-norm constraints</strong>: Enforce <span class="math inline">\(\|\mathbf{e}\|_2 \leq C\)</span> for all entity embeddings</li>
<li><strong>Orthogonality constraints</strong>: Enforce orthogonality between certain embeddings</li>
</ol>
</div>
<p>Proper regularization is especially important for models with many parameters, like RESCAL and TuckER.</p>
</section>
</section>
<section id="performance-analysis" class="level2" data-number="6.13">
<h2 data-number="6.13" class="anchored" data-anchor-id="performance-analysis"><span class="header-section-number">6.13</span> Performance analysis</h2>
<p>Let’s analyze the empirical performance of semantic matching models on standard benchmark datasets:</p>
<p>Performance comparison (Hits@10 in %)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>FB15k</th>
<th>WN18</th>
<th>FB15k-237</th>
<th>WN18RR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RESCAL</td>
<td>58.7</td>
<td>52.8</td>
<td>35.9</td>
<td>42.8</td>
</tr>
<tr class="even">
<td>DistMult</td>
<td>82.4</td>
<td>94.2</td>
<td>41.9</td>
<td>49.1</td>
</tr>
<tr class="odd">
<td>ComplEx</td>
<td>84.0</td>
<td>94.7</td>
<td>42.8</td>
<td>51.0</td>
</tr>
<tr class="even">
<td>HolE</td>
<td>73.1</td>
<td>94.9</td>
<td>40.2</td>
<td>49.1</td>
</tr>
<tr class="odd">
<td>SimplE</td>
<td>83.8</td>
<td>94.2</td>
<td>42.5</td>
<td>49.6</td>
</tr>
<tr class="even">
<td>TuckER</td>
<td>89.2</td>
<td>95.8</td>
<td>54.4</td>
<td>52.6</td>
</tr>
<tr class="odd">
<td>QuatE</td>
<td>88.0</td>
<td>95.9</td>
<td>51.6</td>
<td>53.3</td>
</tr>
</tbody>
</table>
<p>These results show the progression of performance as semantic matching models have evolved to become more expressive while maintaining computational efficiency.</p>
<div id="exm-performance-analysis" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.14 (Performance analysis example)</strong></span> On FB15k, we observe:</p>
<ol type="1">
<li>RESCAL achieves moderate performance (58.7%) despite its expressiveness, likely due to overfitting</li>
<li>DistMult performs surprisingly well (82.4%) despite its symmetry constraint, suggesting many relations in FB15k are effectively symmetric or can be approximately modeled as such</li>
<li>ComplEx improves over DistMult (84.0%) by addressing the symmetry limitation</li>
<li>TuckER and QuatE achieve the best performance (89.2% and 88.0%) through their more expressive formulations</li>
</ol>
<p>This progression reflects the trade-off between model expressiveness and overfitting risk, with the most recent models striking a better balance.</p>
</div>
</section>
<section id="applications-of-semantic-matching-models" class="level2" data-number="6.14">
<h2 data-number="6.14" class="anchored" data-anchor-id="applications-of-semantic-matching-models"><span class="header-section-number">6.14</span> Applications of semantic matching models</h2>
<p>Semantic matching models have been applied to various domains beyond link prediction:</p>
<section id="question-answering" class="level3" data-number="6.14.1">
<h3 data-number="6.14.1" class="anchored" data-anchor-id="question-answering"><span class="header-section-number">6.14.1</span> Question answering</h3>
<p>Knowledge graph embeddings can support question answering systems:</p>
<div id="def-qa-application" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.30 (Knowledge graph-based question answering)</strong></span> Semantic matching models support question answering by:</p>
<ol type="1">
<li>Mapping questions to entities and relations in the knowledge graph</li>
<li>Using embedding scores to rank candidate answers</li>
<li>Providing confidence scores for different answers</li>
</ol>
</div>
<div id="exm-qa" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.15 (Question answering example)</strong></span> For the question “Who wrote the novel 1984?”, a system might:</p>
<ol type="1">
<li>Identify “1984” as an entity and “wrote” as a relation</li>
<li>Use a semantic matching model to score all entities <span class="math inline">\(e\)</span> for the triple <span class="math inline">\((e, \text{wrote}, \text{1984})\)</span></li>
<li>Return “George Orwell” as the highest-scoring entity</li>
</ol>
</div>
</section>
<section id="recommendation-systems" class="level3" data-number="6.14.2">
<h3 data-number="6.14.2" class="anchored" data-anchor-id="recommendation-systems"><span class="header-section-number">6.14.2</span> Recommendation systems</h3>
<p>Semantic matching models can enhance recommendation systems:</p>
<div id="def-recommendation-application" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.31 (Knowledge graph-based recommendation)</strong></span> Semantic matching models support recommendations by:</p>
<ol type="1">
<li>Representing users and items as entities in a knowledge graph</li>
<li>Modeling user preferences as relations (e.g., “likes”, “purchased”)</li>
<li>Using embedding scores to generate personalized recommendations</li>
</ol>
</div>
<div id="exm-recommendation" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.16 (Recommendation system example)</strong></span> In a movie recommendation system:</p>
<ol type="1">
<li>Users and movies are represented as entities</li>
<li>Relations include “watched”, “rated”, “liked”</li>
<li>A semantic matching model can predict scores for unseen user-movie pairs</li>
<li>Movies with high predicted scores for “likes” relation are recommended to users</li>
</ol>
</div>
</section>
<section id="information-extraction" class="level3" data-number="6.14.3">
<h3 data-number="6.14.3" class="anchored" data-anchor-id="information-extraction"><span class="header-section-number">6.14.3</span> Information extraction</h3>
<p>Semantic matching models can assist in information extraction:</p>
<div id="def-ie-application" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.32 (Knowledge graph-based information extraction)</strong></span> Semantic matching models support information extraction by:</p>
<ol type="1">
<li>Scoring candidate triples extracted from text</li>
<li>Filtering out unlikely triples based on embedding scores</li>
<li>Integrating new knowledge into the knowledge graph</li>
</ol>
</div>
<div id="exm-ie" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.17 (Information extraction example)</strong></span> Given the text “Apple released the iPhone 14 in September 2022”:</p>
<ol type="1">
<li>An information extraction system extracts the triple (Apple, released, iPhone_14)</li>
<li>A semantic matching model scores this triple against existing knowledge</li>
<li>If the score is high enough, the triple is added to the knowledge graph</li>
</ol>
</div>
</section>
</section>
<section id="future-directions" class="level2" data-number="6.15">
<h2 data-number="6.15" class="anchored" data-anchor-id="future-directions"><span class="header-section-number">6.15</span> Future directions</h2>
<p>Semantic matching models continue to evolve, with several promising research directions:</p>
<section id="neural-architecture-integration" class="level3" data-number="6.15.1">
<h3 data-number="6.15.1" class="anchored" data-anchor-id="neural-architecture-integration"><span class="header-section-number">6.15.1</span> Neural architecture integration</h3>
<p>Integrating semantic matching models with neural architectures can enhance their expressiveness:</p>
<div id="def-neural-integration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.33 (Neural-enhanced semantic matching)</strong></span> <strong>Neural-enhanced semantic matching models</strong> combine bilinear scoring functions with neural network components:</p>
<ol type="1">
<li><strong>ConvE</strong> (Dettmers et al., 2018): Uses convolutional neural networks over reshaped embeddings</li>
<li><strong>ConvKB</strong> (Nguyen et al., 2018): Applies convolutions over entity-relation-entity triples</li>
<li><strong>CapsE</strong> (Nguyen et al., 2019): Utilizes capsule networks for knowledge graph embedding</li>
</ol>
</div>
<p>These models aim to capture more complex interaction patterns while maintaining computational efficiency.</p>
</section>
<section id="pre-trained-language-model-integration" class="level3" data-number="6.15.2">
<h3 data-number="6.15.2" class="anchored" data-anchor-id="pre-trained-language-model-integration"><span class="header-section-number">6.15.2</span> Pre-trained language model integration</h3>
<p>Leveraging pre-trained language models can enhance semantic matching models:</p>
<div id="def-plm-integration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.34 (Pre-trained language model integration)</strong></span> Recent approaches integrate pre-trained language models with knowledge graph embeddings:</p>
<ol type="1">
<li><strong>KG-BERT</strong> (Yao et al., 2019): Encodes triples using BERT for scoring</li>
<li><strong>KEPLER</strong> (Wang et al., 2021): Jointly trains knowledge embeddings and language representations</li>
<li><strong>KnowledgeBART</strong> (Liu et al., 2022): Fine-tunes BART for knowledge-aware generation</li>
</ol>
</div>
<p>These approaches leverage the semantic understanding captured by pre-trained language models to enhance knowledge graph representations.</p>
</section>
<section id="inductive-learning" class="level3" data-number="6.15.3">
<h3 data-number="6.15.3" class="anchored" data-anchor-id="inductive-learning"><span class="header-section-number">6.15.3</span> Inductive learning</h3>
<p>Enabling inductive learning is another important research direction:</p>
<div id="def-inductive-semantic-matching" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.35 (Inductive semantic matching models)</strong></span> <strong>Inductive semantic matching models</strong> can handle previously unseen entities:</p>
<ol type="1">
<li><strong>DRUM</strong> (Sadeghian et al., 2019): Uses entity descriptions for inductive learning</li>
<li><strong>GraIL</strong> (Teru et al., 2020): Employs local subgraph patterns for inductive reasoning</li>
<li><strong>NBFNet</strong> (Zhu et al., 2021): Leverages neural bellman-ford networks for inductive relation prediction</li>
</ol>
</div>
<p>These models address a key limitation of traditional knowledge graph embeddings, which are transductive in nature.</p>
</section>
</section>
<section id="summary" class="level2" data-number="6.16">
<h2 data-number="6.16" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.16</span> Summary</h2>
<p>In this chapter, we’ve explored semantic matching models for knowledge graph embeddings, which focus on measuring the compatibility between entities based on their embeddings and relation-specific parameters.</p>
<p>We started with RESCAL, a tensor factorization approach that represents relations as matrices, capturing complex interactions between entities. We then examined DistMult, which simplifies RESCAL by restricting relation matrices to be diagonal, significantly reducing the number of parameters but introducing a symmetry constraint.</p>
<p>To address the symmetry limitation, we explored ComplEx, which extends DistMult to the complex domain, allowing it to model both symmetric and antisymmetric relations effectively. We also discussed HolE, which uses circular correlation to capture interactions between entities, and SimplE, which leverages parameter sharing and explicit inverse relations.</p>
<p>More recent models like TuckER and QuatE push the boundaries of expressiveness, with TuckER providing a unifying framework for various semantic matching models and QuatE extending embeddings to quaternions for enhanced representation power.</p>
<p>Throughout the chapter, we analyzed the strengths and limitations of each model, examined their capabilities for modeling different relation patterns, and discussed practical implementation considerations. We also explored applications of semantic matching models beyond link prediction, including question answering, recommendation systems, and information extraction.</p>
<p>Semantic matching models represent a powerful approach to knowledge graph embeddings, offering a balance of expressiveness, interpretability, and computational efficiency. By understanding the principles behind these models, you can select the appropriate approach for your specific knowledge graph and application requirements.</p>
</section>
<section id="further-reading" class="level2" data-number="6.17">
<h2 data-number="6.17" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">6.17</span> Further reading</h2>
<section id="original-papers" class="level3" data-number="6.17.1">
<h3 data-number="6.17.1" class="anchored" data-anchor-id="original-papers"><span class="header-section-number">6.17.1</span> Original papers</h3>
<ul>
<li>Nickel, M., Tresp, V., &amp; Kriegel, H. P. (2011). A Three-Way Model for Collective Learning on Multi-Relational Data. In Proceedings of the 28th International Conference on Machine Learning (pp.&nbsp;809-816).</li>
<li>Yang, B., Yih, W. T., He, X., Gao, J., &amp; Deng, L. (2015). Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In International Conference on Learning Representations.</li>
<li>Trouillon, T., Welbl, J., Riedel, S., Gaussier, É., &amp; Bouchard, G. (2016). Complex Embeddings for Simple Link Prediction. In Proceedings of the 33rd International Conference on Machine Learning (pp.&nbsp;2071-2080).</li>
<li>Nickel, M., Rosasco, L., &amp; Poggio, T. (2016). Holographic Embeddings of Knowledge Graphs. In AAAI Conference on Artificial Intelligence (pp.&nbsp;1955-1961).</li>
<li>Kazemi, S. M., &amp; Poole, D. (2018). SimplE Embedding for Link Prediction in Knowledge Graphs. In Advances in Neural Information Processing Systems (pp.&nbsp;4284-4295).</li>
<li>Balažević, I., Allen, C., &amp; Hospedales, T. M. (2019). TuckER: Tensor Factorization for Knowledge Graph Completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (pp.&nbsp;5185-5194).</li>
<li>Zhang, S., Tay, Y., Yao, L., &amp; Liu, Q. (2019). Quaternion Knowledge Graph Embeddings. In Advances in Neural Information Processing Systems (pp.&nbsp;2731-2741).</li>
</ul>
</section>
<section id="surveys-and-comparative-analyses" class="level3" data-number="6.17.2">
<h3 data-number="6.17.2" class="anchored" data-anchor-id="surveys-and-comparative-analyses"><span class="header-section-number">6.17.2</span> Surveys and comparative analyses</h3>
<ul>
<li>Nickel, M., Murphy, K., Tresp, V., &amp; Gabrilovich, E. (2016). A Review of Relational Machine Learning for Knowledge Graphs. Proceedings of the IEEE, 104(1), 11-33.</li>
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge Graph Embedding: A Survey of Approaches and Applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
<li>Rossi, A., Barbosa, D., Firmani, D., Matinata, A., &amp; Merialdo, P. (2021). Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. ACM Transactions on Knowledge Discovery from Data, 15(2), 1-49.</li>
</ul>
</section>
<section id="mathematical-foundations" class="level3" data-number="6.17.3">
<h3 data-number="6.17.3" class="anchored" data-anchor-id="mathematical-foundations"><span class="header-section-number">6.17.3</span> Mathematical foundations</h3>
<ul>
<li>Kolda, T. G., &amp; Bader, B. W. (2009). Tensor Decompositions and Applications. SIAM Review, 51(3), 455-500.</li>
<li>de Lacerda, G., Hayashi, K., &amp; Shimbo, M. (2021). On the Equivalence Between Holographic and Complex Embeddings for Link Prediction. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No.&nbsp;6, pp.&nbsp;4580-4588).</li>
<li>Kadlcik, T., &amp; Frolov, A. A. (2021). Understanding the Theoretical Relationships Between Semantic Knowledge Graph Embedding Models. IEEE Access, 9, 91340-91362.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/translation-based.html" class="pagination-link" aria-label="Translation-Based Embedding Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/rotation-nn.html" class="pagination-link" aria-label="Advanced Models: Rotations and Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>