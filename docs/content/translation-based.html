<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Translation-Based Embedding Models – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/semantic-matching.html" rel="next">
<link href="../content/embedding.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/translation-based.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-translation-principle" id="toc-the-translation-principle" class="nav-link active" data-scroll-target="#the-translation-principle"><span class="header-section-number">5.1</span> The translation principle</a></li>
  <li><a href="#transe-the-pioneering-translation-model" id="toc-transe-the-pioneering-translation-model" class="nav-link" data-scroll-target="#transe-the-pioneering-translation-model"><span class="header-section-number">5.2</span> TransE: the pioneering translation model</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation"><span class="header-section-number">5.2.1</span> Geometric interpretation</a></li>
  <li><a href="#learning-algorithm" id="toc-learning-algorithm" class="nav-link" data-scroll-target="#learning-algorithm"><span class="header-section-number">5.2.2</span> Learning algorithm</a></li>
  <li><a href="#transe-algorithm" id="toc-transe-algorithm" class="nav-link" data-scroll-target="#transe-algorithm"><span class="header-section-number">5.2.3</span> TransE algorithm</a></li>
  <li><a href="#strengths-and-limitations-of-transe" id="toc-strengths-and-limitations-of-transe" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transe"><span class="header-section-number">5.2.4</span> Strengths and limitations of TransE</a></li>
  </ul></li>
  <li><a href="#transh-modeling-relations-as-hyperplanes" id="toc-transh-modeling-relations-as-hyperplanes" class="nav-link" data-scroll-target="#transh-modeling-relations-as-hyperplanes"><span class="header-section-number">5.3</span> TransH: modeling relations as hyperplanes</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-1" id="toc-geometric-interpretation-1" class="nav-link" data-scroll-target="#geometric-interpretation-1"><span class="header-section-number">5.3.1</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-transh" id="toc-strengths-and-limitations-of-transh" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transh"><span class="header-section-number">5.3.2</span> Strengths and limitations of TransH</a></li>
  </ul></li>
  <li><a href="#transr-modeling-relations-in-relation-specific-spaces" id="toc-transr-modeling-relations-in-relation-specific-spaces" class="nav-link" data-scroll-target="#transr-modeling-relations-in-relation-specific-spaces"><span class="header-section-number">5.4</span> TransR: modeling relations in relation-specific spaces</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-2" id="toc-geometric-interpretation-2" class="nav-link" data-scroll-target="#geometric-interpretation-2"><span class="header-section-number">5.4.1</span> Geometric interpretation</a></li>
  <li><a href="#cluster-based-transr-ctransr" id="toc-cluster-based-transr-ctransr" class="nav-link" data-scroll-target="#cluster-based-transr-ctransr"><span class="header-section-number">5.4.2</span> Cluster-based TransR (CTransR)</a></li>
  <li><a href="#strengths-and-limitations-of-transr" id="toc-strengths-and-limitations-of-transr" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transr"><span class="header-section-number">5.4.3</span> Strengths and limitations of TransR</a></li>
  </ul></li>
  <li><a href="#transd-dynamic-mapping-matrices" id="toc-transd-dynamic-mapping-matrices" class="nav-link" data-scroll-target="#transd-dynamic-mapping-matrices"><span class="header-section-number">5.5</span> TransD: dynamic mapping matrices</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-3" id="toc-geometric-interpretation-3" class="nav-link" data-scroll-target="#geometric-interpretation-3"><span class="header-section-number">5.5.1</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-transd" id="toc-strengths-and-limitations-of-transd" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transd"><span class="header-section-number">5.5.2</span> Strengths and limitations of TransD</a></li>
  </ul></li>
  <li><a href="#transparse-adaptive-sparse-matrices" id="toc-transparse-adaptive-sparse-matrices" class="nav-link" data-scroll-target="#transparse-adaptive-sparse-matrices"><span class="header-section-number">5.6</span> TranSparse: adaptive sparse matrices</a>
  <ul class="collapse">
  <li><a href="#adaptive-sparsity" id="toc-adaptive-sparsity" class="nav-link" data-scroll-target="#adaptive-sparsity"><span class="header-section-number">5.6.1</span> Adaptive sparsity</a></li>
  <li><a href="#strengths-and-limitations-of-transparse" id="toc-strengths-and-limitations-of-transparse" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transparse"><span class="header-section-number">5.6.2</span> Strengths and limitations of TranSparse</a></li>
  </ul></li>
  <li><a href="#rotate-rotation-in-complex-space" id="toc-rotate-rotation-in-complex-space" class="nav-link" data-scroll-target="#rotate-rotation-in-complex-space"><span class="header-section-number">5.7</span> RotatE: rotation in complex space</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-4" id="toc-geometric-interpretation-4" class="nav-link" data-scroll-target="#geometric-interpretation-4"><span class="header-section-number">5.7.1</span> Geometric interpretation</a></li>
  <li><a href="#modeling-relation-patterns-with-rotate" id="toc-modeling-relation-patterns-with-rotate" class="nav-link" data-scroll-target="#modeling-relation-patterns-with-rotate"><span class="header-section-number">5.7.2</span> Modeling relation patterns with RotatE</a></li>
  <li><a href="#self-adversarial-negative-sampling" id="toc-self-adversarial-negative-sampling" class="nav-link" data-scroll-target="#self-adversarial-negative-sampling"><span class="header-section-number">5.7.3</span> Self-adversarial negative sampling</a></li>
  <li><a href="#strengths-and-limitations-of-rotate" id="toc-strengths-and-limitations-of-rotate" class="nav-link" data-scroll-target="#strengths-and-limitations-of-rotate"><span class="header-section-number">5.7.4</span> Strengths and limitations of RotatE</a></li>
  </ul></li>
  <li><a href="#comparing-translation-based-models" id="toc-comparing-translation-based-models" class="nav-link" data-scroll-target="#comparing-translation-based-models"><span class="header-section-number">5.8</span> Comparing translation-based models</a></li>
  <li><a href="#relation-patterns-and-model-capabilities" id="toc-relation-patterns-and-model-capabilities" class="nav-link" data-scroll-target="#relation-patterns-and-model-capabilities"><span class="header-section-number">5.9</span> Relation patterns and model capabilities</a></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations"><span class="header-section-number">5.10</span> Implementation considerations</a>
  <ul class="collapse">
  <li><a href="#normalization-constraints" id="toc-normalization-constraints" class="nav-link" data-scroll-target="#normalization-constraints"><span class="header-section-number">5.10.1</span> Normalization constraints</a></li>
  <li><a href="#initialization-strategies" id="toc-initialization-strategies" class="nav-link" data-scroll-target="#initialization-strategies"><span class="header-section-number">5.10.2</span> Initialization strategies</a></li>
  <li><a href="#negative-sampling-strategies" id="toc-negative-sampling-strategies" class="nav-link" data-scroll-target="#negative-sampling-strategies"><span class="header-section-number">5.10.3</span> Negative sampling strategies</a></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">5.10.4</span> Optimization algorithms</a></li>
  </ul></li>
  <li><a href="#performance-analysis" id="toc-performance-analysis" class="nav-link" data-scroll-target="#performance-analysis"><span class="header-section-number">5.11</span> Performance analysis</a>
  <ul class="collapse">
  <li><a href="#benchmark-datasets" id="toc-benchmark-datasets" class="nav-link" data-scroll-target="#benchmark-datasets"><span class="header-section-number">5.11.1</span> Benchmark datasets</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics"><span class="header-section-number">5.11.2</span> Evaluation metrics</a></li>
  <li><a href="#comparative-results" id="toc-comparative-results" class="nav-link" data-scroll-target="#comparative-results"><span class="header-section-number">5.11.3</span> Comparative results</a></li>
  </ul></li>
  <li><a href="#advanced-topics" id="toc-advanced-topics" class="nav-link" data-scroll-target="#advanced-topics"><span class="header-section-number">5.12</span> Advanced topics</a>
  <ul class="collapse">
  <li><a href="#entity-type-constraints" id="toc-entity-type-constraints" class="nav-link" data-scroll-target="#entity-type-constraints"><span class="header-section-number">5.12.1</span> Entity type constraints</a></li>
  <li><a href="#temporal-knowledge-graphs" id="toc-temporal-knowledge-graphs" class="nav-link" data-scroll-target="#temporal-knowledge-graphs"><span class="header-section-number">5.12.2</span> Temporal knowledge graphs</a></li>
  <li><a href="#multi-modal-knowledge-graphs" id="toc-multi-modal-knowledge-graphs" class="nav-link" data-scroll-target="#multi-modal-knowledge-graphs"><span class="header-section-number">5.12.3</span> Multi-modal knowledge graphs</a></li>
  <li><a href="#uncertainty-modeling" id="toc-uncertainty-modeling" class="nav-link" data-scroll-target="#uncertainty-modeling"><span class="header-section-number">5.12.4</span> Uncertainty modeling</a></li>
  </ul></li>
  <li><a href="#applications-of-translation-based-models" id="toc-applications-of-translation-based-models" class="nav-link" data-scroll-target="#applications-of-translation-based-models"><span class="header-section-number">5.13</span> Applications of translation-based models</a>
  <ul class="collapse">
  <li><a href="#recommendation-systems" id="toc-recommendation-systems" class="nav-link" data-scroll-target="#recommendation-systems"><span class="header-section-number">5.13.1</span> Recommendation systems</a></li>
  <li><a href="#question-answering" id="toc-question-answering" class="nav-link" data-scroll-target="#question-answering"><span class="header-section-number">5.13.2</span> Question answering</a></li>
  <li><a href="#information-extraction" id="toc-information-extraction" class="nav-link" data-scroll-target="#information-extraction"><span class="header-section-number">5.13.3</span> Information extraction</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions"><span class="header-section-number">5.14</span> Future directions</a>
  <ul class="collapse">
  <li><a href="#inductive-learning" id="toc-inductive-learning" class="nav-link" data-scroll-target="#inductive-learning"><span class="header-section-number">5.14.1</span> Inductive learning</a></li>
  <li><a href="#neural-architecture-integration" id="toc-neural-architecture-integration" class="nav-link" data-scroll-target="#neural-architecture-integration"><span class="header-section-number">5.14.2</span> Neural architecture integration</a></li>
  <li><a href="#explainable-embeddings" id="toc-explainable-embeddings" class="nav-link" data-scroll-target="#explainable-embeddings"><span class="header-section-number">5.14.3</span> Explainable embeddings</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">5.15</span> Summary</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">5.16</span> Further reading</a>
  <ul class="collapse">
  <li><a href="#original-papers" id="toc-original-papers" class="nav-link" data-scroll-target="#original-papers"><span class="header-section-number">5.16.1</span> Original papers</a></li>
  <li><a href="#surveys-and-comparative-analyses" id="toc-surveys-and-comparative-analyses" class="nav-link" data-scroll-target="#surveys-and-comparative-analyses"><span class="header-section-number">5.16.2</span> Surveys and comparative analyses</a></li>
  <li><a href="#applications-and-extensions" id="toc-applications-and-extensions" class="nav-link" data-scroll-target="#applications-and-extensions"><span class="header-section-number">5.16.3</span> Applications and extensions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>Translation-based models represent one of the most intuitive and influential approaches to knowledge graph embeddings. These models interpret relations as translations in the embedding space: if a relation connects a head entity to a tail entity, then adding the relation vector to the head entity vector should approximate the tail entity vector. This simple geometric interpretation has proven remarkably effective for capturing many types of relationships in knowledge graphs.</p>
<p>This chapter explores the family of translation-based models, starting with the pioneering TransE model and progressing through various extensions that address its limitations. We’ll examine how these models work, their geometric interpretations, their strengths and limitations, and how they perform on knowledge graph completion tasks. By understanding translation-based models, you’ll gain insight into the core principles that underlie many knowledge graph embedding approaches.</p>
<section id="the-translation-principle" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="the-translation-principle"><span class="header-section-number">5.1</span> The translation principle</h2>
<p>The fundamental idea behind translation-based models is to interpret relations as translations (displacement vectors) in the embedding space:</p>
<div id="def-translation-principle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Translation principle)</strong></span> In translation-based models, entities and relations are embedded in a shared vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math>, where relations are interpreted as translations from head entities to tail entities. For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model aims to enforce:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>,</mo><mi>𝐫</mi><mo>,</mo><mi>𝐭</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}, \mathbf{r}, \mathbf{t} \in \mathbb{R}^d</annotation></semantics></math> are the vector embeddings of the head entity, relation, and tail entity, respectively.</p>
</div>
<p>This principle draws inspiration from word embeddings, where semantic relationships are often captured as consistent vector offsets. For example, in word embeddings, we might observe that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐤</mi><mi>𝐢</mi><mi>𝐧</mi><mi>𝐠</mi></mrow><mo>−</mo><mrow><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>+</mo><mrow><mi>𝐰</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi></mrow><mo>≈</mo><mrow><mi>𝐪</mi><mi>𝐮</mi><mi>𝐞</mi><mi>𝐞</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{king} - \mathbf{man} + \mathbf{woman} \approx \mathbf{queen}</annotation></semantics></math>, indicating that the gender relationship is captured as a consistent offset.</p>
<p>Similarly, in a knowledge graph about geography, we might expect: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{France} + \mathbf{has\_capital} \approx \mathbf{Paris}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐆</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐦</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐲</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐁</mi><mi>𝐞</mi><mi>𝐫</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Germany} + \mathbf{has\_capital} \approx \mathbf{Berlin}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐈</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐲</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐑</mi><mi>𝐨</mi><mi>𝐦</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Italy} + \mathbf{has\_capital} \approx \mathbf{Rome}</annotation></semantics></math></p>
<p>This implies that the “has_capital” relation is represented as a consistent translation vector that, when added to a country’s embedding, approximates the embedding of its capital city.</p>
<div id="exm-translation-intuition" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 (Translation intuition)</strong></span> Consider a simple knowledge graph about family relationships:</p>
<ul>
<li>(John, is_father_of, Mary)</li>
<li>(Bob, is_father_of, Alice)</li>
<li>(Tom, is_father_of, James)</li>
</ul>
<p>In a translation-based model, the “is*father_of” relation would be represented as a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>*</mo><mtext mathvariant="normal">is_father_of</mtext></mrow><annotation encoding="application/x-tex">\mathbf{r}*{\text{is_father_of}}</annotation></semantics></math> such that:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐉</mi><mi>𝐨</mi><mi>𝐡</mi><mi>𝐧</mi></mrow><mo>+</mo><msub><mi>𝐫</mi><mtext mathvariant="normal">is_father_of</mtext></msub><mo>≈</mo><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐲</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{John} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{Mary}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐁</mi><mi>𝐨</mi><mi>𝐛</mi></mrow><mo>+</mo><msub><mi>𝐫</mi><mtext mathvariant="normal">is_father_of</mtext></msub><mo>≈</mo><mrow><mi>𝐀</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐜</mi><mi>𝐞</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Bob} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{Alice}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐓</mi><mi>𝐨</mi><mi>𝐦</mi></mrow><mo>+</mo><msub><mi>𝐫</mi><mtext mathvariant="normal">is_father_of</mtext></msub><mo>≈</mo><mrow><mi>𝐉</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐞</mi><mi>𝐬</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Tom} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{James}</annotation></semantics></math></li>
</ul>
<p>This means that the father-child relationship is captured as a consistent displacement in the embedding space, allowing the model to generalize to new entity pairs.</p>
</div>
</section>
<section id="transe-the-pioneering-translation-model" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="transe-the-pioneering-translation-model"><span class="header-section-number">5.2</span> TransE: the pioneering translation model</h2>
<p>TransE, introduced by Bordes et al.&nbsp;(2013), was the first translation-based model for knowledge graph embeddings and remains one of the most influential models in the field.</p>
<div id="def-transe" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 (TransE model)</strong></span> In the <strong>TransE</strong> model, entities and relations are embedded in the same vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math>. For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model enforces: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></p>
<p>The scoring function, which measures the plausibility of a triple, is defined as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><mi>𝐭</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>⋅</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\cdot\|</annotation></semantics></math> can be either the L1 or L2 norm.</p>
<p>Lower scores (smaller distances) indicate more plausible triples.</p>
</div>
<section id="geometric-interpretation" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="geometric-interpretation"><span class="header-section-number">5.2.1</span> Geometric interpretation</h3>
<p>TransE has a clear geometric interpretation: relations are translations in the embedding space. For a valid triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the tail entity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> should be close to the point reached by starting at head entity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> and moving along the relation vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐫</mi><annotation encoding="application/x-tex">\mathbf{r}</annotation></semantics></math>.</p>
<div id="exm-transe-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 (TransE geometric example)</strong></span> Consider a knowledge graph about countries and their capitals. In a well-trained TransE model, the embeddings might have the following properties:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>≈</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{France} + \mathbf{has\_capital} \approx \mathbf{Paris}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| \approx 0</annotation></semantics></math> (low distance, high plausibility)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐋</mi><mi>𝐨</mi><mi>𝐧</mi><mi>𝐝</mi><mi>𝐨</mi><mi>𝐧</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mo>≫</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\| \gg 0</annotation></semantics></math> (high distance, low plausibility)</li>
</ul>
<p>For a query about the capital of France, TransE would rank all entities by their proximity to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{France} + \mathbf{has\_capital}</annotation></semantics></math>. Paris would be ranked highly (ideally first) because it’s close to this point.</p>
</div>
</section>
<section id="learning-algorithm" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="learning-algorithm"><span class="header-section-number">5.2.2</span> Learning algorithm</h3>
<p>TransE is trained using a margin-based ranking loss:</p>
<div id="def-transe-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (TransE loss function)</strong></span> The TransE model is trained by minimizing the following margin-based ranking loss:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub></mrow></munder><msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>γ</mi><mo>+</mo><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo></msub></mrow><annotation encoding="application/x-tex">L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma + f_r(h, t) - f_r(h', t')]_+</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> is the set of valid triples in the training set</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">S'_{(h,r,t)}</annotation></semantics></math> is the set of corrupted triples created by replacing either <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> with a random entity</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\gamma &gt; 0</annotation></semantics></math> is a margin hyperparameter</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo></msub><mo>=</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">[x]_+ = \max(0, x)</annotation></semantics></math> denotes the positive part of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><mi>𝐭</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</annotation></semantics></math> is the scoring function</li>
</ul>
</div>
<p>This loss function encourages valid triples to have lower scores (smaller distances) than corrupted ones by at least a margin of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>.</p>
<div id="exm-transe-learning" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (TransE learning example)</strong></span> Consider a valid triple (France, has_capital, Paris) and a corrupted triple (France, has_capital, London).</p>
<p>The loss term for this pair would be: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">[</mo><mi>γ</mi><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mi>+</mi><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐋</mi><mi>𝐨</mi><mi>𝐧</mi><mi>𝐝</mi><mi>𝐨</mi><mi>𝐧</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo></msub><annotation encoding="application/x-tex">[\gamma - \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| + \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\|]_+</annotation></semantics></math></p>
<p>For effective training, we want: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mo>&lt;</mo><mo stretchy="false" form="postfix">∥</mo><mrow><mi>𝐅</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐚</mi><mi>𝐩</mi><mi>𝐢</mi><mi>𝐭</mi><mi>𝐚</mi><mi>𝐥</mi></mrow><mo>−</mo><mrow><mi>𝐋</mi><mi>𝐨</mi><mi>𝐧</mi><mi>𝐝</mi><mi>𝐨</mi><mi>𝐧</mi></mrow><mo stretchy="false" form="postfix">∥</mo><mo>−</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| &lt; \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\| - \gamma</annotation></semantics></math></p>
<p>This means the distance for the valid triple should be smaller than the distance for the corrupted triple by at least the margin <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>.</p>
</div>
</section>
<section id="transe-algorithm" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="transe-algorithm"><span class="header-section-number">5.2.3</span> TransE algorithm</h3>
<p>Here’s the algorithm for training a TransE model:</p>
<div id="def-transe-algorithm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 (TransE training algorithm)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Initialize</strong> entity and relation embeddings randomly</li>
<li><strong>Normalize</strong> entity embeddings to have unit L2 norm: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{e}\|_2 = 1</annotation></semantics></math> for all entities <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math></li>
<li><strong>For</strong> each training iteration: a. Sample a mini-batch of valid triples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>⊂</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">B \subset S</annotation></semantics></math> b. For each triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">(h, r, t) \in B</annotation></semantics></math>, create a corrupted triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h', r, t')</annotation></semantics></math> by replacing either <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> c.&nbsp;Compute the loss function for the mini-batch d.&nbsp;Update embeddings using stochastic gradient descent e. Re-normalize entity embeddings</li>
<li><strong>Return</strong> the learned entity and relation embeddings</li>
</ol>
</div>
<p>The normalization of entity embeddings is crucial for TransE. Without this constraint, the model could “cheat” by making the norms of entities very large while keeping relation norms small, satisfying the translation constraint without learning meaningful representations.</p>
</section>
<section id="strengths-and-limitations-of-transe" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="strengths-and-limitations-of-transe"><span class="header-section-number">5.2.4</span> Strengths and limitations of TransE</h3>
<p>TransE has several strengths that contributed to its popularity:</p>
<ol type="1">
<li><strong>Simplicity</strong>: The model is conceptually simple and easy to implement</li>
<li><strong>Efficiency</strong>: With only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|E|d + |R|d)</annotation></semantics></math> parameters, TransE is computationally efficient</li>
<li><strong>Effectiveness</strong>: Despite its simplicity, TransE performs well on many knowledge graph completion tasks</li>
</ol>
<p>However, TransE also has important limitations:</p>
<ol type="1">
<li><p><strong>One-to-many, many-to-one, and many-to-many relations</strong>: TransE struggles with these relation types because it enforces a single translation vector for each relation</p></li>
<li><p><strong>Symmetric relations</strong>: TransE cannot model symmetric relations well because if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐭</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{t} + \mathbf{r} \approx \mathbf{h}</annotation></semantics></math> (for a symmetric relation), then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{r} \approx \mathbf{0}</annotation></semantics></math></p></li>
<li><p><strong>Reflexive relations</strong>: Relations where an entity relates to itself (e.g., “is_similar_to”) are challenging because they would require <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{r} \approx \mathbf{0}</annotation></semantics></math></p></li>
</ol>
<div id="exm-transe-limitations" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 (TransE limitations example)</strong></span> Consider these triples with one-to-many relations:</p>
<ul>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
<li>(Barack_Obama, has_child, Sasha_Obama)</li>
</ul>
<p>TransE would try to enforce:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐡</mi><mi>𝐢</mi><mi>𝐥</mi><mi>𝐝</mi></mrow><mo>≈</mo><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Barack\_Obama} + \mathbf{has\_child} \approx \mathbf{Malia\_Obama}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>+</mo><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐡</mi><mi>𝐢</mi><mi>𝐥</mi><mi>𝐝</mi></mrow><mo>≈</mo><mrow><mi>𝐒</mi><mi>𝐚</mi><mi>𝐬</mi><mi>𝐡</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Barack\_Obama} + \mathbf{has\_child} \approx \mathbf{Sasha\_Obama}</annotation></semantics></math></li>
</ul>
<p>This implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>≈</mo><mrow><mi>𝐒</mi><mi>𝐚</mi><mi>𝐬</mi><mi>𝐡</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Malia\_Obama} \approx \mathbf{Sasha\_Obama}</annotation></semantics></math>, which is incorrect. The model cannot place the two children at different positions while maintaining the translation property for a single relation vector.</p>
</div>
<p>These limitations motivated researchers to develop extensions to the TransE model, which we’ll explore next.</p>
</section>
</section>
<section id="transh-modeling-relations-as-hyperplanes" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="transh-modeling-relations-as-hyperplanes"><span class="header-section-number">5.3</span> TransH: modeling relations as hyperplanes</h2>
<p>TransH (Wang et al., 2014) addresses the limitations of TransE by modeling each relation as a hyperplane, allowing entities to have relation-specific representations.</p>
<div id="def-transh" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 (TransH model)</strong></span> In the <strong>TransH</strong> model, each relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is associated with:</p>
<ol type="1">
<li>A normal vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐰</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{w}_r</annotation></semantics></math> defining a hyperplane</li>
<li>A translation vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐝</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{d}_r</annotation></semantics></math> on the hyperplane</li>
</ol>
<p>For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model:</p>
<ol type="1">
<li>Projects the head and tail entities onto the relation-specific hyperplane:
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mo>⊥</mo></msub><mo>=</mo><mi>𝐡</mi><mo>−</mo><msubsup><mi>𝐰</mi><mi>r</mi><mi>T</mi></msubsup><mi>𝐡</mi><msub><mi>𝐰</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^T \mathbf{h} \mathbf{w}_r</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐭</mi><mo>⊥</mo></msub><mo>=</mo><mi>𝐭</mi><mo>−</mo><msubsup><mi>𝐰</mi><mi>r</mi><mi>T</mi></msubsup><mi>𝐭</mi><msub><mi>𝐰</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^T \mathbf{t} \mathbf{w}_r</annotation></semantics></math></li>
</ul></li>
<li>Applies the translation on the hyperplane:
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mo>⊥</mo></msub><mo>+</mo><msub><mi>𝐝</mi><mi>r</mi></msub><mo>≈</mo><msub><mi>𝐭</mi><mo>⊥</mo></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}</annotation></semantics></math></li>
</ul></li>
</ol>
<p>The scoring function is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐡</mi><mo>⊥</mo></msub><mo>+</mo><msub><mi>𝐝</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐭</mi><mo>⊥</mo></msub><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h}_{\perp} + \mathbf{d}_r - \mathbf{t}_{\perp}\|_2^2</annotation></semantics></math></p>
</div>
<section id="geometric-interpretation-1" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="geometric-interpretation-1"><span class="header-section-number">5.3.1</span> Geometric interpretation</h3>
<p>In TransH, each relation corresponds to a different hyperplane in the embedding space. Entities are projected onto these hyperplanes before the translation is applied. This allows the same entity to have different representations when involved in different relations.</p>
<div id="exm-transh-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 (TransH geometric example)</strong></span> Consider triples involving a one-to-many relation:</p>
<ul>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
<li>(Barack_Obama, has_child, Sasha_Obama)</li>
</ul>
<p>In TransH, Barack_Obama would be projected onto the “has_child” hyperplane, resulting in a specific representation for the “has_child” relation. Then:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub><mo>+</mo><msub><mi>𝐝</mi><mtext mathvariant="normal">has_child</mtext></msub><mo>≈</mo><msub><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub></mrow><annotation encoding="application/x-tex">\mathbf{Barack\_Obama}_{\perp} + \mathbf{d}_{\text{has\_child}} \approx \mathbf{Malia\_Obama}_{\perp}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub><mo>+</mo><msub><mi>𝐝</mi><mtext mathvariant="normal">has_child</mtext></msub><mo>≈</mo><msub><mrow><mi>𝐒</mi><mi>𝐚</mi><mi>𝐬</mi><mi>𝐡</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub></mrow><annotation encoding="application/x-tex">\mathbf{Barack\_Obama}_{\perp} + \mathbf{d}_{\text{has\_child}} \approx \mathbf{Sasha\_Obama}_{\perp}</annotation></semantics></math></li>
</ul>
<p>Since the projections <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>𝐌</mi><mi>𝐚</mi><mi>𝐥</mi><mi>𝐢</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub><annotation encoding="application/x-tex">\mathbf{Malia\_Obama}_{\perp}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>𝐒</mi><mi>𝐚</mi><mi>𝐬</mi><mi>𝐡</mi><mi>𝐚</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mo>⊥</mo></msub><annotation encoding="application/x-tex">\mathbf{Sasha\_Obama}_{\perp}</annotation></semantics></math> can be different from the original embeddings, TransH can model this one-to-many relation effectively.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transh" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transh"><span class="header-section-number">5.3.2</span> Strengths and limitations of TransH</h3>
<p>TransH offers several advantages over TransE:</p>
<ol type="1">
<li><p><strong>Better handling of complex relations</strong>: TransH can model one-to-many, many-to-one, and many-to-many relations by using relation-specific entity representations</p></li>
<li><p><strong>Symmetric relations</strong>: TransH can model symmetric relations because the projections of head and tail entities can be the same while their original embeddings differ</p></li>
</ol>
<p>However, TransH also has limitations:</p>
<ol type="1">
<li><p><strong>Increased complexity</strong>: TransH has more parameters (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|E|d + 2|R|d)</annotation></semantics></math>) and is more complex than TransE</p></li>
<li><p><strong>Limited expressiveness</strong>: While TransH improves on TransE, it still has limitations in modeling certain relation patterns like composition</p></li>
</ol>
</section>
</section>
<section id="transr-modeling-relations-in-relation-specific-spaces" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="transr-modeling-relations-in-relation-specific-spaces"><span class="header-section-number">5.4</span> TransR: modeling relations in relation-specific spaces</h2>
<p>TransR (Lin et al., 2015) takes the idea of relation-specific representations further by projecting entities into entirely different vector spaces for each relation.</p>
<div id="def-transr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6 (TransR model)</strong></span> In the <strong>TransR</strong> model:</p>
<ol type="1">
<li>Entities are embedded in an entity space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></li>
<li>Relations are embedded in relation-specific spaces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^k</annotation></semantics></math></li>
<li>Each relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> has:
<ul>
<li>A projection matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>k</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{M}_r \in \mathbb{R}^{k \times d}</annotation></semantics></math> that maps from entity space to relation space</li>
<li>A translation vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{r} \in \mathbb{R}^k</annotation></semantics></math> in the relation space</li>
</ul></li>
</ol>
<p>For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model:</p>
<ol type="1">
<li>Projects entities into the relation space:
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mi>r</mi></msub><mo>=</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{h}_r = \mathbf{M}_r \mathbf{h}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐭</mi><mi>r</mi></msub><mo>=</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{t}_r = \mathbf{M}_r \mathbf{t}</annotation></semantics></math></li>
</ul></li>
<li>Applies the translation in the relation space:
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mi>r</mi></msub><mo>+</mo><mi>𝐫</mi><mo>≈</mo><msub><mi>𝐭</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_r + \mathbf{r} \approx \mathbf{t}_r</annotation></semantics></math></li>
</ul></li>
</ol>
<p>The scoring function is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{M}_r \mathbf{h} + \mathbf{r} - \mathbf{M}_r \mathbf{t}\|_2^2</annotation></semantics></math></p>
</div>
<section id="geometric-interpretation-2" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="geometric-interpretation-2"><span class="header-section-number">5.4.1</span> Geometric interpretation</h3>
<p>TransR uses different vector spaces for different relations. The projection matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐌</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{M}_r</annotation></semantics></math> transform entities from the entity space to relation-specific spaces, where translations are then applied.</p>
<div id="exm-transr-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6 (TransR geometric example)</strong></span> Consider the different semantics of relations:</p>
<ul>
<li>“is_capital_of” relates cities to countries</li>
<li>“was_born_in” relates people to locations</li>
</ul>
<p>In TransR, these relations would have different projection matrices that transform entities into relation-specific spaces:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐌</mi><mtext mathvariant="normal">is_capital_of</mtext></msub><annotation encoding="application/x-tex">\mathbf{M}_{\text{is\_capital\_of}}</annotation></semantics></math> projects entities into a space suitable for capital-country relationships</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐌</mi><mtext mathvariant="normal">was_born_in</mtext></msub><annotation encoding="application/x-tex">\mathbf{M}_{\text{was\_born\_in}}</annotation></semantics></math> projects entities into a space suitable for person-birthplace relationships</li>
</ul>
<p>This allows TransR to capture the different semantic properties of each relation type.</p>
</div>
</section>
<section id="cluster-based-transr-ctransr" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="cluster-based-transr-ctransr"><span class="header-section-number">5.4.2</span> Cluster-based TransR (CTransR)</h3>
<p>To further improve TransR, the authors proposed a cluster-based variant called CTransR, which clusters entity pairs for each relation and learns a distinct projection for each cluster.</p>
<div id="def-ctransr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.7 (CTransR model)</strong></span> The <strong>CTransR</strong> model extends TransR by:</p>
<ol type="1">
<li>For each relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, clustering the entity pairs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, t)</annotation></semantics></math> into several groups</li>
<li>Learning a distinct translation vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐫</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\mathbf{r}_c</annotation></semantics></math> for each cluster <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> of a relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math></li>
<li>Using the same projection matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐌</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{M}_r</annotation></semantics></math> for all clusters of a relation</li>
</ol>
<p>The scoring function becomes: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>r</mi><mo>,</mo><mi>c</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐡</mi><mo>+</mo><msub><mi>𝐫</mi><mi>c</mi></msub><mo>−</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">f_{r,c}(h, t) = -\|\mathbf{M}_r \mathbf{h} + \mathbf{r}_c - \mathbf{M}_r \mathbf{t}\|_2^2</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> is the cluster assignment for the entity pair <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, t)</annotation></semantics></math> under relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>.</p>
</div>
<p>This approach allows for more fine-grained modeling of relations, as different semantic aspects of a relation can be captured by different clusters.</p>
</section>
<section id="strengths-and-limitations-of-transr" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="strengths-and-limitations-of-transr"><span class="header-section-number">5.4.3</span> Strengths and limitations of TransR</h3>
<p>TransR offers several advantages:</p>
<ol type="1">
<li><p><strong>High expressiveness</strong>: By using relation-specific spaces, TransR can model complex relations more effectively than TransE and TransH</p></li>
<li><p><strong>Semantic differentiation</strong>: TransR can capture the different semantic characteristics of relations through their projection matrices</p></li>
</ol>
<p>However, TransR also has significant limitations:</p>
<ol type="1">
<li><p><strong>High computational complexity</strong>: With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|E|d + |R|k + |R|kd)</annotation></semantics></math> parameters, TransR is much more computationally expensive than TransE and TransH</p></li>
<li><p><strong>Risk of overfitting</strong>: The large number of parameters can lead to overfitting, especially for relations with few training examples</p></li>
</ol>
</section>
</section>
<section id="transd-dynamic-mapping-matrices" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="transd-dynamic-mapping-matrices"><span class="header-section-number">5.5</span> TransD: dynamic mapping matrices</h2>
<p>TransD (Ji et al., 2015) addresses the high computational complexity of TransR by using dynamic mapping matrices constructed from entity and relation vectors.</p>
<div id="def-transd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.8 (TransD model)</strong></span> In the <strong>TransD</strong> model:</p>
<ol type="1">
<li>Each entity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math> has two vector representations:
<ul>
<li>A standard embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{e} \in \mathbb{R}^d</annotation></semantics></math></li>
<li>A projection vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>p</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{e}_p \in \mathbb{R}^m</annotation></semantics></math></li>
</ul></li>
<li>Each relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> has two vector representations:
<ul>
<li>A standard embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{r} \in \mathbb{R}^k</annotation></semantics></math></li>
<li>A projection vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐫</mi><mi>p</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{r}_p \in \mathbb{R}^n</annotation></semantics></math></li>
</ul></li>
<li>Mapping matrices are dynamically constructed:
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>h</mi></mrow></msub><mo>=</mo><msub><mi>𝐫</mi><mi>p</mi></msub><msubsup><mi>𝐡</mi><mi>p</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>𝐈</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{M}_{rh} = \mathbf{r}_p \mathbf{h}_p^T + \mathbf{I}_{m \times n}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>𝐫</mi><mi>p</mi></msub><msubsup><mi>𝐭</mi><mi>p</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>𝐈</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{M}_{rt} = \mathbf{r}_p \mathbf{t}_p^T + \mathbf{I}_{m \times n}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐈</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{I}_{m \times n}</annotation></semantics></math> is a matrix with 1s on the diagonal and 0s elsewhere</li>
</ul></li>
</ol>
<p>The scoring function is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>h</mi></mrow></msub><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>t</mi></mrow></msub><mi>𝐭</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{M}_{rh} \mathbf{h} + \mathbf{r} - \mathbf{M}_{rt} \mathbf{t}\|_2^2</annotation></semantics></math></p>
</div>
<section id="geometric-interpretation-3" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="geometric-interpretation-3"><span class="header-section-number">5.5.1</span> Geometric interpretation</h3>
<p>TransD creates entity-relation-specific projection matrices by combining entity and relation projection vectors. This allows for more flexibility than TransR while maintaining computational efficiency.</p>
<div id="exm-transd-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (TransD geometric example)</strong></span> Consider an entity “Barack_Obama” involved in different relations:</p>
<ul>
<li>(Barack_Obama, was_president_of, United_States)</li>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
</ul>
<p>With TransD, “Barack_Obama” would have:</p>
<ul>
<li>A standard embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><annotation encoding="application/x-tex">\mathbf{Barack\_Obama}</annotation></semantics></math></li>
<li>A projection vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mi>p</mi></msub><annotation encoding="application/x-tex">\mathbf{Barack\_Obama}_p</annotation></semantics></math></li>
</ul>
<p>For each relation, different mapping matrices would be constructed:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mtext mathvariant="normal">was_president_of, Barack_Obama</mtext></msub><mo>=</mo><msub><mrow><mi>𝐰</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐩</mi><mi>𝐫</mi><mi>𝐞</mi><mi>𝐬</mi><mi>𝐢</mi><mi>𝐝</mi><mi>𝐞</mi><mi>𝐧</mi><mi>𝐭</mi><mi mathvariant="bold">_</mi><mi>𝐨</mi><mi>𝐟</mi></mrow><mi>p</mi></msub><msubsup><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mi>p</mi><mi>T</mi></msubsup><mo>+</mo><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_{\text{was\_president\_of, Barack\_Obama}} = \mathbf{was\_president\_of}_p \mathbf{Barack\_Obama}_p^T + \mathbf{I}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mtext mathvariant="normal">has_child, Barack_Obama</mtext></msub><mo>=</mo><msub><mrow><mi>𝐡</mi><mi>𝐚</mi><mi>𝐬</mi><mi mathvariant="bold">_</mi><mi>𝐜</mi><mi>𝐡</mi><mi>𝐢</mi><mi>𝐥</mi><mi>𝐝</mi></mrow><mi>p</mi></msub><msubsup><mrow><mi>𝐁</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐚</mi><mi>𝐜</mi><mi>𝐤</mi><mi mathvariant="bold">_</mi><mi>𝐎</mi><mi>𝐛</mi><mi>𝐚</mi><mi>𝐦</mi><mi>𝐚</mi></mrow><mi>p</mi><mi>T</mi></msubsup><mo>+</mo><mi>𝐈</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_{\text{has\_child, Barack\_Obama}} = \mathbf{has\_child}_p \mathbf{Barack\_Obama}_p^T + \mathbf{I}</annotation></semantics></math></li>
</ul>
<p>This allows for entity-relation-specific projections with fewer parameters than TransR.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transd" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transd"><span class="header-section-number">5.5.2</span> Strengths and limitations of TransD</h3>
<p>TransD offers several advantages:</p>
<ol type="1">
<li><p><strong>Reduced complexity</strong>: With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>+</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>+</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|E|(d+m) + |R|(k+n))</annotation></semantics></math> parameters, TransD is more efficient than TransR while still allowing for entity-relation-specific projections</p></li>
<li><p><strong>Flexibility</strong>: The dynamic mapping matrices can capture both entity-specific and relation-specific features</p></li>
</ol>
<p>However, TransD still has limitations:</p>
<ol type="1">
<li><p><strong>Increased complexity compared to TransE and TransH</strong>: The additional projection vectors add complexity to the model</p></li>
<li><p><strong>Limited projection capacity</strong>: The rank-1 projection matrices may not capture all necessary transformations for complex relations</p></li>
</ol>
</section>
</section>
<section id="transparse-adaptive-sparse-matrices" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="transparse-adaptive-sparse-matrices"><span class="header-section-number">5.6</span> TranSparse: adaptive sparse matrices</h2>
<p>TranSparse (Ji et al., 2016) introduces sparse mapping matrices to further reduce computational complexity while maintaining expressive power.</p>
<div id="def-transparse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.9 (TranSparse model)</strong></span> The <strong>TranSparse</strong> model extends TransR by using sparse projection matrices:</p>
<ol type="1">
<li>Each relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> has a sparsity degree <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>r</mi></msub><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\theta_r \in [0, 1]</annotation></semantics></math></li>
<li>The projection matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐌</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{M}_r</annotation></semantics></math> has a sparsity of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\theta_r</annotation></semantics></math> (proportion of elements set to zero)</li>
<li>Sparsity degrees are determined based on the number of entity pairs involving the relation</li>
</ol>
<p>Two variants exist:</p>
<ol type="1">
<li><strong>TranSparse(share)</strong>: Uses the same sparse pattern for all relations</li>
<li><strong>TranSparse(separate)</strong>: Uses different sparse patterns for each relation</li>
</ol>
<p>The scoring function is similar to TransR: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>−</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝐭</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{M}_r(\theta_r) \mathbf{h} + \mathbf{r} - \mathbf{M}_r(\theta_r) \mathbf{t}\|_2^2</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{M}_r(\theta_r)</annotation></semantics></math> is the sparse projection matrix for relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> with sparsity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\theta_r</annotation></semantics></math>.</p>
</div>
<section id="adaptive-sparsity" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="adaptive-sparsity"><span class="header-section-number">5.6.1</span> Adaptive sparsity</h3>
<p>A key innovation in TranSparse is the use of adaptive sparsity based on the frequency of relations:</p>
<ol type="1">
<li><p>Frequent relations (with many training examples) can use sparser matrices because they have sufficient data to learn even with fewer parameters</p></li>
<li><p>Rare relations (with few training examples) use denser matrices to maintain expressive power despite limited data</p></li>
</ol>
<div id="exm-transparse" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 (TranSparse example)</strong></span> Consider two relations:</p>
<ul>
<li>“is_capital_of” (appears in 200 triples)</li>
<li>“won_nobel_prize_in” (appears in 20 triples)</li>
</ul>
<p>TranSparse might assign:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mtext mathvariant="normal">is_capital_of</mtext></msub><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">\theta_{\text{is\_capital\_of}} = 0.8</annotation></semantics></math> (80% sparsity, only 20% of matrix elements are non-zero)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mtext mathvariant="normal">won_nobel_prize_in</mtext></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">\theta_{\text{won\_nobel\_prize\_in}} = 0.3</annotation></semantics></math> (30% sparsity, 70% of matrix elements are non-zero)</li>
</ul>
<p>This balances model complexity with the amount of available training data for each relation.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transparse" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transparse"><span class="header-section-number">5.6.2</span> Strengths and limitations of TranSparse</h3>
<p>TranSparse offers several advantages:</p>
<ol type="1">
<li><p><strong>Adaptive complexity</strong>: The sparsity of projection matrices adapts to the frequency of relations, reducing overfitting</p></li>
<li><p><strong>Computational efficiency</strong>: Sparse matrices require less memory and computational resources than dense matrices</p></li>
</ol>
<p>However, TranSparse still has limitations:</p>
<ol type="1">
<li><p><strong>Complex implementation</strong>: The management of sparse matrices adds implementation complexity</p></li>
<li><p><strong>Determining sparsity</strong>: Finding optimal sparsity degrees for each relation can be challenging</p></li>
</ol>
</section>
</section>
<section id="rotate-rotation-in-complex-space" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="rotate-rotation-in-complex-space"><span class="header-section-number">5.7</span> RotatE: rotation in complex space</h2>
<p>RotatE (Sun et al., 2019) is a more recent translation-based model that represents relations as rotations in complex vector space, allowing it to model various relation patterns.</p>
<div id="def-rotate" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.10 (RotatE model)</strong></span> In the <strong>RotatE</strong> model:</p>
<ol type="1">
<li>Entities and relations are embedded in complex space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math></li>
<li>Relations are modeled as rotations in the complex plane</li>
<li>For a triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math>, the model enforces: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐭</mi><mo>=</mo><mi>𝐡</mi><mo>∘</mo><mi>𝐫</mi></mrow><annotation encoding="application/x-tex">\mathbf{t} = \mathbf{h} \circ \mathbf{r}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∘</mi><annotation encoding="application/x-tex">\circ</annotation></semantics></math> is the Hadamard (element-wise) product, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>𝐫</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|\mathbf{r}_i| = 1</annotation></semantics></math> for all components <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math></li>
</ol>
<p>The scoring function is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐡</mi><mo>∘</mo><mi>𝐫</mi><mo>−</mo><mi>𝐭</mi><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f_r(h, t) = -\|\mathbf{h} \circ \mathbf{r} - \mathbf{t}\|^2</annotation></semantics></math></p>
</div>
<section id="geometric-interpretation-4" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="geometric-interpretation-4"><span class="header-section-number">5.7.1</span> Geometric interpretation</h3>
<p>In RotatE, each relation is a rotation in the complex plane. For each dimension of the embedding, the relation rotates the head entity’s component by a specific angle to approximate the tail entity’s component.</p>
<div id="exm-rotate-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9 (RotatE geometric example)</strong></span> Consider a single dimension in the complex embedding space:</p>
<ul>
<li>Head entity component: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">h_i = a + bi</annotation></semantics></math></li>
<li>Relation component: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mo>cos</mo><mi>θ</mi><mo>+</mo><mi>i</mi><mo>sin</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">r_i = \cos\theta + i\sin\theta</annotation></semantics></math> (unit modulus complex number)</li>
<li>Tail entity component: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>=</mo><mi>c</mi><mo>+</mo><mi>d</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">t_i = c + di</annotation></semantics></math></li>
</ul>
<p>The rotation is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>cos</mo><mi>θ</mi><mo>+</mo><mi>i</mi><mo>sin</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>a</mi><mo>cos</mo><mi>θ</mi><mo>−</mo><mi>b</mi><mo>sin</mo><mi>θ</mi><mo>+</mo><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>sin</mo><mi>θ</mi><mo>+</mo><mi>b</mi><mo>cos</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_i \cdot r_i = (a + bi)(\cos\theta + i\sin\theta) = a\cos\theta - b\sin\theta + i(a\sin\theta + b\cos\theta)</annotation></semantics></math></p>
<p>This rotates the complex number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mi>i</mi></msub><annotation encoding="application/x-tex">h_i</annotation></semantics></math> by angle <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> in the complex plane.</p>
<p>For symmetric relations (e.g., “is_sibling_of”), the rotation angle would be 0 or π, meaning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = 1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = -1</annotation></semantics></math>.</p>
<p>For asymmetric relations (e.g., “is_parent_of”), the rotation angle would be neither 0 nor π.</p>
<p>For inverse relations (e.g., “is*parent_of” and “is_child_of”), the rotation angles would be negatives of each other, meaning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>*</mo><mtext mathvariant="normal">is_child_of</mtext><mo>=</mo><mover><mrow><mi>r</mi><mi>_</mi><mtext mathvariant="normal">is_parent_of</mtext></mrow><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">r*{\text{is_child_of}} = \overline{r\_{\text{is_parent_of}}}</annotation></semantics></math> (complex conjugate).</p>
</div>
</section>
<section id="modeling-relation-patterns-with-rotate" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="modeling-relation-patterns-with-rotate"><span class="header-section-number">5.7.2</span> Modeling relation patterns with RotatE</h3>
<p>RotatE can model various relation patterns:</p>
<ol type="1">
<li><strong>Symmetry</strong>: For a symmetric relation, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = 1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = -1</annotation></semantics></math> for all dimensions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math></li>
<li><strong>Antisymmetry</strong>: For an antisymmetric relation, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>≠</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i \neq 1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>≠</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i \neq -1</annotation></semantics></math> for some dimensions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math></li>
<li><strong>Inversion</strong>: For inverse relations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>1</mn></msub><annotation encoding="application/x-tex">r_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>2</mn></msub><annotation encoding="application/x-tex">r_2</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>2</mn></msub><mo>=</mo><mover><msub><mi>r</mi><mn>1</mn></msub><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">r_2 = \overline{r_1}</annotation></semantics></math> (complex conjugate)</li>
<li><strong>Composition</strong>: For relations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>1</mn></msub><annotation encoding="application/x-tex">r_1</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>2</mn></msub><annotation encoding="application/x-tex">r_2</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>3</mn></msub><annotation encoding="application/x-tex">r_3</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>∘</mo><msub><mi>r</mi><mn>2</mn></msub><mo>=</mo><msub><mi>r</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">r_1 \circ r_2 = r_3</annotation></semantics></math>, the rotation angles add: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><msub><mi>r</mi><mn>3</mn></msub></msub><mo>=</mo><msub><mi>θ</mi><msub><mi>r</mi><mn>1</mn></msub></msub><mo>+</mo><msub><mi>θ</mi><msub><mi>r</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">\theta_{r_3} = \theta_{r_1} + \theta_{r_2}</annotation></semantics></math></li>
</ol>
<div id="exm-rotate-patterns" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10 (RotatE relation patterns example)</strong></span> Consider these relation patterns:</p>
<ol type="1">
<li><p><strong>Symmetry</strong> (e.g., “is_sibling_of”): If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>∘</mo><mi>𝐫</mi><mo>=</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} \circ \mathbf{r} = \mathbf{t}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐭</mi><mo>∘</mo><mi>𝐫</mi><mo>=</mo><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{t} \circ \mathbf{r} = \mathbf{h}</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>∘</mo><mi>𝐫</mi><mo>=</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">\mathbf{r} \circ \mathbf{r} = \mathbf{1}</annotation></semantics></math>, meaning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = 1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">r_i = -1</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p></li>
<li><p><strong>Inversion</strong> (e.g., “is_parent_of” and “is_child_of”): If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>∘</mo><msub><mi>𝐫</mi><mn>1</mn></msub><mo>=</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} \circ \mathbf{r}_1 = \mathbf{t}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐭</mi><mo>∘</mo><msub><mi>𝐫</mi><mn>2</mn></msub><mo>=</mo><mi>𝐡</mi></mrow><annotation encoding="application/x-tex">\mathbf{t} \circ \mathbf{r}_2 = \mathbf{h}</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐫</mi><mn>1</mn></msub><mo>∘</mo><msub><mi>𝐫</mi><mn>2</mn></msub><mo>=</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">\mathbf{r}_1 \circ \mathbf{r}_2 = \mathbf{1}</annotation></semantics></math>, meaning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>2</mn></msub><mo>=</mo><mover><msub><mi>r</mi><mn>1</mn></msub><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">r_2 = \overline{r_1}</annotation></semantics></math>.</p></li>
<li><p><strong>Composition</strong> (e.g., “is_born_in” and “is_located_in” compose to “has_nationality”): If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>∘</mo><msub><mi>𝐫</mi><mn>1</mn></msub><mo>=</mo><mi>𝐞</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} \circ \mathbf{r}_1 = \mathbf{e}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>∘</mo><msub><mi>𝐫</mi><mn>2</mn></msub><mo>=</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{e} \circ \mathbf{r}_2 = \mathbf{t}</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>∘</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐫</mi><mn>1</mn></msub><mo>∘</mo><msub><mi>𝐫</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} \circ (\mathbf{r}_1 \circ \mathbf{r}_2) = \mathbf{t}</annotation></semantics></math>, meaning <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐫</mi><mn>3</mn></msub><mo>=</mo><msub><mi>𝐫</mi><mn>1</mn></msub><mo>∘</mo><msub><mi>𝐫</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{r}_3 = \mathbf{r}_1 \circ \mathbf{r}_2</annotation></semantics></math>.</p></li>
</ol>
</div>
</section>
<section id="self-adversarial-negative-sampling" class="level3" data-number="5.7.3">
<h3 data-number="5.7.3" class="anchored" data-anchor-id="self-adversarial-negative-sampling"><span class="header-section-number">5.7.3</span> Self-adversarial negative sampling</h3>
<p>RotatE introduces an advanced training technique called self-adversarial negative sampling:</p>
<div id="def-self-adversarial" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.11 (Self-adversarial negative sampling)</strong></span> In <strong>self-adversarial negative sampling</strong>, the weight of a negative sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h', r, t')</annotation></semantics></math> is determined by its current score according to the model:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mi>j</mi></msub><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub></mrow></munder><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mi>j</mi></msub><mi>′</mi><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(h', r, t') = \frac{\exp(\alpha f_r(h', t'))}{\sum_{(h_j', r, t_j') \in S'_{(h,r,t)}} \exp(\alpha f_r(h_j', t_j'))}</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is a temperature hyperparameter.</p>
<p>The negative sampling loss becomes: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi>−</mi><mo>log</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>−</mo><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><munder><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><msub><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></msub></mrow></munder><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L = -\log\sigma(\gamma - f_r(h, t)) - \sum_{(h', r, t') \in S'_{(h,r,t)}} p(h', r, t') \log\sigma(f_r(h', t') - \gamma)</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is the sigmoid function and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the margin.</p>
</div>
<p>This approach weights negative samples based on their current scores, focusing training on “hard” negative samples (those that the model incorrectly scores as plausible).</p>
</section>
<section id="strengths-and-limitations-of-rotate" class="level3" data-number="5.7.4">
<h3 data-number="5.7.4" class="anchored" data-anchor-id="strengths-and-limitations-of-rotate"><span class="header-section-number">5.7.4</span> Strengths and limitations of RotatE</h3>
<p>RotatE offers several advantages:</p>
<ol type="1">
<li><p><strong>Modeling relation patterns</strong>: RotatE can model symmetry, antisymmetry, inversion, and composition relations in a unified framework</p></li>
<li><p><strong>Parameter efficiency</strong>: With <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(|E|d + |R|d)</annotation></semantics></math> parameters (where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the dimension of the complex space), RotatE is as efficient as TransE</p></li>
<li><p><strong>State-of-the-art performance</strong>: RotatE achieves strong results on various benchmark datasets</p></li>
</ol>
<p>However, RotatE still has limitations:</p>
<ol type="1">
<li><p><strong>Complex embeddings</strong>: Working with complex numbers adds some computational complexity</p></li>
<li><p><strong>Limited to rotation transformations</strong>: While rotations are powerful, some relation patterns might require more general transformations</p></li>
</ol>
</section>
</section>
<section id="comparing-translation-based-models" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="comparing-translation-based-models"><span class="header-section-number">5.8</span> Comparing translation-based models</h2>
<p>Let’s compare the key characteristics of the translation-based models we’ve discussed:</p>
<div id="def-model-comparison" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.12 (Comparison of translation-based models)</strong></span> &nbsp;</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Entity Space</th>
<th>Relation Space</th>
<th>Transformation</th>
<th>Parameters</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TransE</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td>Translation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | d + | R | d)</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>TransH</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td>Hyperplanes in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td>Projection + Translation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐡</mi><mo>⊥</mo></msub><mo>+</mo><msub><mi>𝐝</mi><mi>r</mi></msub><mo>≈</mo><msub><mi>𝐭</mi><mo>⊥</mo></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | d + 2 | R | d)</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>TransR</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^k</annotation></semantics></math></td>
<td>Projection + Translation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_r \mathbf{h} + \mathbf{r} \approx \mathbf{M}_r \mathbf{t}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | d + | R | k + | R | kd)</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>TransD</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo>,</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^d, \mathbb{R}^m</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ℝ</mi><mi>k</mi></msup><mo>,</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^k, \mathbb{R}^n</annotation></semantics></math></td>
<td>Dynamic Projection + Translation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>h</mi></mrow></msub><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><msub><mi>𝐌</mi><mrow><mi>r</mi><mi>t</mi></mrow></msub><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_{rh} \mathbf{h} + \mathbf{r} \approx \mathbf{M}_{rt} \mathbf{t}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>+</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>+</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | (d+m) + | R | (k+n))</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>TranSparse</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^k</annotation></semantics></math></td>
<td>Sparse Projection + Translation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐌</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝐡</mi><mo>+</mo><mi>𝐫</mi><mo>≈</mo><msub><mi>𝐌</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}_r(\theta_r) \mathbf{h} + \mathbf{r} \approx \mathbf{M}_r(\theta_r) \mathbf{t}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>k</mi><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | d + | R | k + (1-\theta) | R | kd)</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>RotatE</td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math></td>
<td>Complex rotations</td>
<td>Rotation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐡</mi><mo>∘</mo><mi>𝐫</mi><mo>≈</mo><mi>𝐭</mi></mrow><annotation encoding="application/x-tex">\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>E</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>R</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O( | E | d + | R | d)</annotation></semantics></math></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Each model makes different trade-offs between expressiveness, parameter efficiency, and computational complexity. The evolution of these models reflects a progression toward more flexible representations while trying to maintain computational efficiency.</p>
<div id="exm-model-evolution" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.11 (Model evolution example)</strong></span> Consider modeling a symmetric relation like “is_sibling_of”:</p>
<ol type="1">
<li><p><strong>TransE</strong> would struggle because it would require <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{r} \approx \mathbf{0}</annotation></semantics></math>, which doesn’t distinguish the relation from others.</p></li>
<li><p><strong>TransH</strong> would project entities onto a relation-specific hyperplane, allowing for symmetric relationships through equivalent projections.</p></li>
<li><p><strong>TransR</strong> would project entities into a relation-specific space where the translation could effectively model symmetry.</p></li>
<li><p><strong>RotatE</strong> would model the relation as rotations by 0 or π in the complex plane, naturally capturing symmetry.</p></li>
</ol>
<p>This evolution shows how each model addressed limitations of its predecessors.</p>
</div>
</section>
<section id="relation-patterns-and-model-capabilities" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="relation-patterns-and-model-capabilities"><span class="header-section-number">5.9</span> Relation patterns and model capabilities</h2>
<p>Different translation-based models have different capabilities for modeling relation patterns:</p>
<div id="def-pattern-capabilities" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.13 (Model capabilities for relation patterns)</strong></span> &nbsp;</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Relation Pattern</th>
<th>TransE</th>
<th>TransH</th>
<th>TransR</th>
<th>TransD</th>
<th>RotatE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-to-1</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>1-to-many</td>
<td>Weak</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>Many-to-1</td>
<td>Weak</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="even">
<td>Many-to-many</td>
<td>Weak</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>Symmetry</td>
<td>Weak</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Antisymmetry</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="odd">
<td>Inversion</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Composition</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
</tr>
</tbody>
</table>
</div>
<p>Understanding these capabilities helps in selecting the appropriate model for specific knowledge graphs based on the prevalent relation patterns.</p>
<div id="exm-pattern-selection" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.12 (Model selection example)</strong></span> Consider different knowledge graphs:</p>
<ol type="1">
<li><p><strong>Geographic knowledge graph</strong> (mostly 1-to-1 relations like “is_capital_of”):</p>
<ul>
<li>TransE might be sufficient and computationally efficient</li>
</ul></li>
<li><p><strong>Family relationship knowledge graph</strong> (many symmetric relations like “is_sibling_of” and inverse relations like “is_parent_of”/“is_child_of”):</p>
<ul>
<li>RotatE would be a good choice for capturing these patterns</li>
</ul></li>
<li><p><strong>Academic knowledge graph</strong> (many-to-many relations like “author_of” where an author can write multiple papers and a paper can have multiple authors):</p>
<ul>
<li>TransR or TransD might be more appropriate to handle the complex relationships</li>
</ul></li>
</ol>
</div>
</section>
<section id="implementation-considerations" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="implementation-considerations"><span class="header-section-number">5.10</span> Implementation considerations</h2>
<p>When implementing translation-based models, several practical considerations are important:</p>
<section id="normalization-constraints" class="level3" data-number="5.10.1">
<h3 data-number="5.10.1" class="anchored" data-anchor-id="normalization-constraints"><span class="header-section-number">5.10.1</span> Normalization constraints</h3>
<p>Many translation-based models apply normalization constraints to entity and relation embeddings:</p>
<div id="def-normalization-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.14 (Common normalization constraints)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>TransE</strong>: Entity embeddings are often constrained to unit L2 norm: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{e}\|_2 = 1</annotation></semantics></math></p></li>
<li><p><strong>TransH</strong>: Both entity embeddings and hyperplane normal vectors are normalized: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{e}\|_2 = 1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐰</mi><mi>r</mi></msub><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{w}_r\|_2 = 1</annotation></semantics></math></p></li>
<li><p><strong>TransR/TransD</strong>: Entity embeddings are normalized, and sometimes additional orthogonality constraints are applied to projection matrices</p></li>
<li><p><strong>RotatE</strong>: Relation embeddings are constrained to have unit modulus: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|r_i| = 1</annotation></semantics></math> for all components <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math></p></li>
</ol>
</div>
<p>These constraints prevent the model from “cheating” by scaling embeddings arbitrarily and ensure that the geometric interpretations remain valid.</p>
</section>
<section id="initialization-strategies" class="level3" data-number="5.10.2">
<h3 data-number="5.10.2" class="anchored" data-anchor-id="initialization-strategies"><span class="header-section-number">5.10.2</span> Initialization strategies</h3>
<p>Proper initialization of embeddings is crucial for effective training:</p>
<div id="def-initialization-strategies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.15 (Initialization strategies)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Uniform initialization</strong>: Sample from a uniform distribution, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mfrac><mn>1</mn><msqrt><mi>d</mi></msqrt></mfrac><mo>,</mo><mfrac><mn>1</mn><msqrt><mi>d</mi></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">U(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})</annotation></semantics></math></p></li>
<li><p><strong>Normal initialization</strong>: Sample from a normal distribution, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(0, \frac{1}{d})</annotation></semantics></math></p></li>
<li><p><strong>Xavier/Glorot initialization</strong>: Scale based on input and output dimensions</p></li>
<li><p><strong>Complex initialization</strong>: For RotatE, initialize relation embeddings with random phases but unit modulus</p></li>
</ol>
</div>
<div id="exm-initialization" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.13 (Initialization example)</strong></span> For TransE with embedding dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">d = 100</annotation></semantics></math>:</p>
<ol type="1">
<li>Initialize entity embeddings from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">U(-0.1, 0.1)</annotation></semantics></math></li>
<li>Initialize relation embeddings from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">U(-0.1, 0.1)</annotation></semantics></math></li>
<li>Normalize entity embeddings to have unit L2 norm</li>
<li>Begin training with these normalized initializations</li>
</ol>
</div>
</section>
<section id="negative-sampling-strategies" class="level3" data-number="5.10.3">
<h3 data-number="5.10.3" class="anchored" data-anchor-id="negative-sampling-strategies"><span class="header-section-number">5.10.3</span> Negative sampling strategies</h3>
<p>The selection of negative samples significantly impacts training effectiveness:</p>
<div id="def-negative-sampling" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.16 (Negative sampling strategies)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Random sampling</strong>: Replace head or tail entity with a random entity from the knowledge graph</p></li>
<li><p><strong>Bernoulli sampling</strong>: Choose whether to corrupt the head or tail based on the relation’s mapping properties (implemented in TransH, TransR, etc.)</p></li>
<li><p><strong>Self-adversarial sampling</strong>: Weight negative samples based on their current scores (implemented in RotatE)</p></li>
</ol>
</div>
<div id="exm-bernoulli-sampling" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.14 (Bernoulli sampling example)</strong></span> For a 1-to-many relation like “has_child”:</p>
<ul>
<li>The head entity (parent) has fewer valid tails than a tail entity (child) has valid heads</li>
<li>We should corrupt the head entity more often than the tail entity</li>
<li>Bernoulli sampling might use a head corruption probability of 0.8 and tail corruption probability of 0.2</li>
</ul>
<p>For a many-to-1 relation like “born_in”:</p>
<ul>
<li>The head entity (person) has fewer valid tails than a tail entity (location) has valid heads</li>
<li>We should corrupt the tail entity more often than the head entity</li>
<li>Bernoulli sampling might use a head corruption probability of 0.2 and tail corruption probability of 0.8</li>
</ul>
</div>
</section>
<section id="optimization-algorithms" class="level3" data-number="5.10.4">
<h3 data-number="5.10.4" class="anchored" data-anchor-id="optimization-algorithms"><span class="header-section-number">5.10.4</span> Optimization algorithms</h3>
<p>Various optimization algorithms can be used to train translation-based models:</p>
<div id="def-optimization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.17 (Optimization algorithms)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: Simple and widely used, but may require careful learning rate tuning</p></li>
<li><p><strong>AdaGrad</strong>: Adapts learning rates for each parameter based on historical gradients</p></li>
<li><p><strong>Adam</strong>: Combines adaptive learning rates with momentum for faster convergence</p></li>
<li><p><strong>L-BFGS</strong>: A second-order method that can work well for smaller datasets</p></li>
</ol>
</div>
<p>The choice of optimization algorithm depends on the specific model, dataset size, and computational resources.</p>
</section>
</section>
<section id="performance-analysis" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="performance-analysis"><span class="header-section-number">5.11</span> Performance analysis</h2>
<p>Let’s analyze the empirical performance of translation-based models on standard benchmark datasets:</p>
<section id="benchmark-datasets" class="level3" data-number="5.11.1">
<h3 data-number="5.11.1" class="anchored" data-anchor-id="benchmark-datasets"><span class="header-section-number">5.11.1</span> Benchmark datasets</h3>
<p>Several benchmark datasets are commonly used to evaluate knowledge graph embedding models:</p>
<div id="def-benchmarks" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.18 (Common benchmark datasets)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>FB15k</strong>: A subset of Freebase with 14,951 entities, 1,345 relations, and 592,213 triples</p></li>
<li><p><strong>FB15k-237</strong>: A modified version of FB15k with inverse relations removed, containing 14,541 entities, 237 relations, and 310,116 triples</p></li>
<li><p><strong>WN18</strong>: A subset of WordNet with 40,943 entities, 18 relations, and 151,442 triples</p></li>
<li><p><strong>WN18RR</strong>: A modified version of WN18 with inverse relations removed, containing 40,943 entities, 11 relations, and 93,003 triples</p></li>
<li><p><strong>YAGO3-10</strong>: A subset of YAGO3 with 123,182 entities, 37 relations, and 1,089,040 triples</p></li>
</ol>
</div>
</section>
<section id="evaluation-metrics" class="level3" data-number="5.11.2">
<h3 data-number="5.11.2" class="anchored" data-anchor-id="evaluation-metrics"><span class="header-section-number">5.11.2</span> Evaluation metrics</h3>
<p>Knowledge graph embedding models are typically evaluated using link prediction metrics:</p>
<div id="def-evaluation-metrics" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.19 (Common evaluation metrics)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Mean Rank (MR)</strong>: The average rank of the correct entity among all entities in the knowledge graph</p></li>
<li><p><strong>Mean Reciprocal Rank (MRR)</strong>: The average of the reciprocal ranks of the correct entities <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">MRR</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>Q</mi><mo stretchy="true" form="postfix">|</mo></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>Q</mi><mo stretchy="true" form="postfix">|</mo></mrow></munderover><mfrac><mn>1</mn><msub><mtext mathvariant="normal">rank</mtext><mi>i</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}</annotation></semantics></math></p></li>
<li><p><strong>Hits@k</strong>: The percentage of test cases where the correct entity appears in the top k predictions <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Hits@k</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>Q</mi><mo stretchy="true" form="postfix">|</mo></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>Q</mi><mo stretchy="true" form="postfix">|</mo></mrow></munderover><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">[</mo><msub><mtext mathvariant="normal">rank</mtext><mi>i</mi></msub><mo>≤</mo><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Hits@k} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \mathbf{1}[\text{rank}_i \leq k]</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>𝟏</mn><mrow><mo stretchy="true" form="prefix">[</mo><mi>⋅</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}[\cdot]</annotation></semantics></math> is the indicator function</p></li>
</ol>
</div>
<p>Lower MR and higher MRR and Hits@k indicate better performance.</p>
<div id="exm-evaluation" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.15 (Evaluation example)</strong></span> Consider a test triple (France, has_capital, Paris) and a model that ranks all entities for the query (France, has_capital, ?):</p>
<p>If the model ranks Paris as the 3rd entity (with Rome and Berlin incorrectly ranked higher):</p>
<ul>
<li>Rank = 3</li>
<li>Reciprocal Rank = 1/3</li>
<li>Hits@1 = 0 (incorrect)</li>
<li>Hits@3 = 1 (correct)</li>
<li>Hits@10 = 1 (correct)</li>
</ul>
</div>
</section>
<section id="comparative-results" class="level3" data-number="5.11.3">
<h3 data-number="5.11.3" class="anchored" data-anchor-id="comparative-results"><span class="header-section-number">5.11.3</span> Comparative results</h3>
<p>Based on published results, here’s a comparison of translation-based models on benchmark datasets:</p>
<div id="def-comparative-results" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.20 (Performance comparison (Hits@10 in %))</strong></span> &nbsp;</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>FB15k</th>
<th>WN18</th>
<th>FB15k-237</th>
<th>WN18RR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TransE</td>
<td>47.1</td>
<td>89.2</td>
<td>42.8</td>
<td>43.3</td>
</tr>
<tr class="even">
<td>TransH</td>
<td>64.4</td>
<td>86.7</td>
<td>41.5</td>
<td>43.0</td>
</tr>
<tr class="odd">
<td>TransR</td>
<td>68.7</td>
<td>92.0</td>
<td>40.8</td>
<td>43.2</td>
</tr>
<tr class="even">
<td>TransD</td>
<td>77.3</td>
<td>92.2</td>
<td>41.2</td>
<td>43.5</td>
</tr>
<tr class="odd">
<td>RotatE</td>
<td>83.1</td>
<td>95.9</td>
<td>47.6</td>
<td>57.1</td>
</tr>
</tbody>
</table>
</div>
<p>These results show a general trend of improved performance with more sophisticated models, with RotatE achieving the best results across datasets.</p>
<div id="exm-performance-analysis" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.16 (Performance analysis example)</strong></span> On FB15k, the progression from TransE (47.1%) to RotatE (83.1%) shows a dramatic improvement in Hits@10, likely due to:</p>
<ol type="1">
<li>TransE’s limitations with complex relations (FB15k contains many 1-to-many, many-to-1, and many-to-many relations)</li>
<li>RotatE’s ability to model various relation patterns through complex rotations</li>
<li>RotatE’s self-adversarial negative sampling technique, which improves training efficiency</li>
</ol>
<p>On simpler datasets like WN18, even basic models like TransE perform well (89.2% Hits@10) because the relations are primarily hierarchical and fit well with the translation approach.</p>
</div>
</section>
</section>
<section id="advanced-topics" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="advanced-topics"><span class="header-section-number">5.12</span> Advanced topics</h2>
<p>Several advanced topics extend the basic translation-based models:</p>
<section id="entity-type-constraints" class="level3" data-number="5.12.1">
<h3 data-number="5.12.1" class="anchored" data-anchor-id="entity-type-constraints"><span class="header-section-number">5.12.1</span> Entity type constraints</h3>
<p>Incorporating entity type information can improve performance by restricting the set of possible entities for a given relation:</p>
<div id="def-type-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.21 (Entity type constraints)</strong></span> <strong>Entity type constraints</strong> limit the possible head and tail entities for a relation based on their types.</p>
<p>For example, the relation “director_of” might constrain:</p>
<ul>
<li>Head entities to be of type “Person”</li>
<li>Tail entities to be of type “Movie” or “TVShow”</li>
</ul>
<p>These constraints can be integrated into translation-based models by:</p>
<ol type="1">
<li>Assigning types to entities in preprocessing</li>
<li>Only considering entities of the correct type during link prediction</li>
<li>Incorporating type information into the embedding space</li>
</ol>
</div>
<p>Type constraints can significantly improve performance by reducing the search space for link prediction.</p>
</section>
<section id="temporal-knowledge-graphs" class="level3" data-number="5.12.2">
<h3 data-number="5.12.2" class="anchored" data-anchor-id="temporal-knowledge-graphs"><span class="header-section-number">5.12.2</span> Temporal knowledge graphs</h3>
<p>Many real-world facts are valid only during specific time periods. Temporal knowledge graph embedding models extend translation-based approaches to handle time:</p>
<div id="def-temporal-kg" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.22 (Temporal knowledge graph embeddings)</strong></span> <strong>Temporal knowledge graph embeddings</strong> represent quadruples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>τ</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t, \tau)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> is a time point or interval.</p>
<p>Extensions of translation-based models include:</p>
<ol type="1">
<li><strong>TTransE</strong>: Adds a temporal translation vector</li>
<li><strong>HyTE</strong>: Projects entities and relations onto time-specific hyperplanes</li>
<li><strong>TA-TransE</strong>: Uses temporal attention mechanisms</li>
</ol>
</div>
<div id="exm-temporal" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.17 (Temporal knowledge graph example)</strong></span> Consider the fact “Barack Obama was president of the United States from 2009 to 2017.”</p>
<p>In a temporal knowledge graph, this might be represented as: (Barack_Obama, president_of, United_States, [2009, 2017])</p>
<p>A temporal embedding model would learn to predict not only the entities and relations but also when the fact was valid.</p>
</div>
</section>
<section id="multi-modal-knowledge-graphs" class="level3" data-number="5.12.3">
<h3 data-number="5.12.3" class="anchored" data-anchor-id="multi-modal-knowledge-graphs"><span class="header-section-number">5.12.3</span> Multi-modal knowledge graphs</h3>
<p>Some knowledge graphs incorporate multiple modalities, such as text, images, and numerical values:</p>
<div id="def-multimodal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.23 (Multi-modal knowledge graph embeddings)</strong></span> <strong>Multi-modal knowledge graph embeddings</strong> integrate information from different modalities:</p>
<ol type="1">
<li>Text descriptions of entities and relations</li>
<li>Visual information (images) associated with entities</li>
<li>Numerical attributes of entities</li>
</ol>
<p>Extensions of translation-based models include:</p>
<ol type="1">
<li><strong>IKRL</strong>: Incorporates image information into entity embeddings</li>
<li><strong>KDCoE</strong>: Jointly learns from textual descriptions and structured knowledge</li>
<li><strong>TransEA</strong>: Incorporates numerical attributes</li>
</ol>
</div>
<div id="exm-multimodal" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.18 (Multi-modal knowledge graph example)</strong></span> For the entity “Eiffel Tower,” a multi-modal knowledge graph might include:</p>
<ul>
<li>Structured facts: (Eiffel_Tower, located_in, Paris)</li>
<li>Textual description: “The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.”</li>
<li>Images: Several photographs of the Eiffel Tower</li>
<li>Numerical attributes: Height: 330m, Built: 1889</li>
</ul>
<p>A multi-modal embedding model would learn to integrate all these sources of information into a unified embedding space.</p>
</div>
</section>
<section id="uncertainty-modeling" class="level3" data-number="5.12.4">
<h3 data-number="5.12.4" class="anchored" data-anchor-id="uncertainty-modeling"><span class="header-section-number">5.12.4</span> Uncertainty modeling</h3>
<p>Real-world knowledge often involves uncertainty. Several extensions of translation-based models incorporate uncertainty:</p>
<div id="def-uncertainty" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.24 (Uncertainty in knowledge graph embeddings)</strong></span> <strong>Uncertainty modeling</strong> in knowledge graph embeddings represents entities and relations as distributions rather than point vectors:</p>
<ol type="1">
<li><strong>KG2E</strong>: Models entities and relations as Gaussian distributions</li>
<li><strong>TransG</strong>: Uses Gaussian mixtures for multi-component relations</li>
<li><strong>UKGE</strong>: Incorporates uncertainty through Bayesian approaches</li>
</ol>
</div>
<div id="exm-uncertainty" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.19 (Uncertainty modeling example)</strong></span> In KG2E, instead of representing the entity “Paris” as a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐏</mi><mi>𝐚</mi><mi>𝐫</mi><mi>𝐢</mi><mi>𝐬</mi></mrow><annotation encoding="application/x-tex">\mathbf{Paris}</annotation></semantics></math>, it would be represented as a Gaussian distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛍</mi><mtext mathvariant="normal">Paris</mtext></msub><mo>,</mo><msub><mi>𝚺</mi><mtext mathvariant="normal">Paris</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\boldsymbol{\mu}_{\text{Paris}}, \boldsymbol{\Sigma}_{\text{Paris}})</annotation></semantics></math>.</p>
<p>The mean vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛍</mi><mtext mathvariant="normal">Paris</mtext></msub><annotation encoding="application/x-tex">\boldsymbol{\mu}_{\text{Paris}}</annotation></semantics></math> represents the central tendency of the entity’s embedding, while the covariance matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mtext mathvariant="normal">Paris</mtext></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{\text{Paris}}</annotation></semantics></math> represents the uncertainty about this embedding.</p>
<p>Facts with high certainty would have lower variance, while facts with uncertainty would have higher variance.</p>
</div>
</section>
</section>
<section id="applications-of-translation-based-models" class="level2" data-number="5.13">
<h2 data-number="5.13" class="anchored" data-anchor-id="applications-of-translation-based-models"><span class="header-section-number">5.13</span> Applications of translation-based models</h2>
<p>Translation-based knowledge graph embedding models have been applied to various domains:</p>
<section id="recommendation-systems" class="level3" data-number="5.13.1">
<h3 data-number="5.13.1" class="anchored" data-anchor-id="recommendation-systems"><span class="header-section-number">5.13.1</span> Recommendation systems</h3>
<p>Knowledge graph embeddings can enhance recommendation systems by capturing complex relationships between users, items, and their attributes:</p>
<div id="def-recommendation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.25 (Knowledge graph-based recommendation)</strong></span> <strong>Knowledge graph-based recommendation</strong> systems use embeddings to:</p>
<ol type="1">
<li>Represent users, items, and their attributes in a unified embedding space</li>
<li>Model complex interaction patterns through relation embeddings</li>
<li>Generate recommendations based on embedding similarities or translations</li>
</ol>
</div>
<div id="exm-recommendation" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.20 (Recommendation system example)</strong></span> Consider a movie recommendation system:</p>
<ol type="1">
<li><strong>Entities</strong>: Users, movies, actors, directors, genres</li>
<li><strong>Relations</strong>: watched, acted_in, directed, has_genre</li>
<li><strong>Triples</strong>: (User1, watched, Movie1), (Actor1, acted_in, Movie1), (Movie1, has_genre, Action)</li>
</ol>
<p>A translation-based model might predict: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝐔</mi><mi>𝐬</mi><mi>𝐞</mi><mi>𝐫</mi><mn>𝟏</mn></mrow><mo>+</mo><mrow><mi>𝐥</mi><mi>𝐢</mi><mi>𝐤</mi><mi>𝐞</mi><mi>𝐬</mi></mrow><mo>≈</mo><mrow><mi>𝐀</mi><mi>𝐜</mi><mi>𝐭</mi><mi>𝐢</mi><mi>𝐨</mi><mi>𝐧</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{User1} + \mathbf{likes} \approx \mathbf{Action}</annotation></semantics></math></p>
<p>This could be used to recommend action movies that the user hasn’t watched yet.</p>
</div>
</section>
<section id="question-answering" class="level3" data-number="5.13.2">
<h3 data-number="5.13.2" class="anchored" data-anchor-id="question-answering"><span class="header-section-number">5.13.2</span> Question answering</h3>
<p>Knowledge graph embeddings can support question answering systems by enabling semantic matching and inference:</p>
<div id="def-question-answering" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.26 (Knowledge graph-based question answering)</strong></span> <strong>Knowledge graph-based question answering</strong> systems use embeddings to:</p>
<ol type="1">
<li>Map natural language questions to knowledge graph queries</li>
<li>Perform link prediction to find answers</li>
<li>Rank candidate answers based on embedding scores</li>
</ol>
</div>
<div id="exm-question-answering" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.21 (Question answering example)</strong></span> For the question “Who directed Inception?”, a system might:</p>
<ol type="1">
<li>Identify “Inception” as an entity and “directed” as a relation</li>
<li>Formulate a query (?, directed, Inception)</li>
<li>Use a translation-based model to rank entities based on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><mo>+</mo><mrow><mi>𝐝</mi><mi>𝐢</mi><mi>𝐫</mi><mi>𝐞</mi><mi>𝐜</mi><mi>𝐭</mi><mi>𝐞</mi><mi>𝐝</mi></mrow><mo>−</mo><mrow><mi>𝐈</mi><mi>𝐧</mi><mi>𝐜</mi><mi>𝐞</mi><mi>𝐩</mi><mi>𝐭</mi><mi>𝐢</mi><mi>𝐨</mi><mi>𝐧</mi></mrow><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\mathbf{e} + \mathbf{directed} - \mathbf{Inception}\|</annotation></semantics></math></li>
<li>Return “Christopher Nolan” as the top-ranked entity</li>
</ol>
</div>
</section>
<section id="information-extraction" class="level3" data-number="5.13.3">
<h3 data-number="5.13.3" class="anchored" data-anchor-id="information-extraction"><span class="header-section-number">5.13.3</span> Information extraction</h3>
<p>Knowledge graph embeddings can assist in information extraction by providing semantic context for entity and relation extraction:</p>
<div id="def-information-extraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.27 (Knowledge graph-based information extraction)</strong></span> <strong>Knowledge graph-based information extraction</strong> uses embeddings to:</p>
<ol type="1">
<li>Identify candidate entities and relations in text</li>
<li>Score candidate triples based on their plausibility in the knowledge graph</li>
<li>Filter or rank extraction results based on embedding scores</li>
</ol>
</div>
<div id="exm-information-extraction" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.22 (Information extraction example)</strong></span> Given the text “Tim Cook is the CEO of Apple, which is headquartered in Cupertino”:</p>
<ol type="1">
<li><p>An information extraction system might identify candidate triples:</p>
<ul>
<li>(Tim_Cook, is_CEO_of, Apple)</li>
<li>(Apple, headquartered_in, Cupertino)</li>
</ul></li>
<li><p>A translation-based model could score these triples based on existing knowledge graph embeddings</p></li>
<li><p>High scores would indicate consistency with existing knowledge, supporting the extraction</p></li>
</ol>
</div>
</section>
</section>
<section id="future-directions" class="level2" data-number="5.14">
<h2 data-number="5.14" class="anchored" data-anchor-id="future-directions"><span class="header-section-number">5.14</span> Future directions</h2>
<p>Translation-based models continue to evolve, with several promising research directions:</p>
<section id="inductive-learning" class="level3" data-number="5.14.1">
<h3 data-number="5.14.1" class="anchored" data-anchor-id="inductive-learning"><span class="header-section-number">5.14.1</span> Inductive learning</h3>
<p>Most current translation-based models are transductive, meaning they can only make predictions about entities seen during training. Inductive models aim to handle new entities:</p>
<div id="def-inductive-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.28 (Inductive translation-based models)</strong></span> <strong>Inductive translation-based models</strong> can generate embeddings for previously unseen entities based on their relations with known entities or their attributes.</p>
<p>Approaches include:</p>
<ol type="1">
<li><strong>Graph neural network extensions</strong> that generate embeddings based on local graph structure</li>
<li><strong>Textual description incorporation</strong> to generate embeddings from entity names or descriptions</li>
<li><strong>Attribute-based models</strong> that leverage entity attributes for inductive learning</li>
</ol>
</div>
</section>
<section id="neural-architecture-integration" class="level3" data-number="5.14.2">
<h3 data-number="5.14.2" class="anchored" data-anchor-id="neural-architecture-integration"><span class="header-section-number">5.14.2</span> Neural architecture integration</h3>
<p>Integrating translation-based models with neural architectures can enhance their expressiveness:</p>
<div id="def-neural-integration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.29 (Neural-enhanced translation models)</strong></span> <strong>Neural-enhanced translation models</strong> combine the geometric intuition of translation-based models with the expressive power of neural networks:</p>
<ol type="1">
<li><strong>Graph neural networks</strong> for entity representation learning</li>
<li><strong>Attention mechanisms</strong> to dynamically weight different aspects of embeddings</li>
<li><strong>Transformer-based architectures</strong> for context-aware embeddings</li>
</ol>
</div>
</section>
<section id="explainable-embeddings" class="level3" data-number="5.14.3">
<h3 data-number="5.14.3" class="anchored" data-anchor-id="explainable-embeddings"><span class="header-section-number">5.14.3</span> Explainable embeddings</h3>
<p>Enhancing the explainability of translation-based models is another important research direction:</p>
<div id="def-explainable-embeddings" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.30 (Explainable translation-based models)</strong></span> <strong>Explainable translation-based models</strong> provide interpretable justifications for their predictions:</p>
<ol type="1">
<li><strong>Path-based explanations</strong> that identify relevant paths in the knowledge graph</li>
<li><strong>Rule-based interpretations</strong> that extract symbolic rules from embeddings</li>
<li><strong>Attribution methods</strong> that identify the most influential components of embeddings</li>
</ol>
</div>
</section>
</section>
<section id="summary" class="level2" data-number="5.15">
<h2 data-number="5.15" class="anchored" data-anchor-id="summary"><span class="header-section-number">5.15</span> Summary</h2>
<p>In this chapter, we’ve explored translation-based embedding models for knowledge graphs, starting with the foundational TransE model and progressing through various extensions:</p>
<ul>
<li>TransE introduced the basic idea of representing relations as translations in the embedding space</li>
<li>TransH extended this by projecting entities onto relation-specific hyperplanes</li>
<li>TransR further generalized the approach by mapping entities to relation-specific spaces</li>
<li>TransD optimized the projection matrices through dynamic construction</li>
<li>TranSparse introduced adaptive sparse matrices to balance expressiveness and efficiency</li>
<li>RotatE leveraged complex vector spaces to model various relation patterns through rotations</li>
</ul>
<p>These models demonstrate a progression toward more flexible and expressive representations while trying to maintain computational efficiency. Each model makes different trade-offs and has different capabilities for modeling various relation patterns.</p>
<p>Translation-based models provide an intuitive geometric interpretation of knowledge graph relationships and serve as the foundation for many more sophisticated approaches. Despite the development of alternative paradigms, the principles introduced by translation-based models continue to influence the field of knowledge graph embeddings.</p>
</section>
<section id="further-reading" class="level2" data-number="5.16">
<h2 data-number="5.16" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">5.16</span> Further reading</h2>
<section id="original-papers" class="level3" data-number="5.16.1">
<h3 data-number="5.16.1" class="anchored" data-anchor-id="original-papers"><span class="header-section-number">5.16.1</span> Original papers</h3>
<ul>
<li>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems (pp.&nbsp;2787-2795).</li>
<li>Wang, Z., Zhang, J., Feng, J., &amp; Chen, Z. (2014). Knowledge Graph Embedding by Translating on Hyperplanes. In AAAI Conference on Artificial Intelligence (pp.&nbsp;1112-1119).</li>
<li>Lin, Y., Liu, Z., Sun, M., Liu, Y., &amp; Zhu, X. (2015). Learning Entity and Relation Embeddings for Knowledge Graph Completion. In AAAI Conference on Artificial Intelligence (pp.&nbsp;2181-2187).</li>
<li>Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge Graph Embedding via Dynamic Mapping Matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (pp.&nbsp;687-696).</li>
<li>Ji, G., Liu, K., He, S., &amp; Zhao, J. (2016). Knowledge Graph Completion with Adaptive Sparse Transfer Matrix. In AAAI Conference on Artificial Intelligence (pp.&nbsp;985-991).</li>
<li>Sun, Z., Deng, Z. H., Nie, J. Y., &amp; Tang, J. (2019). RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. In International Conference on Learning Representations.</li>
</ul>
</section>
<section id="surveys-and-comparative-analyses" class="level3" data-number="5.16.2">
<h3 data-number="5.16.2" class="anchored" data-anchor-id="surveys-and-comparative-analyses"><span class="header-section-number">5.16.2</span> Surveys and comparative analyses</h3>
<ul>
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge Graph Embedding: A Survey of Approaches and Applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
<li>Rossi, A., Barbosa, D., Firmani, D., Matinata, A., &amp; Merialdo, P. (2021). Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. ACM Transactions on Knowledge Discovery from Data, 15(2), 1-49.</li>
<li>Ali, M., Berrendorf, M., Hoyt, C. T., Vermue, L., Galkin, M., Sharifzadeh, S., … &amp; Lehmann, J. (2021). PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings. Journal of Machine Learning Research, 22(82), 1-6.</li>
</ul>
</section>
<section id="applications-and-extensions" class="level3" data-number="5.16.3">
<h3 data-number="5.16.3" class="anchored" data-anchor-id="applications-and-extensions"><span class="header-section-number">5.16.3</span> Applications and extensions</h3>
<ul>
<li>Zhang, F., Yuan, N. J., Lian, D., Xie, X., &amp; Ma, W. Y. (2016). Collaborative Knowledge Base Embedding for Recommender Systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;353-362).</li>
<li>Xie, R., Liu, Z., &amp; Sun, M. (2016). Representation Learning of Knowledge Graphs with Hierarchical Types. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (pp.&nbsp;2965-2971).</li>
<li>Lacroix, T., Usunier, N., &amp; Obozinski, G. (2018). Canonical Tensor Decomposition for Knowledge Base Completion. In Proceedings of the 35th International Conference on Machine Learning (pp.&nbsp;2863-2872).</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/embedding.html" class="pagination-link" aria-label="Fundamentals of Vector Space Representations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/semantic-matching.html" class="pagination-link" aria-label="Semantic Matching Models">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>