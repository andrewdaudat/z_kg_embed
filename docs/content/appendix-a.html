<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Appendix A: Mathematical Foundations – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/appendix-b.html" rel="next">
<link href="../content/frontier.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/appendix-a.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-mathematical-foundations" id="toc-introduction-to-mathematical-foundations" class="nav-link active" data-scroll-target="#introduction-to-mathematical-foundations"><span class="header-section-number">15.1</span> Introduction to Mathematical Foundations</a></li>
  <li><a href="#linear-algebra-essentials" id="toc-linear-algebra-essentials" class="nav-link" data-scroll-target="#linear-algebra-essentials"><span class="header-section-number">15.2</span> Linear Algebra Essentials</a>
  <ul class="collapse">
  <li><a href="#vectors-and-vector-spaces" id="toc-vectors-and-vector-spaces" class="nav-link" data-scroll-target="#vectors-and-vector-spaces"><span class="header-section-number">15.2.1</span> Vectors and Vector Spaces</a></li>
  <li><a href="#basis-and-dimension" id="toc-basis-and-dimension" class="nav-link" data-scroll-target="#basis-and-dimension"><span class="header-section-number">15.2.2</span> Basis and Dimension</a></li>
  <li><a href="#inner-products-and-norms" id="toc-inner-products-and-norms" class="nav-link" data-scroll-target="#inner-products-and-norms"><span class="header-section-number">15.2.3</span> Inner Products and Norms</a></li>
  <li><a href="#matrices-and-linear-transformations" id="toc-matrices-and-linear-transformations" class="nav-link" data-scroll-target="#matrices-and-linear-transformations"><span class="header-section-number">15.2.4</span> Matrices and Linear Transformations</a></li>
  <li><a href="#matrix-properties-and-decompositions" id="toc-matrix-properties-and-decompositions" class="nav-link" data-scroll-target="#matrix-properties-and-decompositions"><span class="header-section-number">15.2.5</span> Matrix Properties and Decompositions</a></li>
  </ul></li>
  <li><a href="#tensor-algebra" id="toc-tensor-algebra" class="nav-link" data-scroll-target="#tensor-algebra"><span class="header-section-number">15.3</span> Tensor Algebra</a>
  <ul class="collapse">
  <li><a href="#tensor-definitions-and-operations" id="toc-tensor-definitions-and-operations" class="nav-link" data-scroll-target="#tensor-definitions-and-operations"><span class="header-section-number">15.3.1</span> Tensor Definitions and Operations</a></li>
  <li><a href="#tensor-decompositions" id="toc-tensor-decompositions" class="nav-link" data-scroll-target="#tensor-decompositions"><span class="header-section-number">15.3.2</span> Tensor Decompositions</a></li>
  </ul></li>
  <li><a href="#complex-vector-spaces" id="toc-complex-vector-spaces" class="nav-link" data-scroll-target="#complex-vector-spaces"><span class="header-section-number">15.4</span> Complex Vector Spaces</a>
  <ul class="collapse">
  <li><a href="#complex-numbers-and-operations" id="toc-complex-numbers-and-operations" class="nav-link" data-scroll-target="#complex-numbers-and-operations"><span class="header-section-number">15.4.1</span> Complex Numbers and Operations</a></li>
  <li><a href="#complex-matrix-operations" id="toc-complex-matrix-operations" class="nav-link" data-scroll-target="#complex-matrix-operations"><span class="header-section-number">15.4.2</span> Complex Matrix Operations</a></li>
  </ul></li>
  <li><a href="#probability-and-statistics" id="toc-probability-and-statistics" class="nav-link" data-scroll-target="#probability-and-statistics"><span class="header-section-number">15.5</span> Probability and Statistics</a>
  <ul class="collapse">
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions"><span class="header-section-number">15.5.1</span> Probability Distributions</a></li>
  <li><a href="#bayesian-statistics" id="toc-bayesian-statistics" class="nav-link" data-scroll-target="#bayesian-statistics"><span class="header-section-number">15.5.2</span> Bayesian Statistics</a></li>
  <li><a href="#information-theory" id="toc-information-theory" class="nav-link" data-scroll-target="#information-theory"><span class="header-section-number">15.5.3</span> Information Theory</a></li>
  </ul></li>
  <li><a href="#metric-spaces-and-distance-functions" id="toc-metric-spaces-and-distance-functions" class="nav-link" data-scroll-target="#metric-spaces-and-distance-functions"><span class="header-section-number">15.6</span> Metric Spaces and Distance Functions</a>
  <ul class="collapse">
  <li><a href="#non-euclidean-geometries" id="toc-non-euclidean-geometries" class="nav-link" data-scroll-target="#non-euclidean-geometries"><span class="header-section-number">15.6.1</span> Non-Euclidean Geometries</a></li>
  </ul></li>
  <li><a href="#graph-theory-basics" id="toc-graph-theory-basics" class="nav-link" data-scroll-target="#graph-theory-basics"><span class="header-section-number">15.7</span> Graph Theory Basics</a></li>
  <li><a href="#optimization-theory" id="toc-optimization-theory" class="nav-link" data-scroll-target="#optimization-theory"><span class="header-section-number">15.8</span> Optimization Theory</a>
  <ul class="collapse">
  <li><a href="#objective-functions" id="toc-objective-functions" class="nav-link" data-scroll-target="#objective-functions"><span class="header-section-number">15.8.1</span> Objective Functions</a></li>
  <li><a href="#gradient-descent-and-variants" id="toc-gradient-descent-and-variants" class="nav-link" data-scroll-target="#gradient-descent-and-variants"><span class="header-section-number">15.8.2</span> Gradient Descent and Variants</a></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques"><span class="header-section-number">15.8.3</span> Regularization Techniques</a></li>
  </ul></li>
  <li><a href="#differential-calculus-for-optimization" id="toc-differential-calculus-for-optimization" class="nav-link" data-scroll-target="#differential-calculus-for-optimization"><span class="header-section-number">15.9</span> Differential Calculus for Optimization</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">15.10</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="introduction-to-mathematical-foundations" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="introduction-to-mathematical-foundations"><span class="header-section-number">15.1</span> Introduction to Mathematical Foundations</h2>
<p>This appendix provides a comprehensive overview of the mathematical concepts that underpin knowledge graph embeddings. While the main chapters introduce these concepts as needed, this appendix serves as a more formal and detailed reference. Understanding these mathematical foundations is essential for both implementing existing knowledge graph embedding models and developing new approaches.</p>
<p>We begin with the basics of linear algebra and vector spaces, progress through more advanced topics like tensor algebra and complex vector spaces, and conclude with optimization theory concepts relevant to training knowledge graph embeddings. Each section aims to balance formal definitions with intuitive explanations and relevant examples.</p>
<p>This appendix is designed to be accessible to students with varying levels of mathematical background, while providing the depth necessary for a thorough understanding of the theoretical foundations of knowledge graph embeddings.</p>
</section>
<section id="linear-algebra-essentials" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="linear-algebra-essentials"><span class="header-section-number">15.2</span> Linear Algebra Essentials</h2>
<p>Linear algebra provides the fundamental mathematical framework for knowledge graph embeddings, as entities and relations are represented as vectors, matrices, or tensors.</p>
<section id="vectors-and-vector-spaces" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="vectors-and-vector-spaces"><span class="header-section-number">15.2.1</span> Vectors and Vector Spaces</h3>
<p>A vector space forms the basic mathematical structure in which knowledge graph embeddings operate.</p>
<div id="def-vector-space-formal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.1 (Vector Space (Formal Definition))</strong></span> A vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> over a field <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝔽</mi><annotation encoding="application/x-tex">\mathbb{F}</annotation></semantics></math> consists of a set of vectors along with two operations:</p>
<ol type="1">
<li>Vector addition: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>+</mi><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">+: V \times V \rightarrow V</annotation></semantics></math></li>
<li>Scalar multiplication: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>⋅</mi><mo>:</mo><mi>𝔽</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\cdot: \mathbb{F} \times V \rightarrow V</annotation></semantics></math></li>
</ol>
<p>These operations must satisfy the following axioms for all vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{u}, \mathbf{v}, \mathbf{w} \in V</annotation></semantics></math> and scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>,</mo><mi>β</mi><mo>∈</mo><mi>𝔽</mi></mrow><annotation encoding="application/x-tex">\alpha, \beta \in \mathbb{F}</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>Closure under addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{u} + \mathbf{v} \in V</annotation></semantics></math></li>
<li><strong>Commutativity of addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo>=</mo><mi>𝐯</mi><mo>+</mo><mi>𝐮</mi></mrow><annotation encoding="application/x-tex">\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}</annotation></semantics></math></li>
<li><strong>Associativity of addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>𝐰</mi><mo>=</mo><mi>𝐮</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo>+</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})</annotation></semantics></math></li>
<li><strong>Additive identity</strong>: There exists <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>𝟎</mn><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{0} \in V</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>+</mo><mn>𝟎</mn><mo>=</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">\mathbf{v} + \mathbf{0} = \mathbf{v}</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{v} \in V</annotation></semantics></math></li>
<li><strong>Additive inverse</strong>: For each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{v} \in V</annotation></semantics></math>, there exists <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">-\mathbf{v} \in V</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{v} + (-\mathbf{v}) = \mathbf{0}</annotation></semantics></math></li>
<li><strong>Closure under scalar multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>⋅</mo><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\alpha \cdot \mathbf{v} \in V</annotation></semantics></math></li>
<li><strong>Distributivity of scalar multiplication with respect to vector addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>α</mi><mo>⋅</mo><mi>𝐮</mi><mo>+</mo><mi>α</mi><mo>⋅</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">\alpha \cdot (\mathbf{u} + \mathbf{v}) = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}</annotation></semantics></math></li>
<li><strong>Distributivity of scalar multiplication with respect to field addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo>+</mo><mi>β</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mi>𝐯</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mi>𝐯</mi><mo>+</mo><mi>β</mi><mo>⋅</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">(\alpha + \beta) \cdot \mathbf{v} = \alpha \cdot \mathbf{v} + \beta \cdot \mathbf{v}</annotation></semantics></math></li>
<li><strong>Compatibility of scalar multiplication with field multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>β</mi><mo>⋅</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>β</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">\alpha \cdot (\beta \cdot \mathbf{v}) = (\alpha \beta) \cdot \mathbf{v}</annotation></semantics></math></li>
<li><strong>Scalar multiplication identity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>⋅</mo><mi>𝐯</mi><mo>=</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">1 \cdot \mathbf{v} = \mathbf{v}</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> is the multiplicative identity in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝔽</mi><annotation encoding="application/x-tex">\mathbb{F}</annotation></semantics></math></li>
</ol>
</div>
<p>In the context of knowledge graph embeddings, we typically work with the vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math> (or sometimes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math>), where vectors are represented as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-tuples of real (or complex) numbers.</p>
<div id="exm-vector-space-r3" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.1 (Vector Space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math>)</strong></span> The vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math> consists of all ordered triplets <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, y, z)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">x, y, z \in \mathbb{R}</annotation></semantics></math>.</p>
<p>Vector addition: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo>+</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(x_1, y_1, z_1) + (x_2, y_2, z_2) = (x_1 + x_2, y_1 + y_2, z_1 + z_2)</annotation></semantics></math> Scalar multiplication: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>x</mi><mo>,</mo><mi>α</mi><mi>y</mi><mo>,</mo><mi>α</mi><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha \cdot (x, y, z) = (\alpha x, \alpha y, \alpha z)</annotation></semantics></math></p>
<p>For example:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>9</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(1, 2, 3) + (4, 5, 6) = (5, 7, 9)</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">2 \cdot (1, 2, 3) = (2, 4, 6)</annotation></semantics></math></li>
</ul>
<p>In knowledge graph embeddings, each entity and relation would be represented as a vector in a similar fashion, though typically in a much higher-dimensional space.</p>
</div>
</section>
<section id="basis-and-dimension" class="level3" data-number="15.2.2">
<h3 data-number="15.2.2" class="anchored" data-anchor-id="basis-and-dimension"><span class="header-section-number">15.2.2</span> Basis and Dimension</h3>
<p>The concepts of basis and dimension are fundamental to understanding vector spaces.</p>
<div id="def-basis-dimension" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.2 (Basis and Dimension)</strong></span> A set of vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>𝐯</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐯</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>𝐯</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}</annotation></semantics></math> in a vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> is a <strong>basis</strong> if:</p>
<ol type="1">
<li>The vectors are linearly independent: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝐯</mi><mn>1</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><msub><mi>𝐯</mi><mn>2</mn></msub><mo>+</mo><mi>…</mi><mo>+</mo><msub><mi>α</mi><mi>n</mi></msub><msub><mi>𝐯</mi><mi>n</mi></msub><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \ldots + \alpha_n\mathbf{v}_n = \mathbf{0}</annotation></semantics></math> implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>=</mo><msub><mi>α</mi><mn>2</mn></msub><mo>=</mo><mi>…</mi><mo>=</mo><msub><mi>α</mi><mi>n</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0</annotation></semantics></math>.</li>
<li>The vectors span the space: Every vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{v} \in V</annotation></semantics></math> can be written as a linear combination of the basis vectors: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>𝐯</mi><mn>1</mn></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msub><mi>𝐯</mi><mn>2</mn></msub><mo>+</mo><mi>…</mi><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><msub><mi>𝐯</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v} = \beta_1\mathbf{v}_1 + \beta_2\mathbf{v}_2 + \ldots + \beta_n\mathbf{v}_n</annotation></semantics></math> for some scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>β</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\beta_1, \beta_2, \ldots, \beta_n</annotation></semantics></math>.</li>
</ol>
<p>The <strong>dimension</strong> of a vector space is the number of vectors in any basis for the space. If a vector space has a finite basis, it is called finite-dimensional.</p>
</div>
<p>In <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math>, the standard basis consists of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> unit vectors, each with a 1 in one position and 0s elsewhere.</p>
<div id="exm-standard-basis" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.2 (Standard Basis in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math>)</strong></span> The standard basis for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math> consists of: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_1 = (1, 0, 0)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_2 = (0, 1, 0)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mn>3</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_3 = (0, 0, 1)</annotation></semantics></math></p>
<p>Any vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">(x, y, z) \in \mathbb{R}^3</annotation></semantics></math> can be written as: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><msub><mi>𝐞</mi><mn>1</mn></msub><mo>+</mo><mi>y</mi><msub><mi>𝐞</mi><mn>2</mn></msub><mo>+</mo><mi>z</mi><msub><mi>𝐞</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">(x, y, z) = x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3</annotation></semantics></math></p>
<p>The dimension of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math> is 3.</p>
</div>
</section>
<section id="inner-products-and-norms" class="level3" data-number="15.2.3">
<h3 data-number="15.2.3" class="anchored" data-anchor-id="inner-products-and-norms"><span class="header-section-number">15.2.3</span> Inner Products and Norms</h3>
<p>Inner products and norms are essential for measuring similarities and distances in embedding spaces.</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.3 (Inner Product)</strong></span> An inner product on a vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℝ</mi><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>⋅</mi><mo>,</mo><mi>⋅</mi><mo stretchy="false" form="postfix">⟩</mo><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\langle \cdot, \cdot \rangle: V \times V \rightarrow \mathbb{R}</annotation></semantics></math> that satisfies the following for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{u}, \mathbf{v}, \mathbf{w} \in V</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{R}</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>Linearity in the first argument</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>α</mi><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>α</mi><mo stretchy="false" form="prefix">⟨</mo><mi>𝐮</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="false" form="postfix">⟩</mo><mo>+</mo><mo stretchy="false" form="prefix">⟨</mo><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle \alpha\mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \alpha\langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle</annotation></semantics></math></li>
<li><strong>Symmetry</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>𝐯</mi><mo>,</mo><mi>𝐮</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle</annotation></semantics></math></li>
<li><strong>Positive definiteness</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐯</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\langle \mathbf{v}, \mathbf{v} \rangle \geq 0</annotation></semantics></math>, with equality if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{v} = \mathbf{0}</annotation></semantics></math></li>
</ol>
<p>The standard inner product (dot product) in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>𝐮</mi><mo>⋅</mo><mi>𝐯</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^d u_i v_i</annotation></semantics></math></p>
</div>
<p>The inner product induces a norm, which measures the length of a vector.</p>
<div id="def-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.4 (Norm)</strong></span> A norm on a vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>⋅</mi><mo stretchy="false" form="postfix">∥</mo><mo>:</mo><mi>V</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\|\cdot\|: V \rightarrow \mathbb{R}</annotation></semantics></math> that satisfies the following for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{u}, \mathbf{v} \in V</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{R}</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\| \geq 0</annotation></semantics></math>, with equality if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\mathbf{v} = \mathbf{0}</annotation></semantics></math></li>
<li><strong>Homogeneity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>α</mi><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>α</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\alpha\mathbf{v}\| = |\alpha|\|\mathbf{v}\|</annotation></semantics></math></li>
<li><strong>Triangle inequality</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo><mo>≤</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><mo stretchy="false" form="postfix">∥</mo><mi>+</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|</annotation></semantics></math></li>
</ol>
<p>Given an inner product, the induced norm is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><msqrt><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐯</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}</annotation></semantics></math></p>
<p>Common norms include:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> norm (Euclidean norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^d v_i^2}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_1</annotation></semantics></math> norm (Manhattan norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\|_1 = \sum_{i=1}^d |v_i|</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>p</mi></msub><annotation encoding="application/x-tex">L_p</annotation></semantics></math> norm: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>p</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>p</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>1</mn><mi>/</mi><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\|_p = \left(\sum_{i=1}^d |v_i|^p\right)^{1/p}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \geq 1</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>∞</mi></msub><annotation encoding="application/x-tex">L_\infty</annotation></semantics></math> norm (Maximum norm): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐯</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>∞</mi></msub><mo>=</mo><msub><mo>max</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">\|\mathbf{v}\|_\infty = \max_i |v_i|</annotation></semantics></math></li>
</ul>
</div>
<p>Norms are used to define distances between vectors, which are crucial in many knowledge graph embedding models.</p>
<div id="def-distance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.5 (Distance)</strong></span> Given a norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>⋅</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">\|\cdot\|</annotation></semantics></math>, the induced distance function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>:</mo><mi>V</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">d: V \times V \rightarrow \mathbb{R}</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><mo>−</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|</annotation></semantics></math></p>
<p>This defines a metric space, satisfying:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d(\mathbf{u}, \mathbf{v}) \geq 0</annotation></semantics></math>, with equality if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>=</mo><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">\mathbf{u} = \mathbf{v}</annotation></semantics></math></li>
<li><strong>Symmetry</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo>,</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})</annotation></semantics></math></li>
<li><strong>Triangle inequality</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})</annotation></semantics></math></li>
</ol>
</div>
<div id="exm-inner-product-norm" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.3 (Inner Product, Norm, and Distance Example)</strong></span> Consider vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{u} = (1, 2, 3)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v} = (4, 5, 6)</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^3</annotation></semantics></math>.</p>
<p>Inner product: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mn>1</mn><mo>⋅</mo><mn>4</mn><mo>+</mo><mn>2</mn><mo>⋅</mo><mn>5</mn><mo>+</mo><mn>3</mn><mo>⋅</mo><mn>6</mn><mo>=</mo><mn>4</mn><mo>+</mo><mn>10</mn><mo>+</mo><mn>18</mn><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">\langle \mathbf{u}, \mathbf{v} \rangle = 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 4 + 10 + 18 = 32</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> norm of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msup><mn>1</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>3</mn><mn>2</mn></msup></mrow></msqrt><mo>=</mo><msqrt><mn>14</mn></msqrt><mo>≈</mo><mn>3.74</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{u}\|_2 = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} \approx 3.74</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_1</annotation></semantics></math> norm of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>1</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>2</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>3</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{u}\|_1 = |1| + |2| + |3| = 6</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_2</annotation></semantics></math> distance between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐮</mi><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐯</mi><annotation encoding="application/x-tex">\mathbf{v}</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐮</mi><mo>−</mo><mi>𝐯</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>4</mn><mo>,</mo><mn>2</mn><mo>−</mo><mn>5</mn><mo>,</mo><mn>3</mn><mo>−</mo><mn>6</mn><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>3</mn><mo>,</mo><mi>−</mi><mn>3</mn><mo>,</mo><mi>−</mi><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><mn>9</mn><mo>+</mo><mn>9</mn><mo>+</mo><mn>9</mn></mrow></msqrt><mo>=</mo><msqrt><mn>27</mn></msqrt><mo>≈</mo><mn>5.2</mn></mrow><annotation encoding="application/x-tex">d_2(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|_2 = \|(1-4, 2-5, 3-6)\|_2 = \|(-3, -3, -3)\|_2 = \sqrt{9 + 9 + 9} = \sqrt{27} \approx 5.2</annotation></semantics></math></p>
</div>
</section>
<section id="matrices-and-linear-transformations" class="level3" data-number="15.2.4">
<h3 data-number="15.2.4" class="anchored" data-anchor-id="matrices-and-linear-transformations"><span class="header-section-number">15.2.4</span> Matrices and Linear Transformations</h3>
<p>Matrices are fundamental for representing linear transformations, which are crucial in many knowledge graph embedding models.</p>
<div id="def-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.6 (Matrix)</strong></span> A matrix is a rectangular array of numbers. An <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> rows and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> columns, with entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">A_{ij}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">1 \leq i \leq m</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">1 \leq j \leq n</annotation></semantics></math>.</p>
<p>Basic matrix operations include:</p>
<ol type="1">
<li><strong>Addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>B</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">(A + B)_{ij} = A_{ij} + B_{ij}</annotation></semantics></math> (requires matrices of the same dimensions)</li>
<li><strong>Scalar multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>α</mi><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">(\alpha A)_{ij} = \alpha A_{ij}</annotation></semantics></math></li>
<li><strong>Matrix multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>A</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><msub><mi>B</mi><mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">(AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}</annotation></semantics></math> (requires the number of columns in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> to equal the number of rows in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>)</li>
<li><strong>Transpose</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>T</mi></msubsup><mo>=</mo><msub><mi>A</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A^T_{ij} = A_{ji}</annotation></semantics></math></li>
</ol>
</div>
<div id="def-linear-transformation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.7 (Linear Transformation)</strong></span> A linear transformation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>:</mo><mi>V</mi><mo>→</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">T: V \rightarrow W</annotation></semantics></math> between vector spaces is a function that preserves vector addition and scalar multiplication:</p>
<ol type="1">
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo>+</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐮</mi><mo>,</mo><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{u}, \mathbf{v} \in V</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>α</mi><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T(\alpha\mathbf{v}) = \alpha T(\mathbf{v})</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>𝔽</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{F}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\mathbf{v} \in V</annotation></semantics></math></li>
</ol>
<p>Every linear transformation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>→</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">T: \mathbb{R}^n \rightarrow \mathbb{R}^m</annotation></semantics></math> can be represented by an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> such that: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">T(\mathbf{v}) = A\mathbf{v}</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐯</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{v} \in \mathbb{R}^n</annotation></semantics></math></p>
</div>
<p>In knowledge graph embeddings, matrices often represent relations or relation-specific transformations.</p>
<div id="exm-matrix-transformation" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.4 (Matrix Transformation Example)</strong></span> Consider a relation “capitalOf” represented by the matrix: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.9</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R = \begin{bmatrix} 0.8 &amp; 0.1 \\ 0.2 &amp; 0.9 \end{bmatrix}</annotation></semantics></math></p>
<p>If the entity “Paris” is represented by the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mrow><mi>P</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_{Paris} = (0.6, 0.4)</annotation></semantics></math>, then the transformation by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> gives: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><msub><mi>𝐞</mi><mrow><mi>P</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.9</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.4</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.8</mn><mo>⋅</mo><mn>0.6</mn><mo>+</mo><mn>0.1</mn><mo>⋅</mo><mn>0.4</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.2</mn><mo>⋅</mo><mn>0.6</mn><mo>+</mo><mn>0.9</mn><mo>⋅</mo><mn>0.4</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.52</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.48</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R\mathbf{e}_{Paris} = \begin{bmatrix} 0.8 &amp; 0.1 \\ 0.2 &amp; 0.9 \end{bmatrix} \begin{bmatrix} 0.6 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.8 \cdot 0.6 + 0.1 \cdot 0.4 \\ 0.2 \cdot 0.6 + 0.9 \cdot 0.4 \end{bmatrix} = \begin{bmatrix} 0.52 \\ 0.48 \end{bmatrix}</annotation></semantics></math></p>
<p>In a model like RESCAL, this transformed vector would be compared with the embedding of “France” to determine the plausibility of the triple (“Paris”, “capitalOf”, “France”).</p>
</div>
</section>
<section id="matrix-properties-and-decompositions" class="level3" data-number="15.2.5">
<h3 data-number="15.2.5" class="anchored" data-anchor-id="matrix-properties-and-decompositions"><span class="header-section-number">15.2.5</span> Matrix Properties and Decompositions</h3>
<p>Various matrix properties and decompositions play important roles in knowledge graph embedding models.</p>
<div id="def-matrix-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.8 (Matrix Properties)</strong></span> Key matrix properties include:</p>
<ol type="1">
<li><strong>Symmetry</strong>: A matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is symmetric if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = A^T</annotation></semantics></math></li>
<li><strong>Positive definiteness</strong>: A symmetric matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is positive definite if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>𝐱</mi><mi>T</mi></msup><mi>A</mi><mi>𝐱</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbf{x}^T A \mathbf{x} &gt; 0</annotation></semantics></math> for all non-zero vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></li>
<li><strong>Orthogonality</strong>: A matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is orthogonal if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>Q</mi><mo>=</mo><mi>Q</mi><msup><mi>Q</mi><mi>T</mi></msup><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">Q^T Q = Q Q^T = I</annotation></semantics></math> (its columns and rows form orthonormal bases)</li>
<li><strong>Rank</strong>: The rank of a matrix is the dimension of its column space (or row space)</li>
<li><strong>Determinant</strong>: A scalar value that can be calculated from a square matrix, representing the scaling factor of the transformation</li>
<li><strong>Eigenvalues and eigenvectors</strong>: For a square matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, a non-zero vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐯</mi><annotation encoding="application/x-tex">\mathbf{v}</annotation></semantics></math> is an eigenvector with eigenvalue <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>𝐯</mi><mo>=</mo><mi>λ</mi><mi>𝐯</mi></mrow><annotation encoding="application/x-tex">A\mathbf{v} = \lambda\mathbf{v}</annotation></semantics></math></li>
</ol>
</div>
<p>Matrix decompositions are ways of factoring a matrix into a product of simpler matrices, which can be useful for dimensionality reduction, noise reduction, and efficient computation.</p>
<div id="def-matrix-decompositions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.9 (Matrix Decompositions)</strong></span> Important matrix decompositions include:</p>
<ol type="1">
<li><strong>Eigendecomposition</strong>: For a diagonalizable matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>P</mi><mi>D</mi><msup><mi>P</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">A = P D P^{-1}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> is diagonal and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> has eigenvectors as columns</li>
<li><strong>Singular Value Decomposition (SVD)</strong>: Any matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> can be written as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = U \Sigma V^T</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> are orthogonal and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> is diagonal with non-negative entries</li>
<li><strong>QR Decomposition</strong>: Any matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> can be written as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">A = QR</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is orthogonal and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> is upper triangular</li>
<li><strong>LU Decomposition</strong>: Under certain conditions, a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> can be written as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>L</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">A = LU</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is lower triangular and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is upper triangular</li>
</ol>
</div>
<p>Matrix decompositions are used in some knowledge graph embedding models, particularly those based on tensor factorization.</p>
<div id="exm-svd" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.5 (Singular Value Decomposition Example)</strong></span> Consider a matrix representing entity-relation interactions: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>2</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">A = \begin{bmatrix} 3 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; -2 \end{bmatrix}</annotation></semantics></math></p>
<p>The SVD of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = U \Sigma V^T</annotation></semantics></math> where: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.71</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.71</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.71</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>0.71</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">U = \begin{bmatrix} 0.71 &amp; 0.71 \\ 0.71 &amp; -0.71 \end{bmatrix}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\Sigma = \begin{bmatrix} 5 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \end{bmatrix}</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>0.58</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.58</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>0.82</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">V = \begin{bmatrix} 0.58 &amp; 0.58 &amp; 0.58 \\ 0.58 &amp; 0.58 &amp; -0.58 \\ 0.58 &amp; -0.82 &amp; 0 \end{bmatrix}</annotation></semantics></math></p>
<p>This decomposition can be used to create low-dimensional embeddings by keeping only the largest singular values and their corresponding vectors.</p>
</div>
</section>
</section>
<section id="tensor-algebra" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="tensor-algebra"><span class="header-section-number">15.3</span> Tensor Algebra</h2>
<p>Tensors extend the concepts of vectors and matrices to higher dimensions and provide a natural way to represent multi-relational data.</p>
<section id="tensor-definitions-and-operations" class="level3" data-number="15.3.1">
<h3 data-number="15.3.1" class="anchored" data-anchor-id="tensor-definitions-and-operations"><span class="header-section-number">15.3.1</span> Tensor Definitions and Operations</h3>
<p>A tensor is a multi-dimensional array that generalizes vectors (1D tensors) and matrices (2D tensors).</p>
<div id="def-tensor" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.10 (Tensor)</strong></span> A tensor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒯</mi><annotation encoding="application/x-tex">\mathcal{T}</annotation></semantics></math> of order <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> (or rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>) is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-dimensional array with elements <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><annotation encoding="application/x-tex">\mathcal{T}_{i_1 i_2 \ldots i_n}</annotation></semantics></math> where each index <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>i</mi><mi>j</mi></msub><annotation encoding="application/x-tex">i_j</annotation></semantics></math> ranges from 1 to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding="application/x-tex">d_j</annotation></semantics></math>.</p>
<p>The shape of the tensor is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(d_1, d_2, \ldots, d_n)</annotation></semantics></math>, and the total number of elements is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>×</mo><msub><mi>d</mi><mn>2</mn></msub><mo>×</mo><mi>…</mi><mo>×</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">d_1 \times d_2 \times \ldots \times d_n</annotation></semantics></math>.</p>
<p>Basic tensor operations include:</p>
<ol type="1">
<li><strong>Addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒯</mi><mo>+</mo><mi>𝒰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><mo>=</mo><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><mo>+</mo><msub><mi>𝒰</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">(\mathcal{T} + \mathcal{U})_{i_1 i_2 \ldots i_n} = \mathcal{T}_{i_1 i_2 \ldots i_n} + \mathcal{U}_{i_1 i_2 \ldots i_n}</annotation></semantics></math></li>
<li><strong>Scalar multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mi>𝒯</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><mo>=</mo><mi>α</mi><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">(\alpha\mathcal{T})_{i_1 i_2 \ldots i_n} = \alpha\mathcal{T}_{i_1 i_2 \ldots i_n}</annotation></semantics></math></li>
<li><strong>Tensor product</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒯</mi><mo>⊗</mo><mi>𝒰</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><msub><mi>i</mi><mn>1</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub><msub><mi>j</mi><mn>1</mn></msub><mi>…</mi><msub><mi>j</mi><mi>m</mi></msub></mrow></msub><mo>=</mo><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mi>…</mi><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><msub><mi>𝒰</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><mi>…</mi><msub><mi>j</mi><mi>m</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">(\mathcal{T} \otimes \mathcal{U})_{i_1 \ldots i_n j_1 \ldots j_m} = \mathcal{T}_{i_1 \ldots i_n} \mathcal{U}_{j_1 \ldots j_m}</annotation></semantics></math></li>
<li><strong>Contraction</strong>: Summing over pairs of indices, e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒞</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mi>…</mi><msub><mi>i</mi><mrow><mi>n</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mi>…</mi><msub><mi>i</mi><mrow><mi>n</mi><mo>−</mo><mn>2</mn></mrow></msub><mi>j</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathcal{C}_{i_1 \ldots i_{n-2}} = \sum_{j=1}^d \mathcal{T}_{i_1 \ldots i_{n-2} j j}</annotation></semantics></math></li>
</ol>
</div>
<p>Knowledge graphs can be naturally represented as 3rd-order tensors, with two modes for entities (head and tail) and one mode for relations.</p>
<div id="exm-knowledge-graph-tensor" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.6 (Knowledge Graph as a Tensor)</strong></span> A knowledge graph with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>e</mi></msub><annotation encoding="application/x-tex">n_e</annotation></semantics></math> entities and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>r</mi></msub><annotation encoding="application/x-tex">n_r</annotation></semantics></math> relations can be represented as a 3rd-order tensor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒳</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">}</mo><mrow><msub><mi>n</mi><mi>e</mi></msub><mo>×</mo><msub><mi>n</mi><mi>r</mi></msub><mo>×</mo><msub><mi>n</mi><mi>e</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{X} \in \{0, 1\}^{n_e \times n_r \times n_e}</annotation></semantics></math> where: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{hrt} = 1</annotation></semantics></math> if the triple (entity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>, relation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, entity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>) exists in the knowledge graph <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{hrt} = 0</annotation></semantics></math> otherwise</p>
<p>For example, if we have 3 entities (Alice, Bob, Charlie) and 2 relations (likes, knows), the tensor might be: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{1,1,2} = 1</annotation></semantics></math> (Alice likes Bob) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{1,2,3} = 1</annotation></semantics></math> (Alice knows Charlie) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathcal{X}_{2,2,1} = 1</annotation></semantics></math> (Bob knows Alice) All other entries are 0.</p>
</div>
</section>
<section id="tensor-decompositions" class="level3" data-number="15.3.2">
<h3 data-number="15.3.2" class="anchored" data-anchor-id="tensor-decompositions"><span class="header-section-number">15.3.2</span> Tensor Decompositions</h3>
<p>Tensor decompositions extend matrix factorization techniques to higher-order tensors and are fundamental to several knowledge graph embedding approaches.</p>
<div id="def-tensor-decompositions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.11 (Tensor Decompositions)</strong></span> Major tensor decomposition methods include:</p>
<ol type="1">
<li><p><strong>CP Decomposition (CANDECOMP/PARAFAC)</strong>: Approximates a tensor as a sum of rank-one tensors: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒯</mi><mo>≈</mo><msubsup><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></msubsup><msub><mi>𝐚</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>𝐛</mi><mi>r</mi></msub><mo>⊗</mo><msub><mi>𝐜</mi><mi>r</mi></msub><mo>⊗</mo><mi>…</mi></mrow><annotation encoding="application/x-tex">\mathcal{T} \approx \sum_{r=1}^R \mathbf{a}_r \otimes \mathbf{b}_r \otimes \mathbf{c}_r \otimes \ldots</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⊗</mi><annotation encoding="application/x-tex">\otimes</annotation></semantics></math> denotes the outer product.</p></li>
<li><p><strong>Tucker Decomposition</strong>: Decomposes a tensor into a core tensor multiplied by a matrix along each mode: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒯</mi><mo>≈</mo><mi>𝒢</mi><msub><mo>×</mo><mn>1</mn></msub><mi>A</mi><msub><mo>×</mo><mn>2</mn></msub><mi>B</mi><msub><mo>×</mo><mn>3</mn></msub><mi>C</mi><mo>×</mo><mi>…</mi></mrow><annotation encoding="application/x-tex">\mathcal{T} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C \times \ldots</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>×</mo><mi>n</mi></msub><annotation encoding="application/x-tex">\times_n</annotation></semantics></math> denotes the n-mode product and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒢</mi><annotation encoding="application/x-tex">\mathcal{G}</annotation></semantics></math> is the core tensor.</p></li>
<li><p><strong>Tensor Train Decomposition</strong>: Represents a tensor as a train of lower-order tensors: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><msub><mi>i</mi><mn>2</mn></msub><mi>…</mi><msub><mi>i</mi><mi>d</mi></msub></mrow></msub><mo>≈</mo><msub><mi>G</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>i</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow><msub><mi>G</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>i</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow><mi>…</mi><msub><mi>G</mi><mi>d</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>i</mi><mi>d</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{i_1 i_2 \ldots i_d} \approx G_1[i_1] G_2[i_2] \ldots G_d[i_d]</annotation></semantics></math> where each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">G_k[i_k]</annotation></semantics></math> is a matrix.</p></li>
</ol>
</div>
<p>These decompositions form the basis of several knowledge graph embedding models, particularly RESCAL, which is based on a form of Tucker decomposition.</p>
<div id="exm-rescal-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.7 (RESCAL as Tensor Decomposition)</strong></span> In RESCAL, the knowledge graph tensor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒳</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mi>e</mi></msub><mo>×</mo><msub><mi>n</mi><mi>r</mi></msub><mo>×</mo><msub><mi>n</mi><mi>e</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{X} \in \mathbb{R}^{n_e \times n_r \times n_e}</annotation></semantics></math> is decomposed as: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒳</mi><mo>≈</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>ℛ</mi><mrow><mo>:</mo><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>⊗</mo><msub><mi>𝐀</mi><mrow><mo>:</mo><mo>,</mo><mi>i</mi></mrow></msub><mo>⊗</mo><msub><mi>𝐀</mi><mrow><mo>:</mo><mo>,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathcal{X} \approx \sum_{i=1}^d \sum_{j=1}^d \mathcal{R}_{:,i,j} \otimes \mathbf{A}_{:,i} \otimes \mathbf{A}_{:,j}</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐀</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mi>e</mi></msub><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{A} \in \mathbb{R}^{n_e \times d}</annotation></semantics></math> contains the entity embeddings and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℛ</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mi>r</mi></msub><mo>×</mo><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{R} \in \mathbb{R}^{n_r \times d \times d}</annotation></semantics></math> contains the relation matrices.</p>
<p>This can also be written in terms of matrix products: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝒳</mi><mi>r</mi></msub><mo>≈</mo><mi>𝐀</mi><msub><mi>𝐑</mi><mi>r</mi></msub><msup><mi>𝐀</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{X}_r \approx \mathbf{A} \mathbf{R}_r \mathbf{A}^T</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝒳</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathcal{X}_r</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>-th slice of the tensor (a matrix) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐑</mi><mi>r</mi></msub><annotation encoding="application/x-tex">\mathbf{R}_r</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>-th slice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics></math> (also a matrix).</p>
</div>
</section>
</section>
<section id="complex-vector-spaces" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="complex-vector-spaces"><span class="header-section-number">15.4</span> Complex Vector Spaces</h2>
<p>Complex vector spaces extend real vector spaces by using complex numbers as the scalar field, providing additional modeling capabilities for knowledge graph embeddings.</p>
<section id="complex-numbers-and-operations" class="level3" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="complex-numbers-and-operations"><span class="header-section-number">15.4.1</span> Complex Numbers and Operations</h3>
<p>Complex numbers form the basis of complex vector spaces and have unique properties that can be leveraged in embedding models.</p>
<div id="def-complex-numbers" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.12 (Complex Numbers)</strong></span> A complex number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>∈</mo><mi>ℂ</mi></mrow><annotation encoding="application/x-tex">z \in \mathbb{C}</annotation></semantics></math> is expressed as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">z = a + bi</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">a, b \in \mathbb{R}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> is the imaginary unit satisfying <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>i</mi><mn>2</mn></msup><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">i^2 = -1</annotation></semantics></math>.</p>
<p>The real part of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Re</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">\text{Re}(z) = a</annotation></semantics></math> and the imaginary part is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Im</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">\text{Im}(z) = b</annotation></semantics></math>.</p>
<p>Basic operations on complex numbers include:</p>
<ol type="1">
<li><strong>Addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo>+</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">(a + bi) + (c + di) = (a + c) + (b + d)i</annotation></semantics></math></li>
<li><strong>Multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mi>c</mi><mo>−</mo><mi>b</mi><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mi>d</mi><mo>+</mo><mi>b</mi><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">(a + bi)(c + di) = (ac - bd) + (ad + bc)i</annotation></semantics></math></li>
<li><strong>Complex conjugate</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>z</mi><mo accent="true">¯</mo></mover><mo>=</mo><mover><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi></mrow><mo accent="true">¯</mo></mover><mo>=</mo><mi>a</mi><mo>−</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">\overline{z} = \overline{a + bi} = a - bi</annotation></semantics></math></li>
<li><strong>Modulus</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>z</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">|z| = |a + bi| = \sqrt{a^2 + b^2}</annotation></semantics></math></li>
<li><strong>Argument</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>arg</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mo>tan</mo><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mi>/</mi><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\arg(z) = \tan^{-1}(b/a)</annotation></semantics></math> (adjusted for the quadrant)</li>
</ol>
</div>
<div id="def-complex-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.13 (Complex Vector Space)</strong></span> A complex vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℂ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{C}^d</annotation></semantics></math> consists of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-tuples of complex numbers. Basic operations include:</p>
<ol type="1">
<li><strong>Vector addition</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐳</mi><mo>+</mo><mi>𝐰</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>z</mi><mi>d</mi></msub><mo>+</mo><msub><mi>w</mi><mi>d</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z} + \mathbf{w} = (z_1 + w_1, z_2 + w_2, \ldots, z_d + w_d)</annotation></semantics></math></li>
<li><strong>Scalar multiplication</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>𝐳</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>α</mi><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><mi>α</mi><msub><mi>z</mi><mi>d</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\mathbf{z} = (\alpha z_1, \alpha z_2, \ldots, \alpha z_d)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℂ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{C}</annotation></semantics></math></li>
</ol>
<p>The Hermitian inner product is defined as: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐳</mi><mo>,</mo><mi>𝐰</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>z</mi><mi>j</mi></msub><mover><msub><mi>w</mi><mi>j</mi></msub><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">\langle \mathbf{z}, \mathbf{w} \rangle = \sum_{j=1}^d z_j \overline{w_j}</annotation></semantics></math></p>
<p>The induced norm is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐳</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><msqrt><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>𝐳</mi><mo>,</mo><mi>𝐳</mi><mo stretchy="false" form="postfix">⟩</mo></mrow></msqrt><mo>=</mo><msqrt><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\mathbf{z}\| = \sqrt{\langle \mathbf{z}, \mathbf{z} \rangle} = \sqrt{\sum_{j=1}^d |z_j|^2}</annotation></semantics></math></p>
</div>
<p>Complex vector spaces enable operations like rotations in the complex plane, which are useful for modeling antisymmetric relations.</p>
<div id="exm-complex-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.8 (Complex Embedding Example)</strong></span> In the ComplEx model, entities and relations are embedded in complex space.</p>
<p>If the entity “Paris” is represented by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mrow><mi>P</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.6</mn><mo>+</mo><mn>0.2</mn><mi>i</mi><mo>,</mo><mn>0.1</mn><mo>−</mo><mn>0.5</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_{Paris} = (0.6 + 0.2i, 0.1 - 0.5i)</annotation></semantics></math> and the relation “capitalOf” is represented by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐫</mi><mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>i</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>O</mi><mi>f</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.8</mn><mo>+</mo><mn>0.1</mn><mi>i</mi><mo>,</mo><mn>0.3</mn><mo>+</mo><mn>0.2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{r}_{capitalOf} = (0.8 + 0.1i, 0.3 + 0.2i)</annotation></semantics></math>, then the Hermitian product between them is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><msub><mi>𝐞</mi><mrow><mi>P</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>𝐫</mi><mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>i</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>O</mi><mi>f</mi></mrow></msub><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.6</mn><mo>+</mo><mn>0.2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.8</mn><mo>−</mo><mn>0.1</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.1</mn><mo>−</mo><mn>0.5</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.3</mn><mo>−</mo><mn>0.2</mn><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\langle \mathbf{e}_{Paris}, \mathbf{r}_{capitalOf} \rangle = (0.6 + 0.2i)(0.8 - 0.1i) + (0.1 - 0.5i)(0.3 - 0.2i)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.6</mn><mo>⋅</mo><mn>0.8</mn><mo>+</mo><mn>0.2</mn><mo>⋅</mo><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.2</mn><mo>⋅</mo><mn>0.8</mn><mo>−</mo><mn>0.6</mn><mo>⋅</mo><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.1</mn><mo>⋅</mo><mn>0.3</mn><mo>+</mo><mn>0.5</mn><mo>⋅</mo><mn>0.2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>0.5</mn><mo>⋅</mo><mn>0.3</mn><mo>−</mo><mn>0.1</mn><mo>⋅</mo><mn>0.2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">= (0.6 \cdot 0.8 + 0.2 \cdot 0.1) + i(0.2 \cdot 0.8 - 0.6 \cdot 0.1) + (0.1 \cdot 0.3 + 0.5 \cdot 0.2) + i(- 0.5 \cdot 0.3 - 0.1 \cdot 0.2)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>0.5</mn><mo>+</mo><mn>0.14</mn><mi>i</mi></mrow><annotation encoding="application/x-tex">= 0.5 + 0.14i</annotation></semantics></math></p>
<p>This complex-valued result captures both symmetric and antisymmetric aspects of the relation.</p>
</div>
</section>
<section id="complex-matrix-operations" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="complex-matrix-operations"><span class="header-section-number">15.4.2</span> Complex Matrix Operations</h3>
<p>Complex matrices extend the concept of matrices to complex numbers and provide additional modeling capabilities.</p>
<div id="def-complex-matrices" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.14 (Complex Matrices)</strong></span> A complex matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℂ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{C}^{m \times n}</annotation></semantics></math> has complex entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>i</mi></mrow><annotation encoding="application/x-tex">A_{ij} = a_{ij} + b_{ij}i</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>b</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">a_{ij}, b_{ij} \in \mathbb{R}</annotation></semantics></math>.</p>
<p>Key operations and properties include:</p>
<ol type="1">
<li><strong>Conjugate transpose</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>H</mi></msup><mo>=</mo><mover><msup><mi>A</mi><mi>T</mi></msup><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">A^H = \overline{A^T}</annotation></semantics></math>, with entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mi>H</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mover><msub><mi>A</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">(A^H)_{ij} = \overline{A_{ji}}</annotation></semantics></math></li>
<li><strong>Hermitian matrix</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is Hermitian if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><msup><mi>A</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">A = A^H</annotation></semantics></math></li>
<li><strong>Unitary matrix</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is unitary if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>H</mi></msup><mi>A</mi><mo>=</mo><mi>A</mi><msup><mi>A</mi><mi>H</mi></msup><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">A^H A = A A^H = I</annotation></semantics></math></li>
<li><strong>Spectral decomposition</strong>: A Hermitian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> can be decomposed as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Q</mi><mi>Λ</mi><msup><mi>Q</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">A = Q \Lambda Q^H</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is unitary and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> is diagonal with real entries</li>
</ol>
</div>
<p>Complex matrices are used in models like ComplEx to represent relations with both symmetric and antisymmetric components.</p>
<div id="exm-complex-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.9 (Complex Matrix Example)</strong></span> In an extension of ComplEx, relations can be represented as complex matrices. For example, the relation “capitalOf” might be represented as: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>i</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>O</mi><mi>f</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0.8</mn><mo>+</mo><mn>0.1</mn><mi>i</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0.2</mn><mo>−</mo><mn>0.3</mn><mi>i</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0.1</mn><mo>+</mo><mn>0.4</mn><mi>i</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0.9</mn><mo>+</mo><mn>0.2</mn><mi>i</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">R_{capitalOf} = \begin{bmatrix} 0.8 + 0.1i &amp; 0.2 - 0.3i \\ 0.1 + 0.4i &amp; 0.9 + 0.2i \end{bmatrix}</annotation></semantics></math></p>
<p>This matrix can transform entity embeddings through complex matrix multiplication, capturing both symmetric and antisymmetric aspects of the relation.</p>
</div>
</section>
</section>
<section id="probability-and-statistics" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="probability-and-statistics"><span class="header-section-number">15.5</span> Probability and Statistics</h2>
<p>Probabilistic knowledge graph embedding models rely on concepts from probability and statistics.</p>
<section id="probability-distributions" class="level3" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="probability-distributions"><span class="header-section-number">15.5.1</span> Probability Distributions</h3>
<p>Probability distributions are fundamental to probabilistic knowledge graph embedding models.</p>
<div id="def-probability-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.15 (Probability Distribution)</strong></span> A probability distribution is a function that assigns probabilities to events in a sample space.</p>
<p>For a discrete random variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, the probability mass function (PMF) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X = x)</annotation></semantics></math> gives the probability of each possible value.</p>
<p>For a continuous random variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, the probability density function (PDF) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_X(x)</annotation></semantics></math> satisfies: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>≤</mo><mi>X</mi><mo>≤</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>b</mi></msubsup><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">P(a \leq X \leq b) = \int_a^b f_X(x) dx</annotation></semantics></math></p>
<p>Key properties of probability distributions include:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">P(X = x) \geq 0</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f_X(x) \geq 0</annotation></semantics></math></li>
<li><strong>Normalization</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>x</mi></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_x P(X = x) = 1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mrow><mi>−</mi><mi>∞</mi></mrow><mi>∞</mi></msubsup><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\int_{-\infty}^{\infty} f_X(x) dx = 1</annotation></semantics></math></li>
</ol>
</div>
<p>Common probability distributions used in knowledge graph embeddings include Gaussian (normal), Bernoulli, and uniform distributions.</p>
<div id="exm-gaussian-distribution" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.10 (Gaussian Distribution)</strong></span> The Gaussian (normal) distribution has PDF: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow></mfrac><msup><mi>e</mi><mrow><mi>−</mi><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> is the mean and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is the standard deviation.</p>
<p>In probabilistic knowledge graph embeddings, entity and relation vectors might be modeled as samples from multivariate Gaussian distributions: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>∼</mo><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛍</mi><mi>e</mi></msub><mo>,</mo><msub><mi>𝚺</mi><mi>e</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e} \sim \mathcal{N}(\boldsymbol{\mu}_e, \boldsymbol{\Sigma}_e)</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐫</mi><mo>∼</mo><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛍</mi><mi>r</mi></msub><mo>,</mo><msub><mi>𝚺</mi><mi>r</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{r} \sim \mathcal{N}(\boldsymbol{\mu}_r, \boldsymbol{\Sigma}_r)</annotation></semantics></math></p>
<p>This allows for representing uncertainty in the embeddings.</p>
</div>
</section>
<section id="bayesian-statistics" class="level3" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="bayesian-statistics"><span class="header-section-number">15.5.2</span> Bayesian Statistics</h3>
<p>Bayesian statistics provides a framework for updating probability distributions based on new evidence, which is useful for learning knowledge graph embeddings.</p>
<div id="def-bayes-theorem" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.16 (Bayes’ Theorem)</strong></span> Bayes’ theorem states: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="prefix">|</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A|B) = \frac{P(B|A) P(A)}{P(B)}</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A|B)</annotation></semantics></math> is the posterior probability of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="prefix">|</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(B|A)</annotation></semantics></math> is the likelihood of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A)</annotation></semantics></math> is the prior probability of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(B)</annotation></semantics></math> is the marginal probability of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math></li>
</ul>
<p>In the context of knowledge graph embeddings:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> could represent the embeddings</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> could represent the observed triples</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A|B)</annotation></semantics></math> would be the posterior distribution of embeddings given the observed triples</li>
</ul>
</div>
<p>Bayesian knowledge graph embedding models learn distributions over embeddings rather than point estimates.</p>
<div id="exm-bayesian-kge" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.11 (Bayesian Knowledge Graph Embedding)</strong></span> In a Bayesian KGE model:</p>
<ol type="1">
<li><p><strong>Prior</strong>: We might place prior distributions on entity and relation embeddings: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>𝟎</mn><mo>,</mo><msubsup><mi>σ</mi><mi>e</mi><mn>2</mn></msubsup><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\mathbf{e}_i) = \mathcal{N}(\mathbf{0}, \sigma_e^2\mathbf{I})</annotation></semantics></math> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐫</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>𝟎</mn><mo>,</mo><msubsup><mi>σ</mi><mi>r</mi><mn>2</mn></msubsup><mi>𝐈</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\mathbf{r}_j) = \mathcal{N}(\mathbf{0}, \sigma_r^2\mathbf{I})</annotation></semantics></math></p></li>
<li><p><strong>Likelihood</strong>: We define a likelihood function for observed triples: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">|</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mi>r</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>,</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P((h, r, t)|\mathbf{E}, \mathbf{R}) = \sigma(f_r(\mathbf{e}_h, \mathbf{e}_t))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is the sigmoid function and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>r</mi></msub><annotation encoding="application/x-tex">f_r</annotation></semantics></math> is a scoring function.</p></li>
<li><p><strong>Posterior</strong>: We compute the posterior distribution of embeddings given observed triples: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="false" form="prefix">|</mo><mi>𝒟</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∝</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒟</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\mathbf{E}, \mathbf{R}|\mathcal{D}) \propto P(\mathcal{D}|\mathbf{E}, \mathbf{R}) P(\mathbf{E}) P(\mathbf{R})</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math> is the set of observed triples.</p></li>
<li><p><strong>Inference</strong>: We use techniques like variational inference or MCMC to approximate the posterior.</p></li>
</ol>
</div>
</section>
<section id="information-theory" class="level3" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="information-theory"><span class="header-section-number">15.5.3</span> Information Theory</h3>
<p>Information theory provides tools for measuring uncertainty and information content, which are relevant to knowledge graph embedding learning and evaluation.</p>
<div id="def-entropy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.17 (Entropy and KL Divergence)</strong></span> Entropy measures the uncertainty or information content of a random variable: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><msub><mo>∑</mo><mi>x</mi></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(X) = -\sum_x P(X = x) \log P(X = x)</annotation></semantics></math> (discrete case) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo>∫</mo><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><msub><mi>f</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">H(X) = -\int f_X(x) \log f_X(x) dx</annotation></semantics></math> (continuous case)</p>
<p>Kullback-Leibler (KL) divergence measures the difference between two probability distributions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>∑</mo><mi>x</mi></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}</annotation></semantics></math> (discrete case) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><msub><mi>f</mi><mi>P</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mfrac><mrow><msub><mi>f</mi><mi>P</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>f</mi><mi>Q</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">D_{KL}(P||Q) = \int f_P(x) \log \frac{f_P(x)}{f_Q(x)} dx</annotation></semantics></math> (continuous case)</p>
</div>
<p>Information theory concepts can be used in learning algorithms for knowledge graph embeddings, particularly in variational approaches.</p>
<div id="exm-variational-kge" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.12 (Variational Knowledge Graph Embedding)</strong></span> In a variational approach to knowledge graph embedding:</p>
<ol type="1">
<li><p>We approximate the true posterior <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="false" form="prefix">|</mo><mi>𝒟</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\mathbf{E}, \mathbf{R}|\mathcal{D})</annotation></semantics></math> with a simpler distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(\mathbf{E}, \mathbf{R})</annotation></semantics></math>.</p></li>
<li><p>We minimize the KL divergence between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> and the true posterior: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="false" form="prefix">|</mo><mi>𝒟</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">D_{KL}(Q(\mathbf{E}, \mathbf{R})||P(\mathbf{E}, \mathbf{R}|\mathcal{D}))</annotation></semantics></math></p></li>
<li><p>This is equivalent to maximizing the Evidence Lower Bound (ELBO): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>𝔼</mi><mi>Q</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mo>log</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒟</mi><mo stretchy="false" form="prefix">|</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐄</mi><mo>,</mo><mi>𝐑</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(Q) = \mathbb{E}_Q[\log P(\mathcal{D}|\mathbf{E}, \mathbf{R})] - D_{KL}(Q(\mathbf{E}, \mathbf{R})||P(\mathbf{E}, \mathbf{R}))</annotation></semantics></math></p></li>
<li><p>We can use this approach to learn probabilistic embeddings that capture uncertainty.</p></li>
</ol>
</div>
</section>
</section>
<section id="metric-spaces-and-distance-functions" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="metric-spaces-and-distance-functions"><span class="header-section-number">15.6</span> Metric Spaces and Distance Functions</h2>
<p>Metric spaces provide a general framework for measuring distances, which is essential for many knowledge graph embedding models.</p>
<div id="def-metric-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.18 (Metric Space)</strong></span> A metric space is a set <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> equipped with a distance function (metric) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>:</mo><mi>X</mi><mo>×</mo><mi>X</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">d: X \times X \rightarrow \mathbb{R}</annotation></semantics></math> that satisfies the following for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo>∈</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">x, y, z \in X</annotation></semantics></math>:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d(x, y) \geq 0</annotation></semantics></math></li>
<li><strong>Identity of indiscernibles</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d(x, y) = 0</annotation></semantics></math> if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x = y</annotation></semantics></math></li>
<li><strong>Symmetry</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(x, y) = d(y, x)</annotation></semantics></math></li>
<li><strong>Triangle inequality</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(x, z) \leq d(x, y) + d(y, z)</annotation></semantics></math></li>
</ol>
</div>
<p>Different metric spaces and distance functions capture different aspects of similarity between entities and relations.</p>
<div id="def-distance-functions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.19 (Distance Functions)</strong></span> Common distance functions in knowledge graph embeddings include:</p>
<ol type="1">
<li><strong>Euclidean distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">d_2(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{\sum_i (x_i - y_i)^2}</annotation></semantics></math></li>
<li><strong>Manhattan distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">d_1(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_i |x_i - y_i|</annotation></semantics></math></li>
<li><strong>Minkowski distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>p</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msub><mo stretchy="false" form="postfix">∥</mo><mi>p</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>∑</mo><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>p</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>1</mn><mi>/</mi><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">d_p(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_p = \left(\sum_i |x_i - y_i|^p\right)^{1/p}</annotation></semantics></math></li>
<li><strong>Mahalanobis distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>M</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mrow><annotation encoding="application/x-tex">d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T M (\mathbf{x} - \mathbf{y})}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is a positive definite matrix</li>
<li><strong>Cosine distance</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mo>cos</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>𝐱</mi><mo>⋅</mo><mi>𝐲</mi></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">∥</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐲</mi><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">d_{\cos}(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}</annotation></semantics></math></li>
</ol>
</div>
<div id="exm-distance-functions" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.13 (Distance Functions Example)</strong></span> Consider entity embeddings <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_1 = (1, 2, 3)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}_2 = (4, 2, 1)</annotation></semantics></math>.</p>
<p>Euclidean distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐞</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>−</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt><mo>=</mo><msqrt><mrow><mn>9</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>4</mn></mrow></msqrt><mo>=</mo><msqrt><mn>13</mn></msqrt><mo>≈</mo><mn>3.61</mn></mrow><annotation encoding="application/x-tex">d_2(\mathbf{e}_1, \mathbf{e}_2) = \sqrt{(1-4)^2 + (2-2)^2 + (3-1)^2} = \sqrt{9 + 0 + 4} = \sqrt{13} \approx 3.61</annotation></semantics></math></p>
<p>Manhattan distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐞</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>1</mn><mo>−</mo><mn>4</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>2</mn><mo>−</mo><mn>2</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mn>3</mn><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><mn>3</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>2</mn><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d_1(\mathbf{e}_1, \mathbf{e}_2) = |1-4| + |2-2| + |3-1| = 3 + 0 + 2 = 5</annotation></semantics></math></p>
<p>Cosine distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mo>cos</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐞</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mn>1</mn><mo>⋅</mo><mn>4</mn><mo>+</mo><mn>2</mn><mo>⋅</mo><mn>2</mn><mo>+</mo><mn>3</mn><mo>⋅</mo><mn>1</mn></mrow><mrow><msqrt><mrow><msup><mn>1</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>3</mn><mn>2</mn></msup></mrow></msqrt><mo>⋅</mo><msqrt><mrow><msup><mn>4</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>1</mn><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mn>11</mn><mrow><msqrt><mn>14</mn></msqrt><mo>⋅</mo><msqrt><mn>21</mn></msqrt></mrow></mfrac><mo>≈</mo><mn>0.28</mn></mrow><annotation encoding="application/x-tex">d_{\cos}(\mathbf{e}_1, \mathbf{e}_2) = 1 - \frac{1 \cdot 4 + 2 \cdot 2 + 3 \cdot 1}{\sqrt{1^2 + 2^2 + 3^2} \cdot \sqrt{4^2 + 2^2 + 1^2}} = 1 - \frac{11}{\sqrt{14} \cdot \sqrt{21}} \approx 0.28</annotation></semantics></math></p>
</div>
<section id="non-euclidean-geometries" class="level3" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="non-euclidean-geometries"><span class="header-section-number">15.6.1</span> Non-Euclidean Geometries</h3>
<p>Non-Euclidean geometries, including hyperbolic and spherical spaces, provide alternative embedding spaces with unique properties.</p>
<div id="def-hyperbolic-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.20 (Hyperbolic Space)</strong></span> Hyperbolic space is a non-Euclidean space with constant negative curvature. The Poincaré ball model represents hyperbolic space as the interior of a unit ball.</p>
<p>In the Poincaré ball model, the distance between points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>H</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mo>cosh</mo><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mn>2</mn><mfrac><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo>−</mo><mi>𝐲</mi><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝐲</mi><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d_H(\mathbf{x}, \mathbf{y}) = \cosh^{-1} \left( 1 + 2 \frac{\|\mathbf{x} - \mathbf{y}\|^2}{(1 - \|\mathbf{x}\|^2)(1 - \|\mathbf{y}\|^2)} \right)</annotation></semantics></math></p>
<p>Hyperbolic space has the property that the volume of a ball grows exponentially with its radius, making it well-suited for embedding hierarchical structures.</p>
</div>
<div id="def-spherical-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.21 (Spherical Space)</strong></span> Spherical space is a non-Euclidean space with constant positive curvature. It can be represented as the surface of a unit sphere.</p>
<p>The distance between points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> on the unit sphere is the great-circle distance: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mo>cos</mo><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo>⋅</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d_S(\mathbf{x}, \mathbf{y}) = \cos^{-1}(\mathbf{x} \cdot \mathbf{y})</annotation></semantics></math> (assuming <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐲</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\mathbf{x}\| = \|\mathbf{y}\| = 1</annotation></semantics></math>)</p>
<p>Spherical space is useful for embedding data with bounded, symmetric relationships.</p>
</div>
<div id="exm-non-euclidean" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.14 (Non-Euclidean Embedding Example)</strong></span> In a hyperbolic knowledge graph embedding model:</p>
<ol type="1">
<li><p>Entities are embedded as points in the Poincaré ball: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>∈</mo><msup><mi>𝔹</mi><mi>d</mi></msup><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo>:</mo><mo stretchy="false" form="postfix">∥</mo><mi>𝐱</mi><mo stretchy="false" form="postfix">∥</mo><mo>&lt;</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{e} \in \mathbb{B}^d = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\| &lt; 1\}</annotation></semantics></math></p></li>
<li><p>Hierarchical relationships can be modeled efficiently, with abstract entities (e.g., “Animal”) closer to the origin and specific entities (e.g., “Bengal Tiger”) closer to the boundary.</p></li>
<li><p>The distance function captures the hierarchical similarity, with points at similar depths having smaller distances than points at different depths.</p></li>
</ol>
</div>
</section>
</section>
<section id="graph-theory-basics" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="graph-theory-basics"><span class="header-section-number">15.7</span> Graph Theory Basics</h2>
<p>Knowledge graphs are fundamentally graph structures, so understanding basic graph theory is essential.</p>
<div id="def-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.22 (Graph)</strong></span> A graph <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G = (V, E)</annotation></semantics></math> consists of a set of vertices (or nodes) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> and a set of edges <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>⊆</mo><mi>V</mi><mo>×</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">E \subseteq V \times V</annotation></semantics></math> connecting pairs of vertices.</p>
<p>In a directed graph, edges have a direction, so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">(u, v) \in E</annotation></semantics></math> does not imply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mo>,</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">(v, u) \in E</annotation></semantics></math>.</p>
<p>In a labeled graph, edges are associated with labels from a set <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>, forming triples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(u, l, v)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>,</mo><mi>v</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">u, v \in V</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>∈</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">l \in L</annotation></semantics></math>.</p>
<p>A knowledge graph is a directed, labeled graph where vertices represent entities and edge labels represent relation types.</p>
</div>
<div id="def-graph-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.23 (Graph Properties)</strong></span> Important graph properties include:</p>
<ol type="1">
<li><p><strong>Degree</strong>: The number of edges incident to a vertex. In a directed graph, we distinguish between in-degree and out-degree.</p></li>
<li><p><strong>Path</strong>: A sequence of vertices where each consecutive pair is connected by an edge. The length of a path is the number of edges.</p></li>
<li><p><strong>Connected component</strong>: A maximal subgraph in which any two vertices are connected by a path.</p></li>
<li><p><strong>Cycle</strong>: A path that starts and ends at the same vertex and contains at least one edge.</p></li>
<li><p><strong>Tree</strong>: A connected graph with no cycles.</p></li>
</ol>
</div>
<p>These graph properties can inform the design and analysis of knowledge graph embedding models.</p>
<div id="exm-graph-properties" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.15 (Graph Properties Example)</strong></span> Consider a small knowledge graph with entities E = {Alice, Bob, Charlie, Dave} and relations R = {friendOf, likes}:</p>
<ul>
<li>(Alice, friendOf, Bob)</li>
<li>(Bob, friendOf, Alice)</li>
<li>(Alice, likes, Charlie)</li>
<li>(Bob, likes, Charlie)</li>
<li>(Charlie, friendOf, Dave)</li>
</ul>
<p>Graph properties:</p>
<ul>
<li>Alice has out-degree 2 and in-degree 1</li>
<li>The path (Alice, Bob, Charlie, Dave) connects Alice to Dave</li>
<li>The graph forms a single connected component</li>
<li>(Alice, Bob, Alice) forms a cycle</li>
<li>The subgraph with edges {(Alice, likes, Charlie), (Bob, likes, Charlie), (Charlie, friendOf, Dave)} forms a tree</li>
</ul>
</div>
</section>
<section id="optimization-theory" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="optimization-theory"><span class="header-section-number">15.8</span> Optimization Theory</h2>
<p>Optimization theory provides the mathematical foundation for learning knowledge graph embeddings from data.</p>
<section id="objective-functions" class="level3" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="objective-functions"><span class="header-section-number">15.8.1</span> Objective Functions</h3>
<p>Objective functions quantify how well the embeddings model the knowledge graph.</p>
<div id="def-objective-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.24 (Objective Function)</strong></span> An objective function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\mathbf{\theta})</annotation></semantics></math> maps a set of parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\mathbf{\theta}</annotation></semantics></math> to a scalar value that we aim to minimize or maximize.</p>
<p>In knowledge graph embeddings, the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\mathbf{\theta}</annotation></semantics></math> include the entity and relation embeddings, and the objective function typically measures how well these embeddings model the observed triples.</p>
<p>Common objective functions include:</p>
<ol type="1">
<li><p><strong>Margin-based ranking loss</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mo>=</mo><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi></mrow></msub><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mi>′</mi></mrow></msub><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>γ</mi><mo>+</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L} = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'} \max(0, \gamma + s(h', r, t') - s(h, r, t))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> is the set of observed triples, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">S'</annotation></semantics></math> is the set of corrupted triples, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> is a scoring function, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is a margin.</p></li>
<li><p><strong>Logistic loss</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mo>=</mo><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mo>∪</mo><mi>S</mi><mi>′</mi></mrow></msub><msub><mi>y</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo>log</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L} = \sum_{(h,r,t) \in S \cup S'} y_{hrt} \log \sigma(s(h, r, t)) + (1 - y_{hrt}) \log(1 - \sigma(s(h, r, t)))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y_{hrt} = 1</annotation></semantics></math> for observed triples and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>h</mi><mi>r</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y_{hrt} = 0</annotation></semantics></math> for corrupted triples, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is the sigmoid function.</p></li>
<li><p><strong>Negative sampling loss</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mo>=</mo><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi></mrow></msub><mo>log</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><msub><mi>P</mi><mi>n</mi></msub></mrow></msub><mrow><mo stretchy="true" form="prefix">[</mo><mo>log</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L} = \sum_{(h,r,t) \in S} \log \sigma(s(h, r, t)) + \sum_{i=1}^k \mathbb{E}_{(h',r,t') \sim P_n} [\log \sigma(-s(h', r, t'))]</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>P</mi><mi>n</mi></msub><annotation encoding="application/x-tex">P_n</annotation></semantics></math> is a distribution for sampling negative examples.</p></li>
</ol>
</div>
<div id="exm-objective-function" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.16 (Objective Function Example)</strong></span> For TransE with margin-based ranking loss, the objective function is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℒ</mi><mo>=</mo><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi></mrow></msub><msub><mo>∑</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mi>′</mi></mrow></msub><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>γ</mi><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">∥</mo><mi>+</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mrow><mi>h</mi><mi>′</mi></mrow></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mrow><mi>t</mi><mi>′</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L} = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'} \max(0, \gamma - \|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\| + \|\mathbf{e}_{h'} + \mathbf{r}_r - \mathbf{e}_{t'}\|)</annotation></semantics></math></p>
<p>For positive triple (Alice, friendOf, Bob) and negative triple (Alice, friendOf, Charlie): <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>γ</mi><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mrow><mi>A</mi><mi>l</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>+</mo><msub><mi>𝐫</mi><mrow><mi>f</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>O</mi><mi>f</mi></mrow></msub><mo>−</mo><msub><mi>𝐞</mi><mrow><mi>B</mi><mi>o</mi><mi>b</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mi>+</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mrow><mi>A</mi><mi>l</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>+</mo><msub><mi>𝐫</mi><mrow><mi>f</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>O</mi><mi>f</mi></mrow></msub><mo>−</mo><msub><mi>𝐞</mi><mrow><mi>C</mi><mi>h</mi><mi>a</mi><mi>r</mi><mi>l</mi><mi>i</mi><mi>e</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0, \gamma - \|\mathbf{e}_{Alice} + \mathbf{r}_{friendOf} - \mathbf{e}_{Bob}\| + \|\mathbf{e}_{Alice} + \mathbf{r}_{friendOf} - \mathbf{e}_{Charlie}\|)</annotation></semantics></math></p>
<p>The loss is positive if the negative triple’s score is within <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> of the positive triple’s score, encouraging the model to score positive triples higher than negative ones.</p>
</div>
</section>
<section id="gradient-descent-and-variants" class="level3" data-number="15.8.2">
<h3 data-number="15.8.2" class="anchored" data-anchor-id="gradient-descent-and-variants"><span class="header-section-number">15.8.2</span> Gradient Descent and Variants</h3>
<p>Gradient descent and its variants are the primary optimization algorithms for learning knowledge graph embeddings.</p>
<div id="def-gradient-descent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.25 (Gradient Descent)</strong></span> Gradient descent is an iterative optimization algorithm that updates parameters in the direction of steepest descent of the objective function: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝛉</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>𝛉</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mi>∇</mi><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{\theta}_{t+1} = \mathbf{\theta}_t - \eta \nabla f(\mathbf{\theta}_t)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\eta &gt; 0</annotation></semantics></math> is the learning rate and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f(\mathbf{\theta}_t)</annotation></semantics></math> is the gradient of the objective function with respect to the parameters.</p>
<p>Variants include:</p>
<ol type="1">
<li><strong>Stochastic Gradient Descent (SGD)</strong>: Updates parameters using gradients computed on small batches of data.</li>
<li><strong>Momentum</strong>: Adds a fraction of the previous update to the current update, helping to accelerate learning.</li>
<li><strong>Adagrad</strong>: Adapts the learning rate for each parameter based on historical gradients.</li>
<li><strong>RMSprop</strong>: Normalizes the gradient by a running average of its recent magnitude.</li>
<li><strong>Adam</strong>: Combines ideas from momentum and RMSprop.</li>
</ol>
</div>
<div id="exm-gradient-descent" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.17 (Gradient Descent Example)</strong></span> For TransE with objective function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>, the gradient with respect to the entity embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mi>h</mi></msub><annotation encoding="application/x-tex">\mathbf{e}_h</annotation></semantics></math> for a positive triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math> and negative triple <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mi>′</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(h', r, t')</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><msub><mi>𝐞</mi><mi>h</mi></msub></msub><mi>ℒ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>γ</mi><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="false" form="postfix">∥</mo><mi>+</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mrow><mi>h</mi><mi>′</mi></mrow></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mrow><mi>t</mi><mi>′</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>𝟎</mn></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\mathbf{e}_h} \mathcal{L} = \begin{cases}
\frac{(\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t)}{\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|} &amp; \text{if } \gamma - \|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\| + \|\mathbf{e}_{h'} + \mathbf{r}_r - \mathbf{e}_{t'}\| &gt; 0 \\
\mathbf{0} &amp; \text{otherwise}
\end{cases}</annotation></semantics></math></p>
<p>The update rule using SGD would be: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐞</mi><mi>h</mi></msub><mo>←</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><msub><mi>𝐞</mi><mi>h</mi></msub></msub><mi>ℒ</mi></mrow><annotation encoding="application/x-tex">\mathbf{e}_h \leftarrow \mathbf{e}_h - \eta \nabla_{\mathbf{e}_h} \mathcal{L}</annotation></semantics></math></p>
</div>
</section>
<section id="regularization-techniques" class="level3" data-number="15.8.3">
<h3 data-number="15.8.3" class="anchored" data-anchor-id="regularization-techniques"><span class="header-section-number">15.8.3</span> Regularization Techniques</h3>
<p>Regularization techniques help prevent overfitting and improve generalization in knowledge graph embedding models.</p>
<div id="def-regularization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.26 (Regularization)</strong></span> Regularization adds constraints or penalties to the objective function to prevent overfitting. Common regularization techniques include:</p>
<ol type="1">
<li><p><strong>L2 regularization</strong>: Adds a penalty term proportional to the squared L2 norm of the parameters: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℛ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>λ</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝛉</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{R}(\mathbf{\theta}) = \lambda \|\mathbf{\theta}\|_2^2</annotation></semantics></math></p></li>
<li><p><strong>L1 regularization</strong>: Adds a penalty term proportional to the L1 norm of the parameters: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℛ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>λ</mi><mo stretchy="false" form="postfix">∥</mo><mi>𝛉</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathcal{R}(\mathbf{\theta}) = \lambda \|\mathbf{\theta}\|_1</annotation></semantics></math></p></li>
<li><p><strong>Elastic Net</strong>: Combines L1 and L2 regularization: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℛ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>λ</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">∥</mo><mi>𝛉</mi><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>+</mo><msub><mi>λ</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">∥</mo><mi>𝛉</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{R}(\mathbf{\theta}) = \lambda_1 \|\mathbf{\theta}\|_1 + \lambda_2 \|\mathbf{\theta}\|_2^2</annotation></semantics></math></p></li>
<li><p><strong>Dropout</strong>: Randomly sets a fraction of parameters to zero during training.</p></li>
<li><p><strong>Noise addition</strong>: Adds random noise to parameters during training.</p></li>
<li><p><strong>Norm constraints</strong>: Constrains parameters to have specific norms, e.g., unit norm.</p></li>
</ol>
</div>
<div id="exm-regularization" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.18 (Regularization Example)</strong></span> For TransE with L2 regularization, the regularized objective function is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ℒ</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub><mo>=</mo><mi>ℒ</mi><mo>+</mo><mi>λ</mi><msub><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></msub><mo stretchy="false" form="postfix">∥</mo><mi>𝐫</mi><msubsup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{L}_{reg} = \mathcal{L} + \lambda \sum_{e \in E} \|\mathbf{e}\|_2^2 + \lambda \sum_{r \in R} \|\mathbf{r}\|_2^2</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda &gt; 0</annotation></semantics></math> is the regularization strength.</p>
<p>This encourages the model to learn smaller embedding vectors, reducing the risk of overfitting.</p>
<p>Alternatively, TransE can use norm constraints by projecting entity embeddings onto the unit sphere after each update: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐞</mi><mo>←</mo><mfrac><mi>𝐞</mi><mrow><mo stretchy="false" form="postfix">∥</mo><mi>𝐞</mi><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbf{e} \leftarrow \frac{\mathbf{e}}{\|\mathbf{e}\|}</annotation></semantics></math></p>
</div>
</section>
</section>
<section id="differential-calculus-for-optimization" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="differential-calculus-for-optimization"><span class="header-section-number">15.9</span> Differential Calculus for Optimization</h2>
<p>Differential calculus provides the tools to compute gradients of objective functions, which are essential for optimization.</p>
<div id="def-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.27 (Gradient)</strong></span> The gradient of a scalar function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math> with respect to vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐱</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{x} = (x_1, x_2, \ldots, x_n)</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac><mo>,</mo><mi>…</mi><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f(\mathbf{x}) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right)</annotation></semantics></math></p>
<p>The gradient points in the direction of steepest increase of the function.</p>
</div>
<div id="def-jacobian-hessian" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.28 (Jacobian and Hessian)</strong></span> For a vector-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐟</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>→</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m</annotation></semantics></math>, the Jacobian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> contains all first-order partial derivatives: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">J_{ij} = \frac{\partial f_i}{\partial x_j}</annotation></semantics></math></p>
<p>For a scalar function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math>, the Hessian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> contains all second-order partial derivatives: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>i</mi></msub><mi>∂</mi><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}</annotation></semantics></math></p>
<p>The Hessian characterizes the local curvature of the function.</p>
</div>
<div id="def-chain-rule" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.29 (Chain Rule)</strong></span> The chain rule allows us to compute the derivative of composite functions: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>d</mi><mi>f</mi></mrow><mrow><mi>d</mi><mi>g</mi></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi>d</mi><mi>g</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{d}{dx} f(g(x)) = \frac{df}{dg} \cdot \frac{dg}{dx}</annotation></semantics></math></p>
<p>In higher dimensions, for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐲</mi><mo>=</mo><mi>𝐠</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{y} = \mathbf{g}(\mathbf{x})</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐲</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z = f(\mathbf{y})</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mo>∑</mo><mi>j</mi></msub><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><msub><mi>y</mi><mi>j</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msub><mi>y</mi><mi>j</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}</annotation></semantics></math></p>
<p>The chain rule is fundamental to computing gradients in neural networks and complex knowledge graph embedding models.</p>
</div>
<div id="exm-gradient-computation" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.19 (Gradient Computation Example)</strong></span> For the distance-based scoring function in TransE: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">s(h, r, t) = -\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|_2</annotation></semantics></math></p>
<p>The gradient with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mi>h</mi></msub><annotation encoding="application/x-tex">\mathbf{e}_h</annotation></semantics></math> is: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><msub><mi>𝐞</mi><mi>h</mi></msub></msub><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><msub><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msub></mrow></mfrac><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐞</mi><mi>h</mi></msub><mo>+</mo><msub><mi>𝐫</mi><mi>r</mi></msub><mo>−</mo><msub><mi>𝐞</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\mathbf{e}_h} s(h, r, t) = -\frac{1}{\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|_2} \cdot (\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t)</annotation></semantics></math></p>
<p>This gradient is used in the optimization process to update the entity embedding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐞</mi><mi>h</mi></msub><annotation encoding="application/x-tex">\mathbf{e}_h</annotation></semantics></math>.</p>
</div>
</section>
<section id="conclusion" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">15.10</span> Conclusion</h2>
<p>This appendix has provided a comprehensive overview of the mathematical foundations underlying knowledge graph embeddings. From the basics of linear algebra and vector spaces to more advanced topics like tensor algebra, complex vector spaces, probability theory, and optimization, these mathematical concepts form the building blocks for understanding, implementing, and developing knowledge graph embedding models.</p>
<p>By mastering these mathematical foundations, researchers and practitioners can better understand existing knowledge graph embedding techniques, develop new approaches, and effectively apply these methods to real-world problems. The formal definitions, theorems, and examples provided in this appendix serve as a reference that readers can consult as they work through the main chapters of the book.</p>
<p>The field of knowledge graph embeddings continues to evolve, with new mathematical tools and techniques being incorporated to address the challenges of modeling complex relationships in large-scale knowledge graphs. A solid understanding of these mathematical foundations will provide a strong basis for keeping up with and contributing to these developments.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/frontier.html" class="pagination-link" aria-label="Advanced Topics and Research Frontiers">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/appendix-b.html" class="pagination-link" aria-label="Appendix B: Resources and Tools">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>