<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Evaluation Methodologies and Benchmarks – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/additional-knowledge.html" rel="next">
<link href="../content/training.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/evaluation.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-knowledge-graph-embedding-evaluation" id="toc-introduction-to-knowledge-graph-embedding-evaluation" class="nav-link active" data-scroll-target="#introduction-to-knowledge-graph-embedding-evaluation"><span class="header-section-number">9.1</span> Introduction to Knowledge Graph Embedding Evaluation</a></li>
  <li><a href="#link-prediction-as-an-evaluation-task" id="toc-link-prediction-as-an-evaluation-task" class="nav-link" data-scroll-target="#link-prediction-as-an-evaluation-task"><span class="header-section-number">9.2</span> Link Prediction as an Evaluation Task</a>
  <ul class="collapse">
  <li><a href="#ranking-protocol-for-link-prediction" id="toc-ranking-protocol-for-link-prediction" class="nav-link" data-scroll-target="#ranking-protocol-for-link-prediction"><span class="header-section-number">9.2.1</span> Ranking Protocol for Link Prediction</a></li>
  </ul></li>
  <li><a href="#evaluation-metrics-for-link-prediction" id="toc-evaluation-metrics-for-link-prediction" class="nav-link" data-scroll-target="#evaluation-metrics-for-link-prediction"><span class="header-section-number">9.3</span> Evaluation Metrics for Link Prediction</a>
  <ul class="collapse">
  <li><a href="#mean-rank" id="toc-mean-rank" class="nav-link" data-scroll-target="#mean-rank"><span class="header-section-number">9.3.1</span> Mean Rank</a></li>
  <li><a href="#mean-reciprocal-rank" id="toc-mean-reciprocal-rank" class="nav-link" data-scroll-target="#mean-reciprocal-rank"><span class="header-section-number">9.3.2</span> Mean Reciprocal Rank</a></li>
  <li><a href="#hitsk" id="toc-hitsk" class="nav-link" data-scroll-target="#hitsk"><span class="header-section-number">9.3.3</span> Hits@k</a></li>
  </ul></li>
  <li><a href="#filtered-vs.-raw-evaluation-settings" id="toc-filtered-vs.-raw-evaluation-settings" class="nav-link" data-scroll-target="#filtered-vs.-raw-evaluation-settings"><span class="header-section-number">9.4</span> Filtered vs.&nbsp;Raw Evaluation Settings</a></li>
  <li><a href="#standard-benchmark-datasets" id="toc-standard-benchmark-datasets" class="nav-link" data-scroll-target="#standard-benchmark-datasets"><span class="header-section-number">9.5</span> Standard Benchmark Datasets</a>
  <ul class="collapse">
  <li><a href="#fb15k" id="toc-fb15k" class="nav-link" data-scroll-target="#fb15k"><span class="header-section-number">9.5.1</span> FB15k</a></li>
  <li><a href="#wn18" id="toc-wn18" class="nav-link" data-scroll-target="#wn18"><span class="header-section-number">9.5.2</span> WN18</a></li>
  <li><a href="#fb15k-237" id="toc-fb15k-237" class="nav-link" data-scroll-target="#fb15k-237"><span class="header-section-number">9.5.3</span> FB15k-237</a></li>
  <li><a href="#wn18rr" id="toc-wn18rr" class="nav-link" data-scroll-target="#wn18rr"><span class="header-section-number">9.5.4</span> WN18RR</a></li>
  <li><a href="#yago3-10" id="toc-yago3-10" class="nav-link" data-scroll-target="#yago3-10"><span class="header-section-number">9.5.5</span> YAGO3-10</a></li>
  <li><a href="#newer-benchmark-datasets" id="toc-newer-benchmark-datasets" class="nav-link" data-scroll-target="#newer-benchmark-datasets"><span class="header-section-number">9.5.6</span> Newer Benchmark Datasets</a></li>
  </ul></li>
  <li><a href="#issues-with-standard-benchmarks" id="toc-issues-with-standard-benchmarks" class="nav-link" data-scroll-target="#issues-with-standard-benchmarks"><span class="header-section-number">9.6</span> Issues with Standard Benchmarks</a>
  <ul class="collapse">
  <li><a href="#inverse-relation-bias" id="toc-inverse-relation-bias" class="nav-link" data-scroll-target="#inverse-relation-bias"><span class="header-section-number">9.6.1</span> Inverse Relation Bias</a></li>
  <li><a href="#balanced-vs.-realistic-relations" id="toc-balanced-vs.-realistic-relations" class="nav-link" data-scroll-target="#balanced-vs.-realistic-relations"><span class="header-section-number">9.6.2</span> Balanced vs.&nbsp;Realistic Relations</a></li>
  <li><a href="#domain-specificity" id="toc-domain-specificity" class="nav-link" data-scroll-target="#domain-specificity"><span class="header-section-number">9.6.3</span> Domain Specificity</a></li>
  </ul></li>
  <li><a href="#triple-classification-as-an-evaluation-task" id="toc-triple-classification-as-an-evaluation-task" class="nav-link" data-scroll-target="#triple-classification-as-an-evaluation-task"><span class="header-section-number">9.7</span> Triple Classification as an Evaluation Task</a></li>
  <li><a href="#implementation-of-evaluation-protocols" id="toc-implementation-of-evaluation-protocols" class="nav-link" data-scroll-target="#implementation-of-evaluation-protocols"><span class="header-section-number">9.8</span> Implementation of Evaluation Protocols</a></li>
  <li><a href="#statistical-significance-testing" id="toc-statistical-significance-testing" class="nav-link" data-scroll-target="#statistical-significance-testing"><span class="header-section-number">9.9</span> Statistical Significance Testing</a></li>
  <li><a href="#common-evaluation-mistakes-and-pitfalls" id="toc-common-evaluation-mistakes-and-pitfalls" class="nav-link" data-scroll-target="#common-evaluation-mistakes-and-pitfalls"><span class="header-section-number">9.10</span> Common Evaluation Mistakes and Pitfalls</a>
  <ul class="collapse">
  <li><a href="#reporting-only-favorable-metrics" id="toc-reporting-only-favorable-metrics" class="nav-link" data-scroll-target="#reporting-only-favorable-metrics"><span class="header-section-number">9.10.1</span> Reporting Only Favorable Metrics</a></li>
  <li><a href="#inconsistent-evaluation-protocols" id="toc-inconsistent-evaluation-protocols" class="nav-link" data-scroll-target="#inconsistent-evaluation-protocols"><span class="header-section-number">9.10.2</span> Inconsistent Evaluation Protocols</a></li>
  <li><a href="#ignoring-hyperparameter-sensitivity" id="toc-ignoring-hyperparameter-sensitivity" class="nav-link" data-scroll-target="#ignoring-hyperparameter-sensitivity"><span class="header-section-number">9.10.3</span> Ignoring Hyperparameter Sensitivity</a></li>
  <li><a href="#overtuning-on-test-set" id="toc-overtuning-on-test-set" class="nav-link" data-scroll-target="#overtuning-on-test-set"><span class="header-section-number">9.10.4</span> Overtuning on Test Set</a></li>
  </ul></li>
  <li><a href="#comparison-with-alternative-evaluation-approaches" id="toc-comparison-with-alternative-evaluation-approaches" class="nav-link" data-scroll-target="#comparison-with-alternative-evaluation-approaches"><span class="header-section-number">9.11</span> Comparison with Alternative Evaluation Approaches</a>
  <ul class="collapse">
  <li><a href="#relational-pattern-evaluation" id="toc-relational-pattern-evaluation" class="nav-link" data-scroll-target="#relational-pattern-evaluation"><span class="header-section-number">9.11.1</span> Relational Pattern Evaluation</a></li>
  <li><a href="#inductive-evaluation" id="toc-inductive-evaluation" class="nav-link" data-scroll-target="#inductive-evaluation"><span class="header-section-number">9.11.2</span> Inductive Evaluation</a></li>
  <li><a href="#explainability-assessment" id="toc-explainability-assessment" class="nav-link" data-scroll-target="#explainability-assessment"><span class="header-section-number">9.11.3</span> Explainability Assessment</a></li>
  </ul></li>
  <li><a href="#benchmarking-libraries-and-frameworks" id="toc-benchmarking-libraries-and-frameworks" class="nav-link" data-scroll-target="#benchmarking-libraries-and-frameworks"><span class="header-section-number">9.12</span> Benchmarking Libraries and Frameworks</a></li>
  <li><a href="#toward-better-evaluation-practices" id="toc-toward-better-evaluation-practices" class="nav-link" data-scroll-target="#toward-better-evaluation-practices"><span class="header-section-number">9.13</span> Toward Better Evaluation Practices</a>
  <ul class="collapse">
  <li><a href="#reproducibility-initiatives" id="toc-reproducibility-initiatives" class="nav-link" data-scroll-target="#reproducibility-initiatives"><span class="header-section-number">9.13.1</span> Reproducibility Initiatives</a></li>
  <li><a href="#multi-faceted-evaluation" id="toc-multi-faceted-evaluation" class="nav-link" data-scroll-target="#multi-faceted-evaluation"><span class="header-section-number">9.13.2</span> Multi-Faceted Evaluation</a></li>
  <li><a href="#realistic-evaluation-scenarios" id="toc-realistic-evaluation-scenarios" class="nav-link" data-scroll-target="#realistic-evaluation-scenarios"><span class="header-section-number">9.13.3</span> Realistic Evaluation Scenarios</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9.14</span> Conclusion</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">9.15</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="introduction-to-knowledge-graph-embedding-evaluation" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="introduction-to-knowledge-graph-embedding-evaluation"><span class="header-section-number">9.1</span> Introduction to Knowledge Graph Embedding Evaluation</h2>
<p>Proper evaluation of knowledge graph embedding (KGE) models is crucial for tracking progress in the field, comparing different approaches, and understanding their strengths and limitations. However, evaluating KGE models presents unique challenges, from the selection of appropriate metrics to the design of evaluation protocols and benchmark datasets.</p>
<p>This chapter focuses on the methodologies, metrics, datasets, and best practices for evaluating knowledge graph embedding models. We’ll explore how to correctly assess model performance, how to interpret results from research papers, and how to avoid common pitfalls in evaluation. We’ll also examine the evolution of benchmark datasets and how they’ve shaped the development of KGE models.</p>
<p>Understanding these evaluation methodologies is essential not only for researchers developing new models but also for practitioners selecting appropriate models for specific applications. Let’s begin by examining the primary evaluation tasks used to assess knowledge graph embeddings.</p>
</section>
<section id="link-prediction-as-an-evaluation-task" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="link-prediction-as-an-evaluation-task"><span class="header-section-number">9.2</span> Link Prediction as an Evaluation Task</h2>
<p>Link prediction is the most common task for evaluating knowledge graph embeddings. The task involves predicting missing links in a knowledge graph, which directly aligns with the primary goal of knowledge graph completion.</p>
<div id="def-link-prediction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.1 (Link Prediction Task)</strong></span> Given a knowledge graph <span class="math inline">\(G = (E, R, T)\)</span> where <span class="math inline">\(E\)</span> is the set of entities, <span class="math inline">\(R\)</span> is the set of relations, and <span class="math inline">\(T\)</span> is the set of triples <span class="math inline">\((h, r, t)\)</span>, the link prediction task involves:</p>
<ol type="1">
<li>Removing a subset of triples from the graph to create a test set</li>
<li>Training the embedding model on the remaining triples</li>
<li>For each test triple <span class="math inline">\((h, r, t)\)</span>, evaluating the model’s ability to:
<ul>
<li>Predict the tail entity when given <span class="math inline">\((h, r, ?)\)</span></li>
<li>Predict the head entity when given <span class="math inline">\((?, r, t)\)</span></li>
</ul></li>
</ol>
</div>
<p>Link prediction serves as an effective proxy for measuring how well the embedding model captures the structure and semantics of the knowledge graph. It challenges the model to reconstruct missing information, which is precisely what knowledge graph completion aims to achieve.</p>
<section id="ranking-protocol-for-link-prediction" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="ranking-protocol-for-link-prediction"><span class="header-section-number">9.2.1</span> Ranking Protocol for Link Prediction</h3>
<p>The standard evaluation protocol for link prediction involves a ranking procedure:</p>
<div id="def-ranking-protocol" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.2 (Ranking Protocol)</strong></span> For each test triple <span class="math inline">\((h, r, t)\)</span>:</p>
<ol type="1">
<li><strong>Head Prediction</strong>: Replace the head entity <span class="math inline">\(h\)</span> with every entity <span class="math inline">\(e \in E\)</span> to create corrupted triples <span class="math inline">\((e, r, t)\)</span></li>
<li><strong>Score Computation</strong>: Compute scores for all corrupted triples plus the original triple</li>
<li><strong>Ranking</strong>: Rank all entities based on their scores (in descending or ascending order, depending on whether higher or lower scores are better)</li>
<li><strong>Metric Calculation</strong>: Record the rank of the correct entity</li>
<li><strong>Tail Prediction</strong>: Repeat steps 1-4 for tail prediction by creating corrupted triples <span class="math inline">\((h, r, e)\)</span></li>
<li><strong>Aggregation</strong>: Compute aggregate metrics over all test triples</li>
</ol>
</div>
<p>This procedure tests the model’s ability to distinguish the correct entities from incorrect ones, which is fundamental to the link prediction task.</p>
<div id="exm-ranking-procedure" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.1 (Example of Ranking Procedure)</strong></span> Consider a test triple <span class="math inline">\((Bob, friendOf, Alice)\)</span> in a small knowledge graph with entities <span class="math inline">\(E = \{Alice, Bob, Charlie, David, Eve\}\)</span>.</p>
<p>For tail prediction, the model evaluates scores for:</p>
<ul>
<li><span class="math inline">\((Bob, friendOf, Alice)\)</span> [correct]</li>
<li><span class="math inline">\((Bob, friendOf, Bob)\)</span></li>
<li><span class="math inline">\((Bob, friendOf, Charlie)\)</span></li>
<li><span class="math inline">\((Bob, friendOf, David)\)</span></li>
<li><span class="math inline">\((Bob, friendOf, Eve)\)</span></li>
</ul>
<p>If the scores are <span class="math inline">\([0.8, 0.3, 0.6, 0.2, 0.7]\)</span> and higher scores indicate better triples, the entities would be ranked as: <span class="math inline">\([Alice, Eve, Charlie, Bob, David]\)</span></p>
<p>The correct entity (Alice) has rank 1.</p>
<p>Similarly, for head prediction, the model would evaluate <span class="math inline">\((Alice, friendOf, Alice)\)</span>, <span class="math inline">\((Bob, friendOf, Alice)\)</span>, etc., and rank the entities based on their scores.</p>
</div>
</section>
</section>
<section id="evaluation-metrics-for-link-prediction" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="evaluation-metrics-for-link-prediction"><span class="header-section-number">9.3</span> Evaluation Metrics for Link Prediction</h2>
<p>Several metrics have been developed to evaluate the performance of KGE models on the link prediction task. These metrics capture different aspects of the ranking performance.</p>
<section id="mean-rank" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="mean-rank"><span class="header-section-number">9.3.1</span> Mean Rank</h3>
<p>The simplest metric is the mean rank, which averages the ranks of the correct entities across all test triples.</p>
<div id="def-mean-rank" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.3 (Mean Rank (MR))</strong></span> The mean rank is defined as:</p>
<p><span class="math display">\[MR = \frac{1}{2|T_{test}|} \sum_{(h,r,t) \in T_{test}} (rank_h + rank_t)\]</span></p>
<p>where <span class="math inline">\(T_{test}\)</span> is the set of test triples, <span class="math inline">\(rank_h\)</span> is the rank of the head entity when predicting the head, and <span class="math inline">\(rank_t\)</span> is the rank of the tail entity when predicting the tail.</p>
</div>
<p>Lower mean rank values indicate better performance. However, mean rank is sensitive to outliers (very high ranks) and can be heavily influenced by the size of the entity set.</p>
</section>
<section id="mean-reciprocal-rank" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="mean-reciprocal-rank"><span class="header-section-number">9.3.2</span> Mean Reciprocal Rank</h3>
<p>The mean reciprocal rank (MRR) addresses some of the limitations of mean rank by using the reciprocal of the rank.</p>
<div id="def-mean-reciprocal-rank" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.4 (Mean Reciprocal Rank (MRR))</strong></span> The mean reciprocal rank is defined as:</p>
<p><span class="math display">\[MRR = \frac{1}{2|T_{test}|} \sum_{(h,r,t) \in T_{test}} \left(\frac{1}{rank_h} + \frac{1}{rank_t}\right)\]</span></p>
<p>where <span class="math inline">\(rank_h\)</span> and <span class="math inline">\(rank_t\)</span> are the ranks of the head and tail entities, respectively.</p>
</div>
<p>MRR values range from 0 to 1, with higher values indicating better performance. MRR gives more weight to higher ranks (lower rank values) and is less sensitive to outliers than mean rank.</p>
<div id="exm-mrr-calculation" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.2 (Example of MRR Calculation)</strong></span> Consider three test triples with the following ranks:</p>
<ul>
<li>Triple 1: <span class="math inline">\(rank_h = 3\)</span>, <span class="math inline">\(rank_t = 1\)</span></li>
<li>Triple 2: <span class="math inline">\(rank_h = 10\)</span>, <span class="math inline">\(rank_t = 5\)</span></li>
<li>Triple 3: <span class="math inline">\(rank_h = 2\)</span>, <span class="math inline">\(rank_t = 2\)</span></li>
</ul>
<p>The MRR would be: <span class="math display">\[MRR = \frac{1}{2 \times 3} \left(\frac{1}{3} + \frac{1}{1} + \frac{1}{10} + \frac{1}{5} + \frac{1}{2} + \frac{1}{2}\right) = \frac{1}{6} \times \frac{143}{60} = 0.397\]</span></p>
</div>
</section>
<section id="hitsk" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="hitsk"><span class="header-section-number">9.3.3</span> Hits@k</h3>
<p>Hits@k measures the proportion of test triples where the correct entity is ranked among the top k.</p>
<div id="def-hits-at-k" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.5 (Hits@k)</strong></span> Hits@k is defined as:</p>
<p><span class="math display">\[Hits@k = \frac{1}{2|T_{test}|} \sum_{(h,r,t) \in T_{test}} (I[rank_h \leq k] + I[rank_t \leq k])\]</span></p>
<p>where <span class="math inline">\(I[\cdot]\)</span> is the indicator function that returns 1 if the condition is true and 0 otherwise.</p>
</div>
<p>Common values for k are 1, 3, and 10. Hits@1 measures the accuracy of the model in predicting the exact correct entity, while Hits@10 measures the ability of the model to include the correct entity in a small set of candidates.</p>
<div id="exm-hits-calculation" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.3 (Example of Hits@k Calculation)</strong></span> Using the same three test triples from the MRR example:</p>
<ul>
<li>Triple 1: <span class="math inline">\(rank_h = 3\)</span>, <span class="math inline">\(rank_t = 1\)</span></li>
<li>Triple 2: <span class="math inline">\(rank_h = 10\)</span>, <span class="math inline">\(rank_t = 5\)</span></li>
<li>Triple 3: <span class="math inline">\(rank_h = 2\)</span>, <span class="math inline">\(rank_t = 2\)</span></li>
</ul>
<p>For Hits@1: <span class="math display">\[Hits@1 = \frac{1}{6} (0 + 1 + 0 + 0 + 0 + 0) = \frac{1}{6} = 0.167\]</span></p>
<p>For Hits@3: <span class="math display">\[Hits@3 = \frac{1}{6} (1 + 1 + 0 + 0 + 1 + 1) = \frac{4}{6} = 0.667\]</span></p>
<p>For Hits@10: <span class="math display">\[Hits@10 = \frac{1}{6} (1 + 1 + 1 + 1 + 1 + 1) = 1.0\]</span></p>
</div>
</section>
</section>
<section id="filtered-vs.-raw-evaluation-settings" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="filtered-vs.-raw-evaluation-settings"><span class="header-section-number">9.4</span> Filtered vs.&nbsp;Raw Evaluation Settings</h2>
<p>A critical consideration in link prediction evaluation is the handling of existing triples in the knowledge graph when computing ranks.</p>
<div id="def-raw-setting" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.6 (Raw Evaluation Setting)</strong></span> In the raw setting, all entities are considered when computing ranks, regardless of whether the resulting triples exist in the training, validation, or test set.</p>
</div>
<p>The raw setting can lead to misleading results because a model may rank a valid triple (that exists in the training set) higher than the test triple, which would count as an error even though the prediction is actually correct.</p>
<div id="def-filtered-setting" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.7 (Filtered Evaluation Setting)</strong></span> In the filtered setting, when computing the rank for a test triple <span class="math inline">\((h, r, t)\)</span>:</p>
<ol type="1">
<li>For head prediction, remove all triples <span class="math inline">\((h', r, t)\)</span> where <span class="math inline">\(h' \neq h\)</span> that exist in the training, validation, or test set</li>
<li>For tail prediction, remove all triples <span class="math inline">\((h, r, t')\)</span> where <span class="math inline">\(t' \neq t\)</span> that exist in the training, validation, or test set</li>
<li>Compute ranks considering only the remaining corrupted triples</li>
</ol>
</div>
<p>The filtered setting provides a more accurate assessment of model performance by ensuring that valid triples are not counted as incorrect predictions.</p>
<div id="exm-filtered-vs-raw" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.4 (Example of Filtered vs.&nbsp;Raw Evaluation)</strong></span> Consider a knowledge graph with these triples in the training set:</p>
<ul>
<li><span class="math inline">\((Bob, friendOf, Alice)\)</span></li>
<li><span class="math inline">\((Bob, friendOf, Charlie)\)</span></li>
<li><span class="math inline">\((David, friendOf, Alice)\)</span></li>
</ul>
<p>And the test triple <span class="math inline">\((Eve, friendOf, Alice)\)</span>.</p>
<p>When evaluating tail prediction for <span class="math inline">\((Eve, friendOf, ?)\)</span>:</p>
<p>In the raw setting, if the model scores entities as <span class="math inline">\([Alice &gt; Charlie &gt; Bob &gt; David &gt; Eve]\)</span>, Alice would have rank 1.</p>
<p>In the filtered setting, we would exclude <span class="math inline">\((Bob, friendOf, Alice)\)</span> and <span class="math inline">\((David, friendOf, Alice)\)</span> since they are valid triples. The ranking would then be based on <span class="math inline">\([Alice &gt; Charlie &gt; Eve]\)</span>, and Alice would still have rank 1.</p>
<p>However, if the model ranks entities as <span class="math inline">\([Bob &gt; Alice &gt; David &gt; Charlie &gt; Eve]\)</span>, in the raw setting Alice would have rank 2, but in the filtered setting (excluding Bob and David), Alice would have rank 1.</p>
</div>
<p>The filtered setting is now the standard in the field, as it provides a more accurate assessment of model performance. However, it’s important to recognize that older papers may report only raw metrics.</p>
</section>
<section id="standard-benchmark-datasets" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="standard-benchmark-datasets"><span class="header-section-number">9.5</span> Standard Benchmark Datasets</h2>
<p>Several benchmark datasets have become standard in the field of knowledge graph embeddings. These datasets serve as common ground for comparing different models and approaches.</p>
<section id="fb15k" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="fb15k"><span class="header-section-number">9.5.1</span> FB15k</h3>
<p>FB15k is a subset of Freebase, a large collaborative knowledge base of general facts.</p>
<div id="def-fb15k" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.8 (FB15k Dataset)</strong></span> FB15k contains:</p>
<ul>
<li>14,951 entities</li>
<li>1,345 relation types</li>
<li>483,142 training triples</li>
<li>50,000 validation triples</li>
<li>59,071 test triples</li>
</ul>
</div>
<p>FB15k was one of the first widely used benchmarks for knowledge graph embeddings. However, it was later discovered to have a significant flaw: many test triples can be answered through inverse relations in the training set, which allows models to achieve artificially high performance.</p>
</section>
<section id="wn18" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="wn18"><span class="header-section-number">9.5.2</span> WN18</h3>
<p>WN18 is derived from WordNet, a lexical database for the English language.</p>
<div id="def-wn18" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.9 (WN18 Dataset)</strong></span> WN18 contains:</p>
<ul>
<li>40,943 entities (word senses)</li>
<li>18 relation types</li>
<li>141,442 training triples</li>
<li>5,000 validation triples</li>
<li>5,000 test triples</li>
</ul>
</div>
<p>Like FB15k, WN18 also suffers from the inverse relation issue, where many test triples can be answered by simply finding the inverse of a training triple.</p>
</section>
<section id="fb15k-237" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="fb15k-237"><span class="header-section-number">9.5.3</span> FB15k-237</h3>
<p>FB15k-237 was created to address the inverse relation issue in FB15k by removing redundant relations.</p>
<div id="def-fb15k-237" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.10 (FB15k-237 Dataset)</strong></span> FB15k-237 contains:</p>
<ul>
<li>14,541 entities</li>
<li>237 relation types</li>
<li>272,115 training triples</li>
<li>17,535 validation triples</li>
<li>20,466 test triples</li>
</ul>
</div>
<p>FB15k-237 is considered a more challenging and realistic benchmark than FB15k because it requires models to make more sophisticated inferences rather than simply memorizing inverse relations.</p>
</section>
<section id="wn18rr" class="level3" data-number="9.5.4">
<h3 data-number="9.5.4" class="anchored" data-anchor-id="wn18rr"><span class="header-section-number">9.5.4</span> WN18RR</h3>
<p>WN18RR (WN18 with Reversed Relations removed) addresses the same issue in WN18.</p>
<div id="def-wn18rr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.11 (WN18RR Dataset)</strong></span> WN18RR contains:</p>
<ul>
<li>40,943 entities</li>
<li>11 relation types</li>
<li>86,835 training triples</li>
<li>3,034 validation triples</li>
<li>3,134 test triples</li>
</ul>
</div>
<p>WN18RR is more challenging than WN18 and provides a better assessment of a model’s ability to capture the semantics of the knowledge graph.</p>
<div id="exm-inverse-relation-issue" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.5 (Example of the Inverse Relation Issue)</strong></span> In FB15k, if the training set contains:</p>
<ul>
<li><span class="math inline">\((France, hasCapital, Paris)\)</span></li>
</ul>
<p>And the test set contains:</p>
<ul>
<li><span class="math inline">\((Paris, isCapitalOf, France)\)</span></li>
</ul>
<p>A model that simply learns that <span class="math inline">\(isCapitalOf\)</span> is the inverse of <span class="math inline">\(hasCapital\)</span> can trivially predict the test triple without understanding the semantics of the relations.</p>
<p>FB15k-237 addresses this by keeping only one of these relations in the dataset.</p>
</div>
</section>
<section id="yago3-10" class="level3" data-number="9.5.5">
<h3 data-number="9.5.5" class="anchored" data-anchor-id="yago3-10"><span class="header-section-number">9.5.5</span> YAGO3-10</h3>
<p>YAGO3-10 is derived from YAGO3, a knowledge base that combines information from Wikipedia, WordNet, and GeoNames.</p>
<div id="def-yago3-10" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.12 (YAGO3-10 Dataset)</strong></span> YAGO3-10 contains:</p>
<ul>
<li>123,182 entities</li>
<li>37 relation types</li>
<li>1,079,040 training triples</li>
<li>5,000 validation triples</li>
<li>5,000 test triples</li>
</ul>
</div>
<p>YAGO3-10 is larger than FB15k-237 and WN18RR and provides a more challenging benchmark for evaluating model scalability.</p>
</section>
<section id="newer-benchmark-datasets" class="level3" data-number="9.5.6">
<h3 data-number="9.5.6" class="anchored" data-anchor-id="newer-benchmark-datasets"><span class="header-section-number">9.5.6</span> Newer Benchmark Datasets</h3>
<p>Several newer benchmark datasets have been proposed to address limitations of earlier datasets or to test specific aspects of knowledge graph embeddings:</p>
<div id="def-codex" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.13 (CODEX Datasets)</strong></span> CODEX (Comprehensive Dataset Extraction) is a collection of datasets (CODEX-S, CODEX-M, CODEX-L) derived from Wikidata and Wikipedia, designed to promote entity and content diversity:</p>
<ul>
<li>CODEX-S: 2,034 entities, 42 relations, 36,543 triples</li>
<li>CODEX-M: 17,050 entities, 51 relations, 206,205 triples</li>
<li>CODEX-L: 77,951 entities, 69 relations, 612,437 triples</li>
</ul>
</div>
<div id="def-ogb" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.14 (OGB Knowledge Graph Datasets)</strong></span> The Open Graph Benchmark (OGB) includes knowledge graph datasets optimized for benchmarking purposes:</p>
<ul>
<li>ogbl-wikikg2: 2,500,604 entities, 535 relations, 16,109,182 triples</li>
<li>ogbl-biokg: 93,773 entities, 51 relations, 5,088,434 triples</li>
</ul>
</div>
<p>These newer datasets provide more diverse evaluation scenarios and help assess model performance across different domains and scales.</p>
</section>
</section>
<section id="issues-with-standard-benchmarks" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="issues-with-standard-benchmarks"><span class="header-section-number">9.6</span> Issues with Standard Benchmarks</h2>
<p>Despite their widespread use, standard benchmark datasets have several limitations that researchers should be aware of:</p>
<section id="inverse-relation-bias" class="level3" data-number="9.6.1">
<h3 data-number="9.6.1" class="anchored" data-anchor-id="inverse-relation-bias"><span class="header-section-number">9.6.1</span> Inverse Relation Bias</h3>
<p>As discussed earlier, many early benchmarks like FB15k and WN18 contain inverse relations, which can lead to artificially high performance for models that can exploit this pattern.</p>
<div id="def-inverse-relation-bias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.15 (Inverse Relation Bias)</strong></span> Inverse relation bias occurs when:</p>
<ol type="1">
<li>A test triple <span class="math inline">\((h, r, t)\)</span> has a corresponding inverse triple <span class="math inline">\((t, r', h)\)</span> in the training set</li>
<li>Models can achieve high performance by simply recognizing that <span class="math inline">\(r\)</span> and <span class="math inline">\(r'\)</span> are inverse relations</li>
<li>This doesn’t test the model’s ability to infer new facts, only to recognize relation patterns</li>
</ol>
</div>
<p>While FB15k-237 and WN18RR address this issue to some extent, researchers should still be cautious about other potential biases in benchmark datasets.</p>
</section>
<section id="balanced-vs.-realistic-relations" class="level3" data-number="9.6.2">
<h3 data-number="9.6.2" class="anchored" data-anchor-id="balanced-vs.-realistic-relations"><span class="header-section-number">9.6.2</span> Balanced vs.&nbsp;Realistic Relations</h3>
<p>Many benchmark datasets have a more balanced distribution of relation types than real-world knowledge graphs, which can affect the generalizability of results.</p>
<div id="def-relation-imbalance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.16 (Relation Imbalance)</strong></span> In real-world knowledge graphs:</p>
<ol type="1">
<li>Some relations occur much more frequently than others</li>
<li>Some entities participate in many more triples than others</li>
<li>The distribution of relation types and entity participation is often highly skewed</li>
</ol>
</div>
<p>Models that perform well on balanced benchmark datasets may struggle with the imbalance of real-world knowledge graphs.</p>
</section>
<section id="domain-specificity" class="level3" data-number="9.6.3">
<h3 data-number="9.6.3" class="anchored" data-anchor-id="domain-specificity"><span class="header-section-number">9.6.3</span> Domain Specificity</h3>
<p>Most benchmark datasets focus on general-domain knowledge, but many real-world applications involve domain-specific knowledge graphs.</p>
<div id="def-domain-specificity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.17 (Domain Specificity)</strong></span> Domain-specific knowledge graphs:</p>
<ol type="1">
<li>May have different relation patterns and structures than general-domain knowledge graphs</li>
<li>Often have unique constraints and dependencies</li>
<li>May require different modeling approaches for optimal performance</li>
</ol>
</div>
<p>Performance on standard benchmarks may not accurately predict performance on domain-specific knowledge graphs.</p>
</section>
</section>
<section id="triple-classification-as-an-evaluation-task" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="triple-classification-as-an-evaluation-task"><span class="header-section-number">9.7</span> Triple Classification as an Evaluation Task</h2>
<p>In addition to link prediction, triple classification is another common evaluation task for knowledge graph embeddings.</p>
<div id="def-triple-classification" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.18 (Triple Classification Task)</strong></span> The triple classification task involves:</p>
<ol type="1">
<li>Training the model on the training triples</li>
<li>For each test triple <span class="math inline">\((h, r, t)\)</span>, classifying it as valid or invalid based on its score</li>
<li>Evaluating the classification performance using metrics like accuracy, precision, recall, and F1-score</li>
</ol>
</div>
<p>Triple classification requires a threshold to convert scores into binary classifications. This threshold is typically determined using the validation set to maximize classification performance.</p>
<div id="exm-triple-classification" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.6 (Example of Triple Classification)</strong></span> Consider a model that assigns the following scores to triples:</p>
<ul>
<li><span class="math inline">\((Bob, friendOf, Alice)\)</span>: 0.8</li>
<li><span class="math inline">\((Bob, friendOf, Charlie)\)</span>: 0.6</li>
<li><span class="math inline">\((Bob, livesIn, Paris)\)</span>: 0.2</li>
<li><span class="math inline">\((Bob, friendOf, New York)\)</span>: 0.1</li>
</ul>
<p>If the threshold for the “friendOf” relation is 0.5 and for the “livesIn” relation is 0.3, then:</p>
<ul>
<li><span class="math inline">\((Bob, friendOf, Alice)\)</span> and <span class="math inline">\((Bob, friendOf, Charlie)\)</span> would be classified as valid</li>
<li><span class="math inline">\((Bob, livesIn, Paris)\)</span> would be classified as invalid</li>
<li><span class="math inline">\((Bob, friendOf, New York)\)</span> would be classified as invalid</li>
</ul>
</div>
<p>While triple classification is conceptually simple, it has received less attention than link prediction in recent years. This is partly because the binary nature of the task provides less fine-grained information about model performance compared to ranking-based evaluation.</p>
</section>
<section id="implementation-of-evaluation-protocols" class="level2" data-number="9.8">
<h2 data-number="9.8" class="anchored" data-anchor-id="implementation-of-evaluation-protocols"><span class="header-section-number">9.8</span> Implementation of Evaluation Protocols</h2>
<p>Correctly implementing evaluation protocols is crucial for fair and reproducible comparisons between models. Here’s a pseudo-code example of how to implement the filtered evaluation protocol for link prediction:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_link_prediction(model, test_triples, all_triples, entities, relations):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluate a model on link prediction using the filtered setting.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    model: The trained KGE model</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    test_triples: List of test triples (h, r, t)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    all_triples: Set of all triples in the dataset (train + valid + test)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    entities: List of all entities</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    relations: List of all relations</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Dictionary of evaluation metrics</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    ranks_head <span class="op">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    ranks_tail <span class="op">=</span> []</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h, r, t <span class="kw">in</span> test_triples:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Head prediction</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        head_candidates <span class="op">=</span> [(e, r, t) <span class="cf">for</span> e <span class="kw">in</span> entities]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        head_scores <span class="op">=</span> model.score(head_candidates)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter existing triples</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        filtered_indices <span class="op">=</span> []</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (e, _, _) <span class="kw">in</span> <span class="bu">enumerate</span>(head_candidates):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> e <span class="op">!=</span> h <span class="kw">and</span> (e, r, t) <span class="kw">in</span> all_triples:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                filtered_indices.append(i)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute rank (assuming higher scores are better)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        true_score <span class="op">=</span> model.score([(h, r, t)])[<span class="dv">0</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        rank <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, score <span class="kw">in</span> <span class="bu">enumerate</span>(head_scores):</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> filtered_indices <span class="kw">and</span> score <span class="op">&gt;</span> true_score:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                rank <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        ranks_head.append(rank)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tail prediction (similar process)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        tail_candidates <span class="op">=</span> [(h, r, e) <span class="cf">for</span> e <span class="kw">in</span> entities]</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (similar to head prediction)</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        ranks_tail.append(rank)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute metrics</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    mean_rank <span class="op">=</span> (<span class="bu">sum</span>(ranks_head) <span class="op">+</span> <span class="bu">sum</span>(ranks_tail)) <span class="op">/</span> (<span class="bu">len</span>(ranks_head) <span class="op">+</span> <span class="bu">len</span>(ranks_tail))</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    mrr <span class="op">=</span> <span class="bu">sum</span>(<span class="fl">1.0</span><span class="op">/</span>r <span class="cf">for</span> r <span class="kw">in</span> ranks_head <span class="op">+</span> ranks_tail) <span class="op">/</span> (<span class="bu">len</span>(ranks_head) <span class="op">+</span> <span class="bu">len</span>(ranks_tail))</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    hits <span class="op">=</span> {}</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">10</span>]:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        hits[k] <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> r <span class="kw">in</span> ranks_head <span class="op">+</span> ranks_tail <span class="cf">if</span> r <span class="op">&lt;=</span> k) <span class="op">/</span> (<span class="bu">len</span>(ranks_head) <span class="op">+</span> <span class="bu">len</span>(ranks_tail))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mean_rank'</span>: mean_rank,</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mrr'</span>: mrr,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        <span class="st">'hits'</span>: hits</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In practice, this implementation would be optimized for computational efficiency, especially for large knowledge graphs with many entities.</p>
</section>
<section id="statistical-significance-testing" class="level2" data-number="9.9">
<h2 data-number="9.9" class="anchored" data-anchor-id="statistical-significance-testing"><span class="header-section-number">9.9</span> Statistical Significance Testing</h2>
<p>When comparing the performance of different models, it’s important to assess whether the observed differences are statistically significant.</p>
<div id="def-significance-testing" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.19 (Statistical Significance Testing)</strong></span> Statistical significance testing for KGE models involves:</p>
<ol type="1">
<li>Running multiple evaluations with different random initializations</li>
<li>Computing means and standard deviations of performance metrics</li>
<li>Applying statistical tests (e.g., t-test, Wilcoxon signed-rank test) to assess whether differences between models are significant</li>
</ol>
</div>
<p>Unfortunately, statistical significance testing is often overlooked in knowledge graph embedding research, which can lead to overestimating the importance of small performance differences.</p>
<div id="exm-significance-testing" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.7 (Example of Significance Testing)</strong></span> Consider two models with the following MRR results over 5 runs:</p>
<ul>
<li>Model A: [0.32, 0.30, 0.33, 0.31, 0.29], mean = 0.31, std = 0.015</li>
<li>Model B: [0.33, 0.34, 0.32, 0.35, 0.33], mean = 0.334, std = 0.011</li>
</ul>
<p>A t-test could be performed to determine if the difference (0.334 - 0.31 = 0.024) is statistically significant given the observed variability.</p>
</div>
</section>
<section id="common-evaluation-mistakes-and-pitfalls" class="level2" data-number="9.10">
<h2 data-number="9.10" class="anchored" data-anchor-id="common-evaluation-mistakes-and-pitfalls"><span class="header-section-number">9.10</span> Common Evaluation Mistakes and Pitfalls</h2>
<p>Several common mistakes can lead to misleading or invalid evaluation results:</p>
<section id="reporting-only-favorable-metrics" class="level3" data-number="9.10.1">
<h3 data-number="9.10.1" class="anchored" data-anchor-id="reporting-only-favorable-metrics"><span class="header-section-number">9.10.1</span> Reporting Only Favorable Metrics</h3>
<p>Some papers report only metrics where their model excels, giving an incomplete picture of performance.</p>
<div id="def-cherry-picking" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.20 (Cherry-Picking Metrics)</strong></span> Cherry-picking occurs when:</p>
<ol type="1">
<li>A paper presents only metrics where their model outperforms others</li>
<li>Important metrics where the model underperforms are omitted</li>
<li>This creates a misleading impression of overall performance</li>
</ol>
</div>
<p>To avoid this, researchers should report a comprehensive set of metrics, including mean rank, MRR, and Hits@k for different values of k.</p>
</section>
<section id="inconsistent-evaluation-protocols" class="level3" data-number="9.10.2">
<h3 data-number="9.10.2" class="anchored" data-anchor-id="inconsistent-evaluation-protocols"><span class="header-section-number">9.10.2</span> Inconsistent Evaluation Protocols</h3>
<p>Using different evaluation protocols when comparing models can lead to unfair comparisons.</p>
<div id="def-protocol-inconsistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.21 (Evaluation Protocol Inconsistency)</strong></span> Inconsistencies can include:</p>
<ol type="1">
<li>Using filtered metrics for one model but raw metrics for another</li>
<li>Using different entity filtering strategies</li>
<li>Using different ranking strategies (e.g., handling ties differently)</li>
</ol>
</div>
<p>Researchers should clearly describe their evaluation protocol and ensure that all models are evaluated using the same protocol.</p>
</section>
<section id="ignoring-hyperparameter-sensitivity" class="level3" data-number="9.10.3">
<h3 data-number="9.10.3" class="anchored" data-anchor-id="ignoring-hyperparameter-sensitivity"><span class="header-section-number">9.10.3</span> Ignoring Hyperparameter Sensitivity</h3>
<p>Some models are more sensitive to hyperparameter choices than others, which can affect the fairness of comparisons.</p>
<div id="def-hyperparameter-sensitivity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.22 (Hyperparameter Sensitivity)</strong></span> Issues with hyperparameter sensitivity include:</p>
<ol type="1">
<li>Some models may require extensive tuning to achieve good performance</li>
<li>The reported performance may represent the best-case scenario rather than typical performance</li>
<li>The computational cost of hyperparameter tuning may not be accounted for</li>
</ol>
</div>
<p>Researchers should report both the hyperparameter sensitivity of their models and the resources used for hyperparameter tuning.</p>
</section>
<section id="overtuning-on-test-set" class="level3" data-number="9.10.4">
<h3 data-number="9.10.4" class="anchored" data-anchor-id="overtuning-on-test-set"><span class="header-section-number">9.10.4</span> Overtuning on Test Set</h3>
<p>A serious methodological error is tuning hyperparameters based on test set performance.</p>
<div id="def-test-set-overtuning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.23 (Test Set Overtuning)</strong></span> Test set overtuning occurs when:</p>
<ol type="1">
<li>Hyperparameters are selected based on performance on the test set</li>
<li>This creates an artificial performance boost that won’t generalize to new data</li>
<li>It violates the principle that the test set should only be used for final evaluation</li>
</ol>
</div>
<p>Proper methodology requires using the validation set for hyperparameter tuning and the test set only for final evaluation.</p>
</section>
</section>
<section id="comparison-with-alternative-evaluation-approaches" class="level2" data-number="9.11">
<h2 data-number="9.11" class="anchored" data-anchor-id="comparison-with-alternative-evaluation-approaches"><span class="header-section-number">9.11</span> Comparison with Alternative Evaluation Approaches</h2>
<p>While link prediction and triple classification are the most common evaluation tasks, alternative approaches have been proposed to provide a more comprehensive assessment of knowledge graph embeddings.</p>
<section id="relational-pattern-evaluation" class="level3" data-number="9.11.1">
<h3 data-number="9.11.1" class="anchored" data-anchor-id="relational-pattern-evaluation"><span class="header-section-number">9.11.1</span> Relational Pattern Evaluation</h3>
<p>Relational pattern evaluation focuses on assessing how well models capture different types of relation patterns (symmetry, antisymmetry, inversion, composition, etc.).</p>
<div id="def-pattern-evaluation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.24 (Relational Pattern Evaluation)</strong></span> Relational pattern evaluation involves:</p>
<ol type="1">
<li>Identifying sets of triples that exhibit specific relation patterns</li>
<li>Evaluating model performance on these sets separately</li>
<li>Comparing performance across different relation patterns</li>
</ol>
</div>
<p>This approach provides insights into the strengths and limitations of different models with respect to specific relation patterns.</p>
</section>
<section id="inductive-evaluation" class="level3" data-number="9.11.2">
<h3 data-number="9.11.2" class="anchored" data-anchor-id="inductive-evaluation"><span class="header-section-number">9.11.2</span> Inductive Evaluation</h3>
<p>Traditional evaluation focuses on the transductive setting, where all entities are known during training. Inductive evaluation assesses how well models generalize to previously unseen entities.</p>
<div id="def-inductive-evaluation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.25 (Inductive Evaluation)</strong></span> Inductive evaluation involves:</p>
<ol type="1">
<li>Removing a subset of entities and their associated triples from the training set</li>
<li>Training the model on the remaining data</li>
<li>Evaluating the model’s ability to predict triples involving the unseen entities</li>
</ol>
</div>
<p>Inductive evaluation is particularly relevant for real-world applications where new entities are continually being added to the knowledge graph.</p>
</section>
<section id="explainability-assessment" class="level3" data-number="9.11.3">
<h3 data-number="9.11.3" class="anchored" data-anchor-id="explainability-assessment"><span class="header-section-number">9.11.3</span> Explainability Assessment</h3>
<p>As explainability becomes increasingly important in AI, new evaluation approaches focus on assessing how well the learned embeddings can be interpreted and explained.</p>
<div id="def-explainability-assessment" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.26 (Explainability Assessment)</strong></span> Explainability assessment involves:</p>
<ol type="1">
<li>Evaluating whether the learned embeddings align with human-understandable concepts</li>
<li>Testing whether the model can provide explanations for its predictions</li>
<li>Assessing the quality of generated explanations</li>
</ol>
</div>
<p>Explainability assessment is an emerging area of evaluation that goes beyond traditional performance metrics.</p>
</section>
</section>
<section id="benchmarking-libraries-and-frameworks" class="level2" data-number="9.12">
<h2 data-number="9.12" class="anchored" data-anchor-id="benchmarking-libraries-and-frameworks"><span class="header-section-number">9.12</span> Benchmarking Libraries and Frameworks</h2>
<p>Several libraries and frameworks have been developed to facilitate the benchmarking of knowledge graph embedding models.</p>
<div id="def-benchmarking-frameworks" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.27 (KGE Benchmarking Frameworks)</strong></span> Popular frameworks include:</p>
<ol type="1">
<li><strong>PyKEEN</strong>: A comprehensive framework for training and evaluating knowledge graph embeddings</li>
<li><strong>LibKGE</strong>: A library focused on reproducible experimentation with knowledge graph embeddings</li>
<li><strong>DGL-KE</strong>: A scalable package built on deep graph library for learning large-scale knowledge graph embeddings</li>
<li><strong>OpenKE</strong>: An open-source framework for knowledge embedding</li>
<li><strong>AmpliGraph</strong>: A library for representation learning on knowledge graphs</li>
</ol>
</div>
<p>These frameworks provide standardized implementations of various models and evaluation protocols, making it easier to conduct fair and reproducible comparisons.</p>
</section>
<section id="toward-better-evaluation-practices" class="level2" data-number="9.13">
<h2 data-number="9.13" class="anchored" data-anchor-id="toward-better-evaluation-practices"><span class="header-section-number">9.13</span> Toward Better Evaluation Practices</h2>
<p>As the field of knowledge graph embeddings matures, there is a growing need for more comprehensive and rigorous evaluation practices.</p>
<section id="reproducibility-initiatives" class="level3" data-number="9.13.1">
<h3 data-number="9.13.1" class="anchored" data-anchor-id="reproducibility-initiatives"><span class="header-section-number">9.13.1</span> Reproducibility Initiatives</h3>
<p>Several initiatives aim to improve reproducibility in knowledge graph embedding research:</p>
<div id="def-reproducibility-initiatives" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.28 (Reproducibility Initiatives)</strong></span> Key initiatives include:</p>
<ol type="1">
<li><strong>Open-source code requirements</strong>: Many conferences and journals now require code submissions</li>
<li><strong>Standardized benchmarking platforms</strong>: Platforms like PyKEEN and LibKGE that enforce consistent evaluation</li>
<li><strong>Reproducibility challenges</strong>: Events focused on reproducing and verifying published results</li>
</ol>
</div>
<p>These initiatives help ensure that reported performance gains are real and reproducible.</p>
</section>
<section id="multi-faceted-evaluation" class="level3" data-number="9.13.2">
<h3 data-number="9.13.2" class="anchored" data-anchor-id="multi-faceted-evaluation"><span class="header-section-number">9.13.2</span> Multi-Faceted Evaluation</h3>
<p>Rather than focusing on a single metric or task, multi-faceted evaluation provides a more comprehensive assessment of model capabilities:</p>
<div id="def-multi-faceted-evaluation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.29 (Multi-Faceted Evaluation)</strong></span> Multi-faceted evaluation includes:</p>
<ol type="1">
<li><strong>Multiple tasks</strong>: Link prediction, triple classification, relation prediction, etc.</li>
<li><strong>Multiple metrics</strong>: MRR, Hits@k, precision, recall, etc.</li>
<li><strong>Multiple datasets</strong>: Across different domains and scales</li>
<li><strong>Pattern-specific evaluation</strong>: Performance on different relation patterns</li>
<li><strong>Resource efficiency evaluation</strong>: Training time, memory usage, etc.</li>
</ol>
</div>
<p>This approach gives a more complete picture of a model’s strengths and limitations.</p>
</section>
<section id="realistic-evaluation-scenarios" class="level3" data-number="9.13.3">
<h3 data-number="9.13.3" class="anchored" data-anchor-id="realistic-evaluation-scenarios"><span class="header-section-number">9.13.3</span> Realistic Evaluation Scenarios</h3>
<p>Moving beyond standard benchmarks to more realistic evaluation scenarios:</p>
<div id="def-realistic-evaluation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.30 (Realistic Evaluation Scenarios)</strong></span> Realistic scenarios include:</p>
<ol type="1">
<li><strong>Time-evolving knowledge graphs</strong>: Testing on temporal data</li>
<li><strong>Noisy knowledge graphs</strong>: Testing robustness to errors</li>
<li><strong>Sparse knowledge graphs</strong>: Testing with limited training data</li>
<li><strong>Multi-modal knowledge graphs</strong>: Incorporating different types of information</li>
<li><strong>Domain-specific knowledge graphs</strong>: Testing on specialized domains</li>
</ol>
</div>
<p>These scenarios better reflect the challenges faced in real-world applications of knowledge graph embeddings.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="9.14">
<h2 data-number="9.14" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">9.14</span> Conclusion</h2>
<p>Proper evaluation is essential for tracking progress in knowledge graph embedding research and for selecting appropriate models for specific applications. The field has evolved from simple metrics on small datasets to more comprehensive evaluation approaches that assess multiple facets of model performance.</p>
<p>Key takeaways from this chapter include:</p>
<ol type="1">
<li>The importance of using filtered evaluation settings for link prediction</li>
<li>The limitations of standard benchmark datasets and the need for more diverse evaluation scenarios</li>
<li>The value of reporting multiple metrics to provide a comprehensive assessment of model performance</li>
<li>The need for statistical significance testing to validate performance differences</li>
<li>The importance of avoiding common evaluation pitfalls, such as inconsistent protocols and test set overtuning</li>
</ol>
<p>As the field continues to evolve, evaluation methodologies will likely become even more sophisticated, with greater emphasis on reproducibility, comprehensive assessment, and real-world relevance.</p>
</section>
<section id="further-reading" class="level2" data-number="9.15">
<h2 data-number="9.15" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">9.15</span> Further Reading</h2>
<ol type="1">
<li>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. NIPS.</li>
<li>Toutanova, K., &amp; Chen, D. (2015). Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.</li>
<li>Sun, Z., Deng, Z. H., Nie, J. Y., &amp; Tang, J. (2019). RotatE: Knowledge graph embedding by relational rotation in complex space. ICLR.</li>
<li>Ali, M., Berrendorf, M., Hoyt, C. T., Vermue, L., Galkin, M., Sharifzadeh, S., … &amp; Lehmann, J. (2021). PyKEEN 1.0: A Python library for training and evaluating knowledge graph embeddings. Journal of Machine Learning Research, 22(82), 1-6.</li>
<li>Ruffinelli, D., Broscheit, S., &amp; Gemulla, R. (2020). You CAN teach an old dog new tricks! On training knowledge graph embeddings. ICLR.</li>
<li>Safavi, T., &amp; Koutra, D. (2020). CoDEx: A comprehensive knowledge graph completion benchmark. EMNLP.</li>
<li>Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., … &amp; Leskovec, J. (2020). Open graph benchmark: Datasets for machine learning on graphs. NeurIPS.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/training.html" class="pagination-link" aria-label="Training and Optimization Techniques">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/additional-knowledge.html" class="pagination-link" aria-label="Incorporating Additional Information">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>