<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Appendix A: Mathematical Foundations – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/appendix-b.html" rel="next">
<link href="../content/frontier.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/appendix-a.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-mathematical-foundations" id="toc-introduction-to-mathematical-foundations" class="nav-link active" data-scroll-target="#introduction-to-mathematical-foundations"><span class="header-section-number">15.1</span> Introduction to Mathematical Foundations</a></li>
  <li><a href="#linear-algebra-essentials" id="toc-linear-algebra-essentials" class="nav-link" data-scroll-target="#linear-algebra-essentials"><span class="header-section-number">15.2</span> Linear Algebra Essentials</a>
  <ul class="collapse">
  <li><a href="#vectors-and-vector-spaces" id="toc-vectors-and-vector-spaces" class="nav-link" data-scroll-target="#vectors-and-vector-spaces"><span class="header-section-number">15.2.1</span> Vectors and Vector Spaces</a></li>
  <li><a href="#basis-and-dimension" id="toc-basis-and-dimension" class="nav-link" data-scroll-target="#basis-and-dimension"><span class="header-section-number">15.2.2</span> Basis and Dimension</a></li>
  <li><a href="#inner-products-and-norms" id="toc-inner-products-and-norms" class="nav-link" data-scroll-target="#inner-products-and-norms"><span class="header-section-number">15.2.3</span> Inner Products and Norms</a></li>
  <li><a href="#matrices-and-linear-transformations" id="toc-matrices-and-linear-transformations" class="nav-link" data-scroll-target="#matrices-and-linear-transformations"><span class="header-section-number">15.2.4</span> Matrices and Linear Transformations</a></li>
  <li><a href="#matrix-properties-and-decompositions" id="toc-matrix-properties-and-decompositions" class="nav-link" data-scroll-target="#matrix-properties-and-decompositions"><span class="header-section-number">15.2.5</span> Matrix Properties and Decompositions</a></li>
  </ul></li>
  <li><a href="#tensor-algebra" id="toc-tensor-algebra" class="nav-link" data-scroll-target="#tensor-algebra"><span class="header-section-number">15.3</span> Tensor Algebra</a>
  <ul class="collapse">
  <li><a href="#tensor-definitions-and-operations" id="toc-tensor-definitions-and-operations" class="nav-link" data-scroll-target="#tensor-definitions-and-operations"><span class="header-section-number">15.3.1</span> Tensor Definitions and Operations</a></li>
  <li><a href="#tensor-decompositions" id="toc-tensor-decompositions" class="nav-link" data-scroll-target="#tensor-decompositions"><span class="header-section-number">15.3.2</span> Tensor Decompositions</a></li>
  </ul></li>
  <li><a href="#complex-vector-spaces" id="toc-complex-vector-spaces" class="nav-link" data-scroll-target="#complex-vector-spaces"><span class="header-section-number">15.4</span> Complex Vector Spaces</a>
  <ul class="collapse">
  <li><a href="#complex-numbers-and-operations" id="toc-complex-numbers-and-operations" class="nav-link" data-scroll-target="#complex-numbers-and-operations"><span class="header-section-number">15.4.1</span> Complex Numbers and Operations</a></li>
  <li><a href="#complex-matrix-operations" id="toc-complex-matrix-operations" class="nav-link" data-scroll-target="#complex-matrix-operations"><span class="header-section-number">15.4.2</span> Complex Matrix Operations</a></li>
  </ul></li>
  <li><a href="#probability-and-statistics" id="toc-probability-and-statistics" class="nav-link" data-scroll-target="#probability-and-statistics"><span class="header-section-number">15.5</span> Probability and Statistics</a>
  <ul class="collapse">
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions"><span class="header-section-number">15.5.1</span> Probability Distributions</a></li>
  <li><a href="#bayesian-statistics" id="toc-bayesian-statistics" class="nav-link" data-scroll-target="#bayesian-statistics"><span class="header-section-number">15.5.2</span> Bayesian Statistics</a></li>
  <li><a href="#information-theory" id="toc-information-theory" class="nav-link" data-scroll-target="#information-theory"><span class="header-section-number">15.5.3</span> Information Theory</a></li>
  </ul></li>
  <li><a href="#metric-spaces-and-distance-functions" id="toc-metric-spaces-and-distance-functions" class="nav-link" data-scroll-target="#metric-spaces-and-distance-functions"><span class="header-section-number">15.6</span> Metric Spaces and Distance Functions</a>
  <ul class="collapse">
  <li><a href="#non-euclidean-geometries" id="toc-non-euclidean-geometries" class="nav-link" data-scroll-target="#non-euclidean-geometries"><span class="header-section-number">15.6.1</span> Non-Euclidean Geometries</a></li>
  </ul></li>
  <li><a href="#graph-theory-basics" id="toc-graph-theory-basics" class="nav-link" data-scroll-target="#graph-theory-basics"><span class="header-section-number">15.7</span> Graph Theory Basics</a></li>
  <li><a href="#optimization-theory" id="toc-optimization-theory" class="nav-link" data-scroll-target="#optimization-theory"><span class="header-section-number">15.8</span> Optimization Theory</a>
  <ul class="collapse">
  <li><a href="#objective-functions" id="toc-objective-functions" class="nav-link" data-scroll-target="#objective-functions"><span class="header-section-number">15.8.1</span> Objective Functions</a></li>
  <li><a href="#gradient-descent-and-variants" id="toc-gradient-descent-and-variants" class="nav-link" data-scroll-target="#gradient-descent-and-variants"><span class="header-section-number">15.8.2</span> Gradient Descent and Variants</a></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques"><span class="header-section-number">15.8.3</span> Regularization Techniques</a></li>
  </ul></li>
  <li><a href="#differential-calculus-for-optimization" id="toc-differential-calculus-for-optimization" class="nav-link" data-scroll-target="#differential-calculus-for-optimization"><span class="header-section-number">15.9</span> Differential Calculus for Optimization</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">15.10</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="introduction-to-mathematical-foundations" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="introduction-to-mathematical-foundations"><span class="header-section-number">15.1</span> Introduction to Mathematical Foundations</h2>
<p>This appendix provides a comprehensive overview of the mathematical concepts that underpin knowledge graph embeddings. While the main chapters introduce these concepts as needed, this appendix serves as a more formal and detailed reference. Understanding these mathematical foundations is essential for both implementing existing knowledge graph embedding models and developing new approaches.</p>
<p>We begin with the basics of linear algebra and vector spaces, progress through more advanced topics like tensor algebra and complex vector spaces, and conclude with optimization theory concepts relevant to training knowledge graph embeddings. Each section aims to balance formal definitions with intuitive explanations and relevant examples.</p>
<p>This appendix is designed to be accessible to students with varying levels of mathematical background, while providing the depth necessary for a thorough understanding of the theoretical foundations of knowledge graph embeddings.</p>
</section>
<section id="linear-algebra-essentials" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="linear-algebra-essentials"><span class="header-section-number">15.2</span> Linear Algebra Essentials</h2>
<p>Linear algebra provides the fundamental mathematical framework for knowledge graph embeddings, as entities and relations are represented as vectors, matrices, or tensors.</p>
<section id="vectors-and-vector-spaces" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="vectors-and-vector-spaces"><span class="header-section-number">15.2.1</span> Vectors and Vector Spaces</h3>
<p>A vector space forms the basic mathematical structure in which knowledge graph embeddings operate.</p>
<div id="def-vector-space-formal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.1 (Vector Space (Formal Definition))</strong></span> A vector space <span class="math inline">\(V\)</span> over a field <span class="math inline">\(\mathbb{F}\)</span> consists of a set of vectors along with two operations:</p>
<ol type="1">
<li>Vector addition: <span class="math inline">\(+: V \times V \rightarrow V\)</span></li>
<li>Scalar multiplication: <span class="math inline">\(\cdot: \mathbb{F} \times V \rightarrow V\)</span></li>
</ol>
<p>These operations must satisfy the following axioms for all vectors <span class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span> and scalars <span class="math inline">\(\alpha, \beta \in \mathbb{F}\)</span>:</p>
<ol type="1">
<li><strong>Closure under addition</strong>: <span class="math inline">\(\mathbf{u} + \mathbf{v} \in V\)</span></li>
<li><strong>Commutativity of addition</strong>: <span class="math inline">\(\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}\)</span></li>
<li><strong>Associativity of addition</strong>: <span class="math inline">\((\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})\)</span></li>
<li><strong>Additive identity</strong>: There exists <span class="math inline">\(\mathbf{0} \in V\)</span> such that <span class="math inline">\(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</span> for all <span class="math inline">\(\mathbf{v} \in V\)</span></li>
<li><strong>Additive inverse</strong>: For each <span class="math inline">\(\mathbf{v} \in V\)</span>, there exists <span class="math inline">\(-\mathbf{v} \in V\)</span> such that <span class="math inline">\(\mathbf{v} + (-\mathbf{v}) = \mathbf{0}\)</span></li>
<li><strong>Closure under scalar multiplication</strong>: <span class="math inline">\(\alpha \cdot \mathbf{v} \in V\)</span></li>
<li><strong>Distributivity of scalar multiplication with respect to vector addition</strong>: <span class="math inline">\(\alpha \cdot (\mathbf{u} + \mathbf{v}) = \alpha \cdot \mathbf{u} + \alpha \cdot \mathbf{v}\)</span></li>
<li><strong>Distributivity of scalar multiplication with respect to field addition</strong>: <span class="math inline">\((\alpha + \beta) \cdot \mathbf{v} = \alpha \cdot \mathbf{v} + \beta \cdot \mathbf{v}\)</span></li>
<li><strong>Compatibility of scalar multiplication with field multiplication</strong>: <span class="math inline">\(\alpha \cdot (\beta \cdot \mathbf{v}) = (\alpha \beta) \cdot \mathbf{v}\)</span></li>
<li><strong>Scalar multiplication identity</strong>: <span class="math inline">\(1 \cdot \mathbf{v} = \mathbf{v}\)</span>, where <span class="math inline">\(1\)</span> is the multiplicative identity in <span class="math inline">\(\mathbb{F}\)</span></li>
</ol>
</div>
<p>In the context of knowledge graph embeddings, we typically work with the vector space <span class="math inline">\(\mathbb{R}^d\)</span> (or sometimes <span class="math inline">\(\mathbb{C}^d\)</span>), where vectors are represented as <span class="math inline">\(d\)</span>-tuples of real (or complex) numbers.</p>
<div id="exm-vector-space-r3" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.1 (Vector Space <span class="math inline">\(\mathbb{R}^3\)</span>)</strong></span> The vector space <span class="math inline">\(\mathbb{R}^3\)</span> consists of all ordered triplets <span class="math inline">\((x, y, z)\)</span> where <span class="math inline">\(x, y, z \in \mathbb{R}\)</span>.</p>
<p>Vector addition: <span class="math inline">\((x_1, y_1, z_1) + (x_2, y_2, z_2) = (x_1 + x_2, y_1 + y_2, z_1 + z_2)\)</span> Scalar multiplication: <span class="math inline">\(\alpha \cdot (x, y, z) = (\alpha x, \alpha y, \alpha z)\)</span></p>
<p>For example:</p>
<ul>
<li><span class="math inline">\((1, 2, 3) + (4, 5, 6) = (5, 7, 9)\)</span></li>
<li><span class="math inline">\(2 \cdot (1, 2, 3) = (2, 4, 6)\)</span></li>
</ul>
<p>In knowledge graph embeddings, each entity and relation would be represented as a vector in a similar fashion, though typically in a much higher-dimensional space.</p>
</div>
</section>
<section id="basis-and-dimension" class="level3" data-number="15.2.2">
<h3 data-number="15.2.2" class="anchored" data-anchor-id="basis-and-dimension"><span class="header-section-number">15.2.2</span> Basis and Dimension</h3>
<p>The concepts of basis and dimension are fundamental to understanding vector spaces.</p>
<div id="def-basis-dimension" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.2 (Basis and Dimension)</strong></span> A set of vectors <span class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}\)</span> in a vector space <span class="math inline">\(V\)</span> is a <strong>basis</strong> if:</p>
<ol type="1">
<li>The vectors are linearly independent: <span class="math inline">\(\alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \ldots + \alpha_n\mathbf{v}_n = \mathbf{0}\)</span> implies <span class="math inline">\(\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0\)</span>.</li>
<li>The vectors span the space: Every vector <span class="math inline">\(\mathbf{v} \in V\)</span> can be written as a linear combination of the basis vectors: <span class="math inline">\(\mathbf{v} = \beta_1\mathbf{v}_1 + \beta_2\mathbf{v}_2 + \ldots + \beta_n\mathbf{v}_n\)</span> for some scalars <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_n\)</span>.</li>
</ol>
<p>The <strong>dimension</strong> of a vector space is the number of vectors in any basis for the space. If a vector space has a finite basis, it is called finite-dimensional.</p>
</div>
<p>In <span class="math inline">\(\mathbb{R}^d\)</span>, the standard basis consists of the <span class="math inline">\(d\)</span> unit vectors, each with a 1 in one position and 0s elsewhere.</p>
<div id="exm-standard-basis" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.2 (Standard Basis in <span class="math inline">\(\mathbb{R}^3\)</span>)</strong></span> The standard basis for <span class="math inline">\(\mathbb{R}^3\)</span> consists of: <span class="math inline">\(\mathbf{e}_1 = (1, 0, 0)\)</span> <span class="math inline">\(\mathbf{e}_2 = (0, 1, 0)\)</span> <span class="math inline">\(\mathbf{e}_3 = (0, 0, 1)\)</span></p>
<p>Any vector <span class="math inline">\((x, y, z) \in \mathbb{R}^3\)</span> can be written as: <span class="math inline">\((x, y, z) = x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3\)</span></p>
<p>The dimension of <span class="math inline">\(\mathbb{R}^3\)</span> is 3.</p>
</div>
</section>
<section id="inner-products-and-norms" class="level3" data-number="15.2.3">
<h3 data-number="15.2.3" class="anchored" data-anchor-id="inner-products-and-norms"><span class="header-section-number">15.2.3</span> Inner Products and Norms</h3>
<p>Inner products and norms are essential for measuring similarities and distances in embedding spaces.</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.3 (Inner Product)</strong></span> An inner product on a vector space <span class="math inline">\(V\)</span> over <span class="math inline">\(\mathbb{R}\)</span> is a function <span class="math inline">\(\langle \cdot, \cdot \rangle: V \times V \rightarrow \mathbb{R}\)</span> that satisfies the following for all <span class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>:</p>
<ol type="1">
<li><strong>Linearity in the first argument</strong>: <span class="math inline">\(\langle \alpha\mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \alpha\langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle\)</span></li>
<li><strong>Symmetry</strong>: <span class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle\)</span></li>
<li><strong>Positive definiteness</strong>: <span class="math inline">\(\langle \mathbf{v}, \mathbf{v} \rangle \geq 0\)</span>, with equality if and only if <span class="math inline">\(\mathbf{v} = \mathbf{0}\)</span></li>
</ol>
<p>The standard inner product (dot product) in <span class="math inline">\(\mathbb{R}^d\)</span> is: <span class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^d u_i v_i\)</span></p>
</div>
<p>The inner product induces a norm, which measures the length of a vector.</p>
<div id="def-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.4 (Norm)</strong></span> A norm on a vector space <span class="math inline">\(V\)</span> is a function <span class="math inline">\(\|\cdot\|: V \rightarrow \mathbb{R}\)</span> that satisfies the following for all <span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <span class="math inline">\(\|\mathbf{v}\| \geq 0\)</span>, with equality if and only if <span class="math inline">\(\mathbf{v} = \mathbf{0}\)</span></li>
<li><strong>Homogeneity</strong>: <span class="math inline">\(\|\alpha\mathbf{v}\| = |\alpha|\|\mathbf{v}\|\)</span></li>
<li><strong>Triangle inequality</strong>: <span class="math inline">\(\|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|\)</span></li>
</ol>
<p>Given an inner product, the induced norm is: <span class="math inline">\(\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}\)</span></p>
<p>Common norms include:</p>
<ul>
<li><span class="math inline">\(L_2\)</span> norm (Euclidean norm): <span class="math inline">\(\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^d v_i^2}\)</span></li>
<li><span class="math inline">\(L_1\)</span> norm (Manhattan norm): <span class="math inline">\(\|\mathbf{v}\|_1 = \sum_{i=1}^d |v_i|\)</span></li>
<li><span class="math inline">\(L_p\)</span> norm: <span class="math inline">\(\|\mathbf{v}\|_p = \left(\sum_{i=1}^d |v_i|^p\right)^{1/p}\)</span> for <span class="math inline">\(p \geq 1\)</span></li>
<li><span class="math inline">\(L_\infty\)</span> norm (Maximum norm): <span class="math inline">\(\|\mathbf{v}\|_\infty = \max_i |v_i|\)</span></li>
</ul>
</div>
<p>Norms are used to define distances between vectors, which are crucial in many knowledge graph embedding models.</p>
<div id="def-distance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.5 (Distance)</strong></span> Given a norm <span class="math inline">\(\|\cdot\|\)</span>, the induced distance function <span class="math inline">\(d: V \times V \rightarrow \mathbb{R}\)</span> is: <span class="math inline">\(d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|\)</span></p>
<p>This defines a metric space, satisfying:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <span class="math inline">\(d(\mathbf{u}, \mathbf{v}) \geq 0\)</span>, with equality if and only if <span class="math inline">\(\mathbf{u} = \mathbf{v}\)</span></li>
<li><strong>Symmetry</strong>: <span class="math inline">\(d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})\)</span></li>
<li><strong>Triangle inequality</strong>: <span class="math inline">\(d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})\)</span></li>
</ol>
</div>
<div id="exm-inner-product-norm" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.3 (Inner Product, Norm, and Distance Example)</strong></span> Consider vectors <span class="math inline">\(\mathbf{u} = (1, 2, 3)\)</span> and <span class="math inline">\(\mathbf{v} = (4, 5, 6)\)</span> in <span class="math inline">\(\mathbb{R}^3\)</span>.</p>
<p>Inner product: <span class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle = 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 4 + 10 + 18 = 32\)</span></p>
<p><span class="math inline">\(L_2\)</span> norm of <span class="math inline">\(\mathbf{u}\)</span>: <span class="math inline">\(\|\mathbf{u}\|_2 = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} \approx 3.74\)</span></p>
<p><span class="math inline">\(L_1\)</span> norm of <span class="math inline">\(\mathbf{u}\)</span>: <span class="math inline">\(\|\mathbf{u}\|_1 = |1| + |2| + |3| = 6\)</span></p>
<p><span class="math inline">\(L_2\)</span> distance between <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span>: <span class="math inline">\(d_2(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|_2 = \|(1-4, 2-5, 3-6)\|_2 = \|(-3, -3, -3)\|_2 = \sqrt{9 + 9 + 9} = \sqrt{27} \approx 5.2\)</span></p>
</div>
</section>
<section id="matrices-and-linear-transformations" class="level3" data-number="15.2.4">
<h3 data-number="15.2.4" class="anchored" data-anchor-id="matrices-and-linear-transformations"><span class="header-section-number">15.2.4</span> Matrices and Linear Transformations</h3>
<p>Matrices are fundamental for representing linear transformations, which are crucial in many knowledge graph embedding models.</p>
<div id="def-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.6 (Matrix)</strong></span> A matrix is a rectangular array of numbers. An <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> has <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns, with entries <span class="math inline">\(A_{ij}\)</span> for <span class="math inline">\(1 \leq i \leq m\)</span> and <span class="math inline">\(1 \leq j \leq n\)</span>.</p>
<p>Basic matrix operations include:</p>
<ol type="1">
<li><strong>Addition</strong>: <span class="math inline">\((A + B)_{ij} = A_{ij} + B_{ij}\)</span> (requires matrices of the same dimensions)</li>
<li><strong>Scalar multiplication</strong>: <span class="math inline">\((\alpha A)_{ij} = \alpha A_{ij}\)</span></li>
<li><strong>Matrix multiplication</strong>: <span class="math inline">\((AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}\)</span> (requires the number of columns in <span class="math inline">\(A\)</span> to equal the number of rows in <span class="math inline">\(B\)</span>)</li>
<li><strong>Transpose</strong>: <span class="math inline">\(A^T_{ij} = A_{ji}\)</span></li>
</ol>
</div>
<div id="def-linear-transformation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.7 (Linear Transformation)</strong></span> A linear transformation <span class="math inline">\(T: V \rightarrow W\)</span> between vector spaces is a function that preserves vector addition and scalar multiplication:</p>
<ol type="1">
<li><span class="math inline">\(T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})\)</span> for all <span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span></li>
<li><span class="math inline">\(T(\alpha\mathbf{v}) = \alpha T(\mathbf{v})\)</span> for all <span class="math inline">\(\alpha \in \mathbb{F}\)</span> and <span class="math inline">\(\mathbf{v} \in V\)</span></li>
</ol>
<p>Every linear transformation <span class="math inline">\(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> can be represented by an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> such that: <span class="math inline">\(T(\mathbf{v}) = A\mathbf{v}\)</span> for all <span class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span></p>
</div>
<p>In knowledge graph embeddings, matrices often represent relations or relation-specific transformations.</p>
<div id="exm-matrix-transformation" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.4 (Matrix Transformation Example)</strong></span> Consider a relation “capitalOf” represented by the matrix: <span class="math inline">\(R = \begin{bmatrix} 0.8 &amp; 0.1 \\ 0.2 &amp; 0.9 \end{bmatrix}\)</span></p>
<p>If the entity “Paris” is represented by the vector <span class="math inline">\(\mathbf{e}_{Paris} = (0.6, 0.4)\)</span>, then the transformation by <span class="math inline">\(R\)</span> gives: <span class="math inline">\(R\mathbf{e}_{Paris} = \begin{bmatrix} 0.8 &amp; 0.1 \\ 0.2 &amp; 0.9 \end{bmatrix} \begin{bmatrix} 0.6 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.8 \cdot 0.6 + 0.1 \cdot 0.4 \\ 0.2 \cdot 0.6 + 0.9 \cdot 0.4 \end{bmatrix} = \begin{bmatrix} 0.52 \\ 0.48 \end{bmatrix}\)</span></p>
<p>In a model like RESCAL, this transformed vector would be compared with the embedding of “France” to determine the plausibility of the triple (“Paris”, “capitalOf”, “France”).</p>
</div>
</section>
<section id="matrix-properties-and-decompositions" class="level3" data-number="15.2.5">
<h3 data-number="15.2.5" class="anchored" data-anchor-id="matrix-properties-and-decompositions"><span class="header-section-number">15.2.5</span> Matrix Properties and Decompositions</h3>
<p>Various matrix properties and decompositions play important roles in knowledge graph embedding models.</p>
<div id="def-matrix-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.8 (Matrix Properties)</strong></span> Key matrix properties include:</p>
<ol type="1">
<li><strong>Symmetry</strong>: A matrix <span class="math inline">\(A\)</span> is symmetric if <span class="math inline">\(A = A^T\)</span></li>
<li><strong>Positive definiteness</strong>: A symmetric matrix <span class="math inline">\(A\)</span> is positive definite if <span class="math inline">\(\mathbf{x}^T A \mathbf{x} &gt; 0\)</span> for all non-zero vectors <span class="math inline">\(\mathbf{x}\)</span></li>
<li><strong>Orthogonality</strong>: A matrix <span class="math inline">\(Q\)</span> is orthogonal if <span class="math inline">\(Q^T Q = Q Q^T = I\)</span> (its columns and rows form orthonormal bases)</li>
<li><strong>Rank</strong>: The rank of a matrix is the dimension of its column space (or row space)</li>
<li><strong>Determinant</strong>: A scalar value that can be calculated from a square matrix, representing the scaling factor of the transformation</li>
<li><strong>Eigenvalues and eigenvectors</strong>: For a square matrix <span class="math inline">\(A\)</span>, a non-zero vector <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector with eigenvalue <span class="math inline">\(\lambda\)</span> if <span class="math inline">\(A\mathbf{v} = \lambda\mathbf{v}\)</span></li>
</ol>
</div>
<p>Matrix decompositions are ways of factoring a matrix into a product of simpler matrices, which can be useful for dimensionality reduction, noise reduction, and efficient computation.</p>
<div id="def-matrix-decompositions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.9 (Matrix Decompositions)</strong></span> Important matrix decompositions include:</p>
<ol type="1">
<li><strong>Eigendecomposition</strong>: For a diagonalizable matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(A = P D P^{-1}\)</span> where <span class="math inline">\(D\)</span> is diagonal and <span class="math inline">\(P\)</span> has eigenvectors as columns</li>
<li><strong>Singular Value Decomposition (SVD)</strong>: Any matrix <span class="math inline">\(A\)</span> can be written as <span class="math inline">\(A = U \Sigma V^T\)</span> where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal and <span class="math inline">\(\Sigma\)</span> is diagonal with non-negative entries</li>
<li><strong>QR Decomposition</strong>: Any matrix <span class="math inline">\(A\)</span> can be written as <span class="math inline">\(A = QR\)</span> where <span class="math inline">\(Q\)</span> is orthogonal and <span class="math inline">\(R\)</span> is upper triangular</li>
<li><strong>LU Decomposition</strong>: Under certain conditions, a matrix <span class="math inline">\(A\)</span> can be written as <span class="math inline">\(A = LU\)</span> where <span class="math inline">\(L\)</span> is lower triangular and <span class="math inline">\(U\)</span> is upper triangular</li>
</ol>
</div>
<p>Matrix decompositions are used in some knowledge graph embedding models, particularly those based on tensor factorization.</p>
<div id="exm-svd" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.5 (Singular Value Decomposition Example)</strong></span> Consider a matrix representing entity-relation interactions: <span class="math inline">\(A = \begin{bmatrix} 3 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; -2 \end{bmatrix}\)</span></p>
<p>The SVD of <span class="math inline">\(A\)</span> is <span class="math inline">\(A = U \Sigma V^T\)</span> where: <span class="math inline">\(U = \begin{bmatrix} 0.71 &amp; 0.71 \\ 0.71 &amp; -0.71 \end{bmatrix}\)</span> <span class="math inline">\(\Sigma = \begin{bmatrix} 5 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \end{bmatrix}\)</span> <span class="math inline">\(V = \begin{bmatrix} 0.58 &amp; 0.58 &amp; 0.58 \\ 0.58 &amp; 0.58 &amp; -0.58 \\ 0.58 &amp; -0.82 &amp; 0 \end{bmatrix}\)</span></p>
<p>This decomposition can be used to create low-dimensional embeddings by keeping only the largest singular values and their corresponding vectors.</p>
</div>
</section>
</section>
<section id="tensor-algebra" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="tensor-algebra"><span class="header-section-number">15.3</span> Tensor Algebra</h2>
<p>Tensors extend the concepts of vectors and matrices to higher dimensions and provide a natural way to represent multi-relational data.</p>
<section id="tensor-definitions-and-operations" class="level3" data-number="15.3.1">
<h3 data-number="15.3.1" class="anchored" data-anchor-id="tensor-definitions-and-operations"><span class="header-section-number">15.3.1</span> Tensor Definitions and Operations</h3>
<p>A tensor is a multi-dimensional array that generalizes vectors (1D tensors) and matrices (2D tensors).</p>
<div id="def-tensor" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.10 (Tensor)</strong></span> A tensor <span class="math inline">\(\mathcal{T}\)</span> of order <span class="math inline">\(n\)</span> (or rank <span class="math inline">\(n\)</span>) is an <span class="math inline">\(n\)</span>-dimensional array with elements <span class="math inline">\(\mathcal{T}_{i_1 i_2 \ldots i_n}\)</span> where each index <span class="math inline">\(i_j\)</span> ranges from 1 to <span class="math inline">\(d_j\)</span>.</p>
<p>The shape of the tensor is <span class="math inline">\((d_1, d_2, \ldots, d_n)\)</span>, and the total number of elements is <span class="math inline">\(d_1 \times d_2 \times \ldots \times d_n\)</span>.</p>
<p>Basic tensor operations include:</p>
<ol type="1">
<li><strong>Addition</strong>: <span class="math inline">\((\mathcal{T} + \mathcal{U})_{i_1 i_2 \ldots i_n} = \mathcal{T}_{i_1 i_2 \ldots i_n} + \mathcal{U}_{i_1 i_2 \ldots i_n}\)</span></li>
<li><strong>Scalar multiplication</strong>: <span class="math inline">\((\alpha\mathcal{T})_{i_1 i_2 \ldots i_n} = \alpha\mathcal{T}_{i_1 i_2 \ldots i_n}\)</span></li>
<li><strong>Tensor product</strong>: <span class="math inline">\((\mathcal{T} \otimes \mathcal{U})_{i_1 \ldots i_n j_1 \ldots j_m} = \mathcal{T}_{i_1 \ldots i_n} \mathcal{U}_{j_1 \ldots j_m}\)</span></li>
<li><strong>Contraction</strong>: Summing over pairs of indices, e.g., <span class="math inline">\(\mathcal{C}_{i_1 \ldots i_{n-2}} = \sum_{j=1}^d \mathcal{T}_{i_1 \ldots i_{n-2} j j}\)</span></li>
</ol>
</div>
<p>Knowledge graphs can be naturally represented as 3rd-order tensors, with two modes for entities (head and tail) and one mode for relations.</p>
<div id="exm-knowledge-graph-tensor" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.6 (Knowledge Graph as a Tensor)</strong></span> A knowledge graph with <span class="math inline">\(n_e\)</span> entities and <span class="math inline">\(n_r\)</span> relations can be represented as a 3rd-order tensor <span class="math inline">\(\mathcal{X} \in \{0, 1\}^{n_e \times n_r \times n_e}\)</span> where: <span class="math inline">\(\mathcal{X}_{hrt} = 1\)</span> if the triple (entity <span class="math inline">\(h\)</span>, relation <span class="math inline">\(r\)</span>, entity <span class="math inline">\(t\)</span>) exists in the knowledge graph <span class="math inline">\(\mathcal{X}_{hrt} = 0\)</span> otherwise</p>
<p>For example, if we have 3 entities (Alice, Bob, Charlie) and 2 relations (likes, knows), the tensor might be: <span class="math inline">\(\mathcal{X}_{1,1,2} = 1\)</span> (Alice likes Bob) <span class="math inline">\(\mathcal{X}_{1,2,3} = 1\)</span> (Alice knows Charlie) <span class="math inline">\(\mathcal{X}_{2,2,1} = 1\)</span> (Bob knows Alice) All other entries are 0.</p>
</div>
</section>
<section id="tensor-decompositions" class="level3" data-number="15.3.2">
<h3 data-number="15.3.2" class="anchored" data-anchor-id="tensor-decompositions"><span class="header-section-number">15.3.2</span> Tensor Decompositions</h3>
<p>Tensor decompositions extend matrix factorization techniques to higher-order tensors and are fundamental to several knowledge graph embedding approaches.</p>
<div id="def-tensor-decompositions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.11 (Tensor Decompositions)</strong></span> Major tensor decomposition methods include:</p>
<ol type="1">
<li><p><strong>CP Decomposition (CANDECOMP/PARAFAC)</strong>: Approximates a tensor as a sum of rank-one tensors: <span class="math inline">\(\mathcal{T} \approx \sum_{r=1}^R \mathbf{a}_r \otimes \mathbf{b}_r \otimes \mathbf{c}_r \otimes \ldots\)</span> where <span class="math inline">\(\otimes\)</span> denotes the outer product.</p></li>
<li><p><strong>Tucker Decomposition</strong>: Decomposes a tensor into a core tensor multiplied by a matrix along each mode: <span class="math inline">\(\mathcal{T} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C \times \ldots\)</span> where <span class="math inline">\(\times_n\)</span> denotes the n-mode product and <span class="math inline">\(\mathcal{G}\)</span> is the core tensor.</p></li>
<li><p><strong>Tensor Train Decomposition</strong>: Represents a tensor as a train of lower-order tensors: <span class="math inline">\(\mathcal{T}_{i_1 i_2 \ldots i_d} \approx G_1[i_1] G_2[i_2] \ldots G_d[i_d]\)</span> where each <span class="math inline">\(G_k[i_k]\)</span> is a matrix.</p></li>
</ol>
</div>
<p>These decompositions form the basis of several knowledge graph embedding models, particularly RESCAL, which is based on a form of Tucker decomposition.</p>
<div id="exm-rescal-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.7 (RESCAL as Tensor Decomposition)</strong></span> In RESCAL, the knowledge graph tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{n_e \times n_r \times n_e}\)</span> is decomposed as: <span class="math inline">\(\mathcal{X} \approx \sum_{i=1}^d \sum_{j=1}^d \mathcal{R}_{:,i,j} \otimes \mathbf{A}_{:,i} \otimes \mathbf{A}_{:,j}\)</span></p>
<p>where <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n_e \times d}\)</span> contains the entity embeddings and <span class="math inline">\(\mathcal{R} \in \mathbb{R}^{n_r \times d \times d}\)</span> contains the relation matrices.</p>
<p>This can also be written in terms of matrix products: <span class="math inline">\(\mathcal{X}_r \approx \mathbf{A} \mathbf{R}_r \mathbf{A}^T\)</span></p>
<p>where <span class="math inline">\(\mathcal{X}_r\)</span> is the <span class="math inline">\(r\)</span>-th slice of the tensor (a matrix) and <span class="math inline">\(\mathbf{R}_r\)</span> is the <span class="math inline">\(r\)</span>-th slice of <span class="math inline">\(\mathcal{R}\)</span> (also a matrix).</p>
</div>
</section>
</section>
<section id="complex-vector-spaces" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="complex-vector-spaces"><span class="header-section-number">15.4</span> Complex Vector Spaces</h2>
<p>Complex vector spaces extend real vector spaces by using complex numbers as the scalar field, providing additional modeling capabilities for knowledge graph embeddings.</p>
<section id="complex-numbers-and-operations" class="level3" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="complex-numbers-and-operations"><span class="header-section-number">15.4.1</span> Complex Numbers and Operations</h3>
<p>Complex numbers form the basis of complex vector spaces and have unique properties that can be leveraged in embedding models.</p>
<div id="def-complex-numbers" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.12 (Complex Numbers)</strong></span> A complex number <span class="math inline">\(z \in \mathbb{C}\)</span> is expressed as <span class="math inline">\(z = a + bi\)</span>, where <span class="math inline">\(a, b \in \mathbb{R}\)</span> and <span class="math inline">\(i\)</span> is the imaginary unit satisfying <span class="math inline">\(i^2 = -1\)</span>.</p>
<p>The real part of <span class="math inline">\(z\)</span> is <span class="math inline">\(\text{Re}(z) = a\)</span> and the imaginary part is <span class="math inline">\(\text{Im}(z) = b\)</span>.</p>
<p>Basic operations on complex numbers include:</p>
<ol type="1">
<li><strong>Addition</strong>: <span class="math inline">\((a + bi) + (c + di) = (a + c) + (b + d)i\)</span></li>
<li><strong>Multiplication</strong>: <span class="math inline">\((a + bi)(c + di) = (ac - bd) + (ad + bc)i\)</span></li>
<li><strong>Complex conjugate</strong>: <span class="math inline">\(\overline{z} = \overline{a + bi} = a - bi\)</span></li>
<li><strong>Modulus</strong>: <span class="math inline">\(|z| = |a + bi| = \sqrt{a^2 + b^2}\)</span></li>
<li><strong>Argument</strong>: <span class="math inline">\(\arg(z) = \tan^{-1}(b/a)\)</span> (adjusted for the quadrant)</li>
</ol>
</div>
<div id="def-complex-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.13 (Complex Vector Space)</strong></span> A complex vector space <span class="math inline">\(\mathbb{C}^d\)</span> consists of <span class="math inline">\(d\)</span>-tuples of complex numbers. Basic operations include:</p>
<ol type="1">
<li><strong>Vector addition</strong>: <span class="math inline">\(\mathbf{z} + \mathbf{w} = (z_1 + w_1, z_2 + w_2, \ldots, z_d + w_d)\)</span></li>
<li><strong>Scalar multiplication</strong>: <span class="math inline">\(\alpha\mathbf{z} = (\alpha z_1, \alpha z_2, \ldots, \alpha z_d)\)</span> for <span class="math inline">\(\alpha \in \mathbb{C}\)</span></li>
</ol>
<p>The Hermitian inner product is defined as: <span class="math inline">\(\langle \mathbf{z}, \mathbf{w} \rangle = \sum_{j=1}^d z_j \overline{w_j}\)</span></p>
<p>The induced norm is: <span class="math inline">\(\|\mathbf{z}\| = \sqrt{\langle \mathbf{z}, \mathbf{z} \rangle} = \sqrt{\sum_{j=1}^d |z_j|^2}\)</span></p>
</div>
<p>Complex vector spaces enable operations like rotations in the complex plane, which are useful for modeling antisymmetric relations.</p>
<div id="exm-complex-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.8 (Complex Embedding Example)</strong></span> In the ComplEx model, entities and relations are embedded in complex space.</p>
<p>If the entity “Paris” is represented by <span class="math inline">\(\mathbf{e}_{Paris} = (0.6 + 0.2i, 0.1 - 0.5i)\)</span> and the relation “capitalOf” is represented by <span class="math inline">\(\mathbf{r}_{capitalOf} = (0.8 + 0.1i, 0.3 + 0.2i)\)</span>, then the Hermitian product between them is: <span class="math inline">\(\langle \mathbf{e}_{Paris}, \mathbf{r}_{capitalOf} \rangle = (0.6 + 0.2i)(0.8 - 0.1i) + (0.1 - 0.5i)(0.3 - 0.2i)\)</span> <span class="math inline">\(= (0.6 \cdot 0.8 + 0.2 \cdot 0.1) + i(0.2 \cdot 0.8 - 0.6 \cdot 0.1) + (0.1 \cdot 0.3 + 0.5 \cdot 0.2) + i(- 0.5 \cdot 0.3 - 0.1 \cdot 0.2)\)</span> <span class="math inline">\(= 0.5 + 0.14i\)</span></p>
<p>This complex-valued result captures both symmetric and antisymmetric aspects of the relation.</p>
</div>
</section>
<section id="complex-matrix-operations" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="complex-matrix-operations"><span class="header-section-number">15.4.2</span> Complex Matrix Operations</h3>
<p>Complex matrices extend the concept of matrices to complex numbers and provide additional modeling capabilities.</p>
<div id="def-complex-matrices" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.14 (Complex Matrices)</strong></span> A complex matrix <span class="math inline">\(A \in \mathbb{C}^{m \times n}\)</span> has complex entries <span class="math inline">\(A_{ij} = a_{ij} + b_{ij}i\)</span> where <span class="math inline">\(a_{ij}, b_{ij} \in \mathbb{R}\)</span>.</p>
<p>Key operations and properties include:</p>
<ol type="1">
<li><strong>Conjugate transpose</strong>: <span class="math inline">\(A^H = \overline{A^T}\)</span>, with entries <span class="math inline">\((A^H)_{ij} = \overline{A_{ji}}\)</span></li>
<li><strong>Hermitian matrix</strong>: <span class="math inline">\(A\)</span> is Hermitian if <span class="math inline">\(A = A^H\)</span></li>
<li><strong>Unitary matrix</strong>: <span class="math inline">\(A\)</span> is unitary if <span class="math inline">\(A^H A = A A^H = I\)</span></li>
<li><strong>Spectral decomposition</strong>: A Hermitian matrix <span class="math inline">\(A\)</span> can be decomposed as <span class="math inline">\(A = Q \Lambda Q^H\)</span> where <span class="math inline">\(Q\)</span> is unitary and <span class="math inline">\(\Lambda\)</span> is diagonal with real entries</li>
</ol>
</div>
<p>Complex matrices are used in models like ComplEx to represent relations with both symmetric and antisymmetric components.</p>
<div id="exm-complex-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.9 (Complex Matrix Example)</strong></span> In an extension of ComplEx, relations can be represented as complex matrices. For example, the relation “capitalOf” might be represented as: <span class="math inline">\(R_{capitalOf} = \begin{bmatrix} 0.8 + 0.1i &amp; 0.2 - 0.3i \\ 0.1 + 0.4i &amp; 0.9 + 0.2i \end{bmatrix}\)</span></p>
<p>This matrix can transform entity embeddings through complex matrix multiplication, capturing both symmetric and antisymmetric aspects of the relation.</p>
</div>
</section>
</section>
<section id="probability-and-statistics" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="probability-and-statistics"><span class="header-section-number">15.5</span> Probability and Statistics</h2>
<p>Probabilistic knowledge graph embedding models rely on concepts from probability and statistics.</p>
<section id="probability-distributions" class="level3" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="probability-distributions"><span class="header-section-number">15.5.1</span> Probability Distributions</h3>
<p>Probability distributions are fundamental to probabilistic knowledge graph embedding models.</p>
<div id="def-probability-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.15 (Probability Distribution)</strong></span> A probability distribution is a function that assigns probabilities to events in a sample space.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the probability mass function (PMF) <span class="math inline">\(P(X = x)\)</span> gives the probability of each possible value.</p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the probability density function (PDF) <span class="math inline">\(f_X(x)\)</span> satisfies: <span class="math inline">\(P(a \leq X \leq b) = \int_a^b f_X(x) dx\)</span></p>
<p>Key properties of probability distributions include:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <span class="math inline">\(P(X = x) \geq 0\)</span> or <span class="math inline">\(f_X(x) \geq 0\)</span></li>
<li><strong>Normalization</strong>: <span class="math inline">\(\sum_x P(X = x) = 1\)</span> or <span class="math inline">\(\int_{-\infty}^{\infty} f_X(x) dx = 1\)</span></li>
</ol>
</div>
<p>Common probability distributions used in knowledge graph embeddings include Gaussian (normal), Bernoulli, and uniform distributions.</p>
<div id="exm-gaussian-distribution" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.10 (Gaussian Distribution)</strong></span> The Gaussian (normal) distribution has PDF: <span class="math inline">\(f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>In probabilistic knowledge graph embeddings, entity and relation vectors might be modeled as samples from multivariate Gaussian distributions: <span class="math inline">\(\mathbf{e} \sim \mathcal{N}(\boldsymbol{\mu}_e, \boldsymbol{\Sigma}_e)\)</span> <span class="math inline">\(\mathbf{r} \sim \mathcal{N}(\boldsymbol{\mu}_r, \boldsymbol{\Sigma}_r)\)</span></p>
<p>This allows for representing uncertainty in the embeddings.</p>
</div>
</section>
<section id="bayesian-statistics" class="level3" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="bayesian-statistics"><span class="header-section-number">15.5.2</span> Bayesian Statistics</h3>
<p>Bayesian statistics provides a framework for updating probability distributions based on new evidence, which is useful for learning knowledge graph embeddings.</p>
<div id="def-bayes-theorem" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.16 (Bayes’ Theorem)</strong></span> Bayes’ theorem states: <span class="math inline">\(P(A|B) = \frac{P(B|A) P(A)}{P(B)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(P(A|B)\)</span> is the posterior probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(P(B|A)\)</span> is the likelihood of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span></li>
<li><span class="math inline">\(P(A)\)</span> is the prior probability of <span class="math inline">\(A\)</span></li>
<li><span class="math inline">\(P(B)\)</span> is the marginal probability of <span class="math inline">\(B\)</span></li>
</ul>
<p>In the context of knowledge graph embeddings:</p>
<ul>
<li><span class="math inline">\(A\)</span> could represent the embeddings</li>
<li><span class="math inline">\(B\)</span> could represent the observed triples</li>
<li><span class="math inline">\(P(A|B)\)</span> would be the posterior distribution of embeddings given the observed triples</li>
</ul>
</div>
<p>Bayesian knowledge graph embedding models learn distributions over embeddings rather than point estimates.</p>
<div id="exm-bayesian-kge" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.11 (Bayesian Knowledge Graph Embedding)</strong></span> In a Bayesian KGE model:</p>
<ol type="1">
<li><p><strong>Prior</strong>: We might place prior distributions on entity and relation embeddings: <span class="math inline">\(P(\mathbf{e}_i) = \mathcal{N}(\mathbf{0}, \sigma_e^2\mathbf{I})\)</span> <span class="math inline">\(P(\mathbf{r}_j) = \mathcal{N}(\mathbf{0}, \sigma_r^2\mathbf{I})\)</span></p></li>
<li><p><strong>Likelihood</strong>: We define a likelihood function for observed triples: <span class="math inline">\(P((h, r, t)|\mathbf{E}, \mathbf{R}) = \sigma(f_r(\mathbf{e}_h, \mathbf{e}_t))\)</span> where <span class="math inline">\(\sigma\)</span> is the sigmoid function and <span class="math inline">\(f_r\)</span> is a scoring function.</p></li>
<li><p><strong>Posterior</strong>: We compute the posterior distribution of embeddings given observed triples: <span class="math inline">\(P(\mathbf{E}, \mathbf{R}|\mathcal{D}) \propto P(\mathcal{D}|\mathbf{E}, \mathbf{R}) P(\mathbf{E}) P(\mathbf{R})\)</span> where <span class="math inline">\(\mathcal{D}\)</span> is the set of observed triples.</p></li>
<li><p><strong>Inference</strong>: We use techniques like variational inference or MCMC to approximate the posterior.</p></li>
</ol>
</div>
</section>
<section id="information-theory" class="level3" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="information-theory"><span class="header-section-number">15.5.3</span> Information Theory</h3>
<p>Information theory provides tools for measuring uncertainty and information content, which are relevant to knowledge graph embedding learning and evaluation.</p>
<div id="def-entropy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.17 (Entropy and KL Divergence)</strong></span> Entropy measures the uncertainty or information content of a random variable: <span class="math inline">\(H(X) = -\sum_x P(X = x) \log P(X = x)\)</span> (discrete case) <span class="math inline">\(H(X) = -\int f_X(x) \log f_X(x) dx\)</span> (continuous case)</p>
<p>Kullback-Leibler (KL) divergence measures the difference between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>: <span class="math inline">\(D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}\)</span> (discrete case) <span class="math inline">\(D_{KL}(P||Q) = \int f_P(x) \log \frac{f_P(x)}{f_Q(x)} dx\)</span> (continuous case)</p>
</div>
<p>Information theory concepts can be used in learning algorithms for knowledge graph embeddings, particularly in variational approaches.</p>
<div id="exm-variational-kge" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.12 (Variational Knowledge Graph Embedding)</strong></span> In a variational approach to knowledge graph embedding:</p>
<ol type="1">
<li><p>We approximate the true posterior <span class="math inline">\(P(\mathbf{E}, \mathbf{R}|\mathcal{D})\)</span> with a simpler distribution <span class="math inline">\(Q(\mathbf{E}, \mathbf{R})\)</span>.</p></li>
<li><p>We minimize the KL divergence between <span class="math inline">\(Q\)</span> and the true posterior: <span class="math inline">\(D_{KL}(Q(\mathbf{E}, \mathbf{R})||P(\mathbf{E}, \mathbf{R}|\mathcal{D}))\)</span></p></li>
<li><p>This is equivalent to maximizing the Evidence Lower Bound (ELBO): <span class="math inline">\(\mathcal{L}(Q) = \mathbb{E}_Q[\log P(\mathcal{D}|\mathbf{E}, \mathbf{R})] - D_{KL}(Q(\mathbf{E}, \mathbf{R})||P(\mathbf{E}, \mathbf{R}))\)</span></p></li>
<li><p>We can use this approach to learn probabilistic embeddings that capture uncertainty.</p></li>
</ol>
</div>
</section>
</section>
<section id="metric-spaces-and-distance-functions" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="metric-spaces-and-distance-functions"><span class="header-section-number">15.6</span> Metric Spaces and Distance Functions</h2>
<p>Metric spaces provide a general framework for measuring distances, which is essential for many knowledge graph embedding models.</p>
<div id="def-metric-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.18 (Metric Space)</strong></span> A metric space is a set <span class="math inline">\(X\)</span> equipped with a distance function (metric) <span class="math inline">\(d: X \times X \rightarrow \mathbb{R}\)</span> that satisfies the following for all <span class="math inline">\(x, y, z \in X\)</span>:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: <span class="math inline">\(d(x, y) \geq 0\)</span></li>
<li><strong>Identity of indiscernibles</strong>: <span class="math inline">\(d(x, y) = 0\)</span> if and only if <span class="math inline">\(x = y\)</span></li>
<li><strong>Symmetry</strong>: <span class="math inline">\(d(x, y) = d(y, x)\)</span></li>
<li><strong>Triangle inequality</strong>: <span class="math inline">\(d(x, z) \leq d(x, y) + d(y, z)\)</span></li>
</ol>
</div>
<p>Different metric spaces and distance functions capture different aspects of similarity between entities and relations.</p>
<div id="def-distance-functions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.19 (Distance Functions)</strong></span> Common distance functions in knowledge graph embeddings include:</p>
<ol type="1">
<li><strong>Euclidean distance</strong>: <span class="math inline">\(d_2(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{\sum_i (x_i - y_i)^2}\)</span></li>
<li><strong>Manhattan distance</strong>: <span class="math inline">\(d_1(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_i |x_i - y_i|\)</span></li>
<li><strong>Minkowski distance</strong>: <span class="math inline">\(d_p(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_p = \left(\sum_i |x_i - y_i|^p\right)^{1/p}\)</span></li>
<li><strong>Mahalanobis distance</strong>: <span class="math inline">\(d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T M (\mathbf{x} - \mathbf{y})}\)</span> where <span class="math inline">\(M\)</span> is a positive definite matrix</li>
<li><strong>Cosine distance</strong>: <span class="math inline">\(d_{\cos}(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}\)</span></li>
</ol>
</div>
<div id="exm-distance-functions" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.13 (Distance Functions Example)</strong></span> Consider entity embeddings <span class="math inline">\(\mathbf{e}_1 = (1, 2, 3)\)</span> and <span class="math inline">\(\mathbf{e}_2 = (4, 2, 1)\)</span>.</p>
<p>Euclidean distance: <span class="math inline">\(d_2(\mathbf{e}_1, \mathbf{e}_2) = \sqrt{(1-4)^2 + (2-2)^2 + (3-1)^2} = \sqrt{9 + 0 + 4} = \sqrt{13} \approx 3.61\)</span></p>
<p>Manhattan distance: <span class="math inline">\(d_1(\mathbf{e}_1, \mathbf{e}_2) = |1-4| + |2-2| + |3-1| = 3 + 0 + 2 = 5\)</span></p>
<p>Cosine distance: <span class="math inline">\(d_{\cos}(\mathbf{e}_1, \mathbf{e}_2) = 1 - \frac{1 \cdot 4 + 2 \cdot 2 + 3 \cdot 1}{\sqrt{1^2 + 2^2 + 3^2} \cdot \sqrt{4^2 + 2^2 + 1^2}} = 1 - \frac{11}{\sqrt{14} \cdot \sqrt{21}} \approx 0.28\)</span></p>
</div>
<section id="non-euclidean-geometries" class="level3" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="non-euclidean-geometries"><span class="header-section-number">15.6.1</span> Non-Euclidean Geometries</h3>
<p>Non-Euclidean geometries, including hyperbolic and spherical spaces, provide alternative embedding spaces with unique properties.</p>
<div id="def-hyperbolic-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.20 (Hyperbolic Space)</strong></span> Hyperbolic space is a non-Euclidean space with constant negative curvature. The Poincaré ball model represents hyperbolic space as the interior of a unit ball.</p>
<p>In the Poincaré ball model, the distance between points <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is: <span class="math inline">\(d_H(\mathbf{x}, \mathbf{y}) = \cosh^{-1} \left( 1 + 2 \frac{\|\mathbf{x} - \mathbf{y}\|^2}{(1 - \|\mathbf{x}\|^2)(1 - \|\mathbf{y}\|^2)} \right)\)</span></p>
<p>Hyperbolic space has the property that the volume of a ball grows exponentially with its radius, making it well-suited for embedding hierarchical structures.</p>
</div>
<div id="def-spherical-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.21 (Spherical Space)</strong></span> Spherical space is a non-Euclidean space with constant positive curvature. It can be represented as the surface of a unit sphere.</p>
<p>The distance between points <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> on the unit sphere is the great-circle distance: <span class="math inline">\(d_S(\mathbf{x}, \mathbf{y}) = \cos^{-1}(\mathbf{x} \cdot \mathbf{y})\)</span> (assuming <span class="math inline">\(\|\mathbf{x}\| = \|\mathbf{y}\| = 1\)</span>)</p>
<p>Spherical space is useful for embedding data with bounded, symmetric relationships.</p>
</div>
<div id="exm-non-euclidean" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.14 (Non-Euclidean Embedding Example)</strong></span> In a hyperbolic knowledge graph embedding model:</p>
<ol type="1">
<li><p>Entities are embedded as points in the Poincaré ball: <span class="math inline">\(\mathbf{e} \in \mathbb{B}^d = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\| &lt; 1\}\)</span></p></li>
<li><p>Hierarchical relationships can be modeled efficiently, with abstract entities (e.g., “Animal”) closer to the origin and specific entities (e.g., “Bengal Tiger”) closer to the boundary.</p></li>
<li><p>The distance function captures the hierarchical similarity, with points at similar depths having smaller distances than points at different depths.</p></li>
</ol>
</div>
</section>
</section>
<section id="graph-theory-basics" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="graph-theory-basics"><span class="header-section-number">15.7</span> Graph Theory Basics</h2>
<p>Knowledge graphs are fundamentally graph structures, so understanding basic graph theory is essential.</p>
<div id="def-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.22 (Graph)</strong></span> A graph <span class="math inline">\(G = (V, E)\)</span> consists of a set of vertices (or nodes) <span class="math inline">\(V\)</span> and a set of edges <span class="math inline">\(E \subseteq V \times V\)</span> connecting pairs of vertices.</p>
<p>In a directed graph, edges have a direction, so <span class="math inline">\((u, v) \in E\)</span> does not imply <span class="math inline">\((v, u) \in E\)</span>.</p>
<p>In a labeled graph, edges are associated with labels from a set <span class="math inline">\(L\)</span>, forming triples <span class="math inline">\((u, l, v)\)</span> where <span class="math inline">\(u, v \in V\)</span> and <span class="math inline">\(l \in L\)</span>.</p>
<p>A knowledge graph is a directed, labeled graph where vertices represent entities and edge labels represent relation types.</p>
</div>
<div id="def-graph-properties" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.23 (Graph Properties)</strong></span> Important graph properties include:</p>
<ol type="1">
<li><p><strong>Degree</strong>: The number of edges incident to a vertex. In a directed graph, we distinguish between in-degree and out-degree.</p></li>
<li><p><strong>Path</strong>: A sequence of vertices where each consecutive pair is connected by an edge. The length of a path is the number of edges.</p></li>
<li><p><strong>Connected component</strong>: A maximal subgraph in which any two vertices are connected by a path.</p></li>
<li><p><strong>Cycle</strong>: A path that starts and ends at the same vertex and contains at least one edge.</p></li>
<li><p><strong>Tree</strong>: A connected graph with no cycles.</p></li>
</ol>
</div>
<p>These graph properties can inform the design and analysis of knowledge graph embedding models.</p>
<div id="exm-graph-properties" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.15 (Graph Properties Example)</strong></span> Consider a small knowledge graph with entities E = {Alice, Bob, Charlie, Dave} and relations R = {friendOf, likes}:</p>
<ul>
<li>(Alice, friendOf, Bob)</li>
<li>(Bob, friendOf, Alice)</li>
<li>(Alice, likes, Charlie)</li>
<li>(Bob, likes, Charlie)</li>
<li>(Charlie, friendOf, Dave)</li>
</ul>
<p>Graph properties:</p>
<ul>
<li>Alice has out-degree 2 and in-degree 1</li>
<li>The path (Alice, Bob, Charlie, Dave) connects Alice to Dave</li>
<li>The graph forms a single connected component</li>
<li>(Alice, Bob, Alice) forms a cycle</li>
<li>The subgraph with edges {(Alice, likes, Charlie), (Bob, likes, Charlie), (Charlie, friendOf, Dave)} forms a tree</li>
</ul>
</div>
</section>
<section id="optimization-theory" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="optimization-theory"><span class="header-section-number">15.8</span> Optimization Theory</h2>
<p>Optimization theory provides the mathematical foundation for learning knowledge graph embeddings from data.</p>
<section id="objective-functions" class="level3" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="objective-functions"><span class="header-section-number">15.8.1</span> Objective Functions</h3>
<p>Objective functions quantify how well the embeddings model the knowledge graph.</p>
<div id="def-objective-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.24 (Objective Function)</strong></span> An objective function <span class="math inline">\(f(\mathbf{\theta})\)</span> maps a set of parameters <span class="math inline">\(\mathbf{\theta}\)</span> to a scalar value that we aim to minimize or maximize.</p>
<p>In knowledge graph embeddings, the parameters <span class="math inline">\(\mathbf{\theta}\)</span> include the entity and relation embeddings, and the objective function typically measures how well these embeddings model the observed triples.</p>
<p>Common objective functions include:</p>
<ol type="1">
<li><p><strong>Margin-based ranking loss</strong>: <span class="math inline">\(\mathcal{L} = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'} \max(0, \gamma + s(h', r, t') - s(h, r, t))\)</span> where <span class="math inline">\(S\)</span> is the set of observed triples, <span class="math inline">\(S'\)</span> is the set of corrupted triples, <span class="math inline">\(s\)</span> is a scoring function, and <span class="math inline">\(\gamma\)</span> is a margin.</p></li>
<li><p><strong>Logistic loss</strong>: <span class="math inline">\(\mathcal{L} = \sum_{(h,r,t) \in S \cup S'} y_{hrt} \log \sigma(s(h, r, t)) + (1 - y_{hrt}) \log(1 - \sigma(s(h, r, t)))\)</span> where <span class="math inline">\(y_{hrt} = 1\)</span> for observed triples and <span class="math inline">\(y_{hrt} = 0\)</span> for corrupted triples, and <span class="math inline">\(\sigma\)</span> is the sigmoid function.</p></li>
<li><p><strong>Negative sampling loss</strong>: <span class="math inline">\(\mathcal{L} = \sum_{(h,r,t) \in S} \log \sigma(s(h, r, t)) + \sum_{i=1}^k \mathbb{E}_{(h',r,t') \sim P_n} [\log \sigma(-s(h', r, t'))]\)</span> where <span class="math inline">\(P_n\)</span> is a distribution for sampling negative examples.</p></li>
</ol>
</div>
<div id="exm-objective-function" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.16 (Objective Function Example)</strong></span> For TransE with margin-based ranking loss, the objective function is: <span class="math inline">\(\mathcal{L} = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'} \max(0, \gamma - \|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\| + \|\mathbf{e}_{h'} + \mathbf{r}_r - \mathbf{e}_{t'}\|)\)</span></p>
<p>For positive triple (Alice, friendOf, Bob) and negative triple (Alice, friendOf, Charlie): <span class="math inline">\(\max(0, \gamma - \|\mathbf{e}_{Alice} + \mathbf{r}_{friendOf} - \mathbf{e}_{Bob}\| + \|\mathbf{e}_{Alice} + \mathbf{r}_{friendOf} - \mathbf{e}_{Charlie}\|)\)</span></p>
<p>The loss is positive if the negative triple’s score is within <span class="math inline">\(\gamma\)</span> of the positive triple’s score, encouraging the model to score positive triples higher than negative ones.</p>
</div>
</section>
<section id="gradient-descent-and-variants" class="level3" data-number="15.8.2">
<h3 data-number="15.8.2" class="anchored" data-anchor-id="gradient-descent-and-variants"><span class="header-section-number">15.8.2</span> Gradient Descent and Variants</h3>
<p>Gradient descent and its variants are the primary optimization algorithms for learning knowledge graph embeddings.</p>
<div id="def-gradient-descent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.25 (Gradient Descent)</strong></span> Gradient descent is an iterative optimization algorithm that updates parameters in the direction of steepest descent of the objective function: <span class="math inline">\(\mathbf{\theta}_{t+1} = \mathbf{\theta}_t - \eta \nabla f(\mathbf{\theta}_t)\)</span> where <span class="math inline">\(\eta &gt; 0\)</span> is the learning rate and <span class="math inline">\(\nabla f(\mathbf{\theta}_t)\)</span> is the gradient of the objective function with respect to the parameters.</p>
<p>Variants include:</p>
<ol type="1">
<li><strong>Stochastic Gradient Descent (SGD)</strong>: Updates parameters using gradients computed on small batches of data.</li>
<li><strong>Momentum</strong>: Adds a fraction of the previous update to the current update, helping to accelerate learning.</li>
<li><strong>Adagrad</strong>: Adapts the learning rate for each parameter based on historical gradients.</li>
<li><strong>RMSprop</strong>: Normalizes the gradient by a running average of its recent magnitude.</li>
<li><strong>Adam</strong>: Combines ideas from momentum and RMSprop.</li>
</ol>
</div>
<div id="exm-gradient-descent" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.17 (Gradient Descent Example)</strong></span> For TransE with objective function <span class="math inline">\(\mathcal{L}\)</span>, the gradient with respect to the entity embedding <span class="math inline">\(\mathbf{e}_h\)</span> for a positive triple <span class="math inline">\((h, r, t)\)</span> and negative triple <span class="math inline">\((h', r, t')\)</span> is: <span class="math inline">\(\nabla_{\mathbf{e}_h} \mathcal{L} = \begin{cases}
\frac{(\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t)}{\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|} &amp; \text{if } \gamma - \|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\| + \|\mathbf{e}_{h'} + \mathbf{r}_r - \mathbf{e}_{t'}\| &gt; 0 \\
\mathbf{0} &amp; \text{otherwise}
\end{cases}\)</span></p>
<p>The update rule using SGD would be: <span class="math inline">\(\mathbf{e}_h \leftarrow \mathbf{e}_h - \eta \nabla_{\mathbf{e}_h} \mathcal{L}\)</span></p>
</div>
</section>
<section id="regularization-techniques" class="level3" data-number="15.8.3">
<h3 data-number="15.8.3" class="anchored" data-anchor-id="regularization-techniques"><span class="header-section-number">15.8.3</span> Regularization Techniques</h3>
<p>Regularization techniques help prevent overfitting and improve generalization in knowledge graph embedding models.</p>
<div id="def-regularization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.26 (Regularization)</strong></span> Regularization adds constraints or penalties to the objective function to prevent overfitting. Common regularization techniques include:</p>
<ol type="1">
<li><p><strong>L2 regularization</strong>: Adds a penalty term proportional to the squared L2 norm of the parameters: <span class="math inline">\(\mathcal{R}(\mathbf{\theta}) = \lambda \|\mathbf{\theta}\|_2^2\)</span></p></li>
<li><p><strong>L1 regularization</strong>: Adds a penalty term proportional to the L1 norm of the parameters: <span class="math inline">\(\mathcal{R}(\mathbf{\theta}) = \lambda \|\mathbf{\theta}\|_1\)</span></p></li>
<li><p><strong>Elastic Net</strong>: Combines L1 and L2 regularization: <span class="math inline">\(\mathcal{R}(\mathbf{\theta}) = \lambda_1 \|\mathbf{\theta}\|_1 + \lambda_2 \|\mathbf{\theta}\|_2^2\)</span></p></li>
<li><p><strong>Dropout</strong>: Randomly sets a fraction of parameters to zero during training.</p></li>
<li><p><strong>Noise addition</strong>: Adds random noise to parameters during training.</p></li>
<li><p><strong>Norm constraints</strong>: Constrains parameters to have specific norms, e.g., unit norm.</p></li>
</ol>
</div>
<div id="exm-regularization" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.18 (Regularization Example)</strong></span> For TransE with L2 regularization, the regularized objective function is: <span class="math inline">\(\mathcal{L}_{reg} = \mathcal{L} + \lambda \sum_{e \in E} \|\mathbf{e}\|_2^2 + \lambda \sum_{r \in R} \|\mathbf{r}\|_2^2\)</span></p>
<p>where <span class="math inline">\(\lambda &gt; 0\)</span> is the regularization strength.</p>
<p>This encourages the model to learn smaller embedding vectors, reducing the risk of overfitting.</p>
<p>Alternatively, TransE can use norm constraints by projecting entity embeddings onto the unit sphere after each update: <span class="math inline">\(\mathbf{e} \leftarrow \frac{\mathbf{e}}{\|\mathbf{e}\|}\)</span></p>
</div>
</section>
</section>
<section id="differential-calculus-for-optimization" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="differential-calculus-for-optimization"><span class="header-section-number">15.9</span> Differential Calculus for Optimization</h2>
<p>Differential calculus provides the tools to compute gradients of objective functions, which are essential for optimization.</p>
<div id="def-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.27 (Gradient)</strong></span> The gradient of a scalar function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> with respect to vector <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span> is: <span class="math inline">\(\nabla f(\mathbf{x}) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right)\)</span></p>
<p>The gradient points in the direction of steepest increase of the function.</p>
</div>
<div id="def-jacobian-hessian" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.28 (Jacobian and Hessian)</strong></span> For a vector-valued function <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>, the Jacobian matrix <span class="math inline">\(J\)</span> contains all first-order partial derivatives: <span class="math inline">\(J_{ij} = \frac{\partial f_i}{\partial x_j}\)</span></p>
<p>For a scalar function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, the Hessian matrix <span class="math inline">\(H\)</span> contains all second-order partial derivatives: <span class="math inline">\(H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}\)</span></p>
<p>The Hessian characterizes the local curvature of the function.</p>
</div>
<div id="def-chain-rule" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15.29 (Chain Rule)</strong></span> The chain rule allows us to compute the derivative of composite functions: <span class="math inline">\(\frac{d}{dx} f(g(x)) = \frac{df}{dg} \cdot \frac{dg}{dx}\)</span></p>
<p>In higher dimensions, for <span class="math inline">\(\mathbf{y} = \mathbf{g}(\mathbf{x})\)</span> and <span class="math inline">\(z = f(\mathbf{y})\)</span>: <span class="math inline">\(\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}\)</span></p>
<p>The chain rule is fundamental to computing gradients in neural networks and complex knowledge graph embedding models.</p>
</div>
<div id="exm-gradient-computation" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.19 (Gradient Computation Example)</strong></span> For the distance-based scoring function in TransE: <span class="math inline">\(s(h, r, t) = -\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|_2\)</span></p>
<p>The gradient with respect to <span class="math inline">\(\mathbf{e}_h\)</span> is: <span class="math inline">\(\nabla_{\mathbf{e}_h} s(h, r, t) = -\frac{1}{\|\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t\|_2} \cdot (\mathbf{e}_h + \mathbf{r}_r - \mathbf{e}_t)\)</span></p>
<p>This gradient is used in the optimization process to update the entity embedding <span class="math inline">\(\mathbf{e}_h\)</span>.</p>
</div>
</section>
<section id="conclusion" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">15.10</span> Conclusion</h2>
<p>This appendix has provided a comprehensive overview of the mathematical foundations underlying knowledge graph embeddings. From the basics of linear algebra and vector spaces to more advanced topics like tensor algebra, complex vector spaces, probability theory, and optimization, these mathematical concepts form the building blocks for understanding, implementing, and developing knowledge graph embedding models.</p>
<p>By mastering these mathematical foundations, researchers and practitioners can better understand existing knowledge graph embedding techniques, develop new approaches, and effectively apply these methods to real-world problems. The formal definitions, theorems, and examples provided in this appendix serve as a reference that readers can consult as they work through the main chapters of the book.</p>
<p>The field of knowledge graph embeddings continues to evolve, with new mathematical tools and techniques being incorporated to address the challenges of modeling complex relationships in large-scale knowledge graphs. A solid understanding of these mathematical foundations will provide a strong basis for keeping up with and contributing to these developments.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/frontier.html" class="pagination-link" aria-label="Advanced Topics and Research Frontiers">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/appendix-b.html" class="pagination-link" aria-label="Appendix B: Resources and Tools">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>