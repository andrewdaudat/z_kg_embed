<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Introduction to Knowledge Graphs and Representations – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/translation-based.html" rel="next">
<link href="../content/outline.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/intro.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-are-graphs" id="toc-what-are-graphs" class="nav-link active" data-scroll-target="#what-are-graphs"><span class="header-section-number">2.1</span> What are graphs?</a>
  <ul class="collapse">
  <li><a href="#types-of-graphs" id="toc-types-of-graphs" class="nav-link" data-scroll-target="#types-of-graphs"><span class="header-section-number">2.1.1</span> Types of graphs</a></li>
  </ul></li>
  <li><a href="#knowledge-graphs-definition-and-components" id="toc-knowledge-graphs-definition-and-components" class="nav-link" data-scroll-target="#knowledge-graphs-definition-and-components"><span class="header-section-number">2.2</span> Knowledge graphs: definition and components</a>
  <ul class="collapse">
  <li><a href="#special-types-of-knowledge-graphs" id="toc-special-types-of-knowledge-graphs" class="nav-link" data-scroll-target="#special-types-of-knowledge-graphs"><span class="header-section-number">2.2.1</span> Special types of knowledge graphs</a></li>
  </ul></li>
  <li><a href="#real-world-knowledge-graphs" id="toc-real-world-knowledge-graphs" class="nav-link" data-scroll-target="#real-world-knowledge-graphs"><span class="header-section-number">2.3</span> Real-world knowledge graphs</a>
  <ul class="collapse">
  <li><a href="#public-knowledge-graphs" id="toc-public-knowledge-graphs" class="nav-link" data-scroll-target="#public-knowledge-graphs"><span class="header-section-number">2.3.1</span> Public knowledge graphs</a></li>
  <li><a href="#commercial-knowledge-graphs" id="toc-commercial-knowledge-graphs" class="nav-link" data-scroll-target="#commercial-knowledge-graphs"><span class="header-section-number">2.3.2</span> Commercial knowledge graphs</a></li>
  </ul></li>
  <li><a href="#applications-of-knowledge-graphs" id="toc-applications-of-knowledge-graphs" class="nav-link" data-scroll-target="#applications-of-knowledge-graphs"><span class="header-section-number">2.4</span> Applications of knowledge graphs</a>
  <ul class="collapse">
  <li><a href="#search-and-question-answering" id="toc-search-and-question-answering" class="nav-link" data-scroll-target="#search-and-question-answering"><span class="header-section-number">2.4.1</span> Search and question answering</a></li>
  <li><a href="#recommendation-systems" id="toc-recommendation-systems" class="nav-link" data-scroll-target="#recommendation-systems"><span class="header-section-number">2.4.2</span> Recommendation systems</a></li>
  <li><a href="#virtual-assistants-and-conversational-ai" id="toc-virtual-assistants-and-conversational-ai" class="nav-link" data-scroll-target="#virtual-assistants-and-conversational-ai"><span class="header-section-number">2.4.3</span> Virtual assistants and conversational AI</a></li>
  <li><a href="#data-integration" id="toc-data-integration" class="nav-link" data-scroll-target="#data-integration"><span class="header-section-number">2.4.4</span> Data integration</a></li>
  </ul></li>
  <li><a href="#the-challenge-of-incomplete-knowledge-graphs" id="toc-the-challenge-of-incomplete-knowledge-graphs" class="nav-link" data-scroll-target="#the-challenge-of-incomplete-knowledge-graphs"><span class="header-section-number">2.5</span> The challenge of incomplete knowledge graphs</a></li>
  <li><a href="#link-prediction-in-knowledge-graphs" id="toc-link-prediction-in-knowledge-graphs" class="nav-link" data-scroll-target="#link-prediction-in-knowledge-graphs"><span class="header-section-number">2.6</span> Link prediction in knowledge graphs</a></li>
  <li><a href="#approaches-to-knowledge-graph-completion" id="toc-approaches-to-knowledge-graph-completion" class="nav-link" data-scroll-target="#approaches-to-knowledge-graph-completion"><span class="header-section-number">2.7</span> Approaches to knowledge graph completion</a>
  <ul class="collapse">
  <li><a href="#rule-based-methods" id="toc-rule-based-methods" class="nav-link" data-scroll-target="#rule-based-methods"><span class="header-section-number">2.7.1</span> Rule-based methods</a></li>
  <li><a href="#graph-based-methods" id="toc-graph-based-methods" class="nav-link" data-scroll-target="#graph-based-methods"><span class="header-section-number">2.7.2</span> Graph-based methods</a></li>
  <li><a href="#embedding-based-methods" id="toc-embedding-based-methods" class="nav-link" data-scroll-target="#embedding-based-methods"><span class="header-section-number">2.7.3</span> Embedding-based methods</a></li>
  </ul></li>
  <li><a href="#representing-knowledge-symbolic-vs.-distributed-approaches" id="toc-representing-knowledge-symbolic-vs.-distributed-approaches" class="nav-link" data-scroll-target="#representing-knowledge-symbolic-vs.-distributed-approaches"><span class="header-section-number">2.8</span> Representing knowledge: symbolic vs.&nbsp;distributed approaches</a>
  <ul class="collapse">
  <li><a href="#symbolic-representations" id="toc-symbolic-representations" class="nav-link" data-scroll-target="#symbolic-representations"><span class="header-section-number">2.8.1</span> Symbolic representations</a></li>
  <li><a href="#distributed-representations" id="toc-distributed-representations" class="nav-link" data-scroll-target="#distributed-representations"><span class="header-section-number">2.8.2</span> Distributed representations</a></li>
  </ul></li>
  <li><a href="#embedding-space-properties" id="toc-embedding-space-properties" class="nav-link" data-scroll-target="#embedding-space-properties"><span class="header-section-number">2.9</span> Embedding space properties</a>
  <ul class="collapse">
  <li><a href="#dimensionality" id="toc-dimensionality" class="nav-link" data-scroll-target="#dimensionality"><span class="header-section-number">2.9.1</span> Dimensionality</a></li>
  <li><a href="#distance-and-similarity-measures" id="toc-distance-and-similarity-measures" class="nav-link" data-scroll-target="#distance-and-similarity-measures"><span class="header-section-number">2.9.2</span> Distance and similarity measures</a></li>
  <li><a href="#geometric-transformations" id="toc-geometric-transformations" class="nav-link" data-scroll-target="#geometric-transformations"><span class="header-section-number">2.9.3</span> Geometric transformations</a></li>
  </ul></li>
  <li><a href="#scoring-functions-for-link-prediction" id="toc-scoring-functions-for-link-prediction" class="nav-link" data-scroll-target="#scoring-functions-for-link-prediction"><span class="header-section-number">2.10</span> Scoring functions for link prediction</a></li>
  <li><a href="#the-link-prediction-task-formal-definition" id="toc-the-link-prediction-task-formal-definition" class="nav-link" data-scroll-target="#the-link-prediction-task-formal-definition"><span class="header-section-number">2.11</span> The link prediction task: formal definition</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.12</span> Summary</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">2.13</span> Further reading</a>
  <ul class="collapse">
  <li><a href="#books-and-surveys" id="toc-books-and-surveys" class="nav-link" data-scroll-target="#books-and-surveys"><span class="header-section-number">2.13.1</span> Books and surveys</a></li>
  <li><a href="#knowledge-graphs-and-applications" id="toc-knowledge-graphs-and-applications" class="nav-link" data-scroll-target="#knowledge-graphs-and-applications"><span class="header-section-number">2.13.2</span> Knowledge graphs and applications</a></li>
  <li><a href="#embedding-models-introductions" id="toc-embedding-models-introductions" class="nav-link" data-scroll-target="#embedding-models-introductions"><span class="header-section-number">2.13.3</span> Embedding models introductions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>Knowledge has always been one of humanity’s most valuable resources. From ancient libraries to modern databases, we have continually developed better ways to store, organize, and retrieve information. In our digital age, the volume of available information has grown exponentially, creating both opportunities and challenges. How can we structure this vast sea of data to make it useful? How can machines process and reason with this information in ways that approach human understanding?</p>
<p>Knowledge graphs represent one of the most promising solutions to these challenges. These structured representations organize information as a network of entities connected by relationships, providing a framework that is both human-interpretable and machine-processable. The concept has gained significant traction in recent years, with major technology companies like Google, Facebook, and Microsoft developing their own knowledge graph systems to power search engines, recommendation systems, and AI assistants.</p>
<p>This chapter introduces the fundamental concepts of knowledge graphs, their representation, and the challenges they present. We’ll explore why knowledge graph completion—particularly through embedding techniques—has become such an important area of research. By the end of this chapter, you’ll have a solid foundation for understanding the more advanced embedding models and techniques presented in later chapters.</p>
<section id="what-are-graphs" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="what-are-graphs"><span class="header-section-number">2.1</span> What are graphs?</h2>
<p>Before diving into knowledge graphs specifically, let’s establish a common understanding of graphs in general. Graphs provide a powerful mathematical abstraction for representing relationships between entities.</p>
<div id="def-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Graph)</strong></span> A <strong>graph</strong> <span class="math inline">G = (V, E)</span> consists of:</p>
<ul>
<li>A set of <strong>vertices</strong> (or <strong>nodes</strong>) <span class="math inline">V</span></li>
<li>A set of <strong>edges</strong> <span class="math inline">E \subseteq V \times V</span> that connect pairs of vertices</li>
</ul>
<p>An edge <span class="math inline">(u, v) \in E</span> indicates a relationship or connection between vertices <span class="math inline">u</span> and <span class="math inline">v</span>.</p>
</div>
<p>Graphs can be visualized as a collection of dots (vertices) connected by lines (edges). This intuitive visual representation makes graphs accessible even to those without a mathematical background.</p>
<div id="exm-social-network" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Social network graph)</strong></span> Consider a simple social network where people are represented as vertices and friendships as edges. If we have users Alice, Bob, Charlie, Diana, and Elena, with friendships between (Alice, Bob), (Bob, Charlie), (Charlie, Diana), (Diana, Elena), and (Elena, Alice), we can represent this as a graph:</p>
<ul>
<li>Vertices: <span class="math inline">V = \{\text{Alice}, \text{Bob}, \text{Charlie}, \text{Diana}, \text{Elena}\}</span></li>
<li>Edges: <span class="math inline">E = \{(\text{Alice}, \text{Bob}), (\text{Bob}, \text{Charlie}), (\text{Charlie}, \text{Diana}), (\text{Diana}, \text{Elena}), (\text{Elena}, \text{Alice})\}</span></li>
</ul>
<p>This particular graph forms a cycle, where each person is friends with exactly two others, creating a circular friendship network.</p>
</div>
<section id="types-of-graphs" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="types-of-graphs"><span class="header-section-number">2.1.1</span> Types of graphs</h3>
<p>Graphs come in various forms, each with properties suitable for different applications:</p>
<div id="def-undirected-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Undirected graph)</strong></span> An <strong>undirected graph</strong> is a graph where edges have no direction. If <span class="math inline">(u, v) \in E</span>, then <span class="math inline">(v, u)</span> is also in <span class="math inline">E</span>, representing a symmetric relationship between <span class="math inline">u</span> and <span class="math inline">v</span>.</p>
</div>
<div id="def-directed-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Directed graph)</strong></span> A <strong>directed graph</strong> (or <strong>digraph</strong>) is a graph where edges have direction. An edge <span class="math inline">(u, v)</span> represents a relationship from <span class="math inline">u</span> to <span class="math inline">v</span> that doesn’t necessarily apply in the reverse direction.</p>
</div>
<div id="def-weighted-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Weighted graph)</strong></span> A <strong>weighted graph</strong> is a graph where each edge has an associated numerical value called a <strong>weight</strong>. Formally, there is a weight function <span class="math inline">w: E \rightarrow \mathbb{R}</span> that assigns a real number to each edge.</p>
</div>
<div id="def-labeled-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Labeled graph)</strong></span> A <strong>labeled graph</strong> is a graph where vertices and/or edges have labels or types. Formally, there are labeling functions <span class="math inline">l_V: V \rightarrow L_V</span> and <span class="math inline">l_E: E \rightarrow L_E</span> that assign labels from sets <span class="math inline">L_V</span> and <span class="math inline">L_E</span> to vertices and edges, respectively.</p>
</div>
<div id="def-multigraph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Multigraph)</strong></span> A <strong>multigraph</strong> is a graph that allows multiple edges (or parallel edges) between the same pair of vertices.</p>
</div>
<div id="exm-graph-types" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Different graph types)</strong></span> Consider representing transportation connections between cities:</p>
<ul>
<li><strong>Undirected graph</strong>: Cities connected by highways, where travel is possible in both directions</li>
<li><strong>Directed graph</strong>: One-way streets connecting locations in a city</li>
<li><strong>Weighted graph</strong>: Cities connected by roads, with weights representing distances or travel times</li>
<li><strong>Labeled graph</strong>: A transportation network where edges are labeled by transportation type (bus, train, flight)</li>
<li><strong>Multigraph</strong>: Cities connected by multiple transportation options (e.g., both a highway and a railway between two cities)</li>
</ul>
</div>
</section>
</section>
<section id="knowledge-graphs-definition-and-components" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="knowledge-graphs-definition-and-components"><span class="header-section-number">2.2</span> Knowledge graphs: definition and components</h2>
<p>Now that we understand graphs in general, let’s focus on knowledge graphs specifically.</p>
<div id="def-knowledge-graph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 (Knowledge graph)</strong></span> A <strong>knowledge graph</strong> is a labeled, directed multigraph <span class="math inline">G = (E, R, T)</span> where:</p>
<ul>
<li><span class="math inline">E</span> is a set of entities (vertices)</li>
<li><span class="math inline">R</span> is a set of relation types (edge labels)</li>
<li><span class="math inline">T \subseteq E \times R \times E</span> is a set of triples or facts in the form <span class="math inline">(h, r, t)</span>, where <span class="math inline">h</span> is the head entity, <span class="math inline">r</span> is the relation, and <span class="math inline">t</span> is the tail entity</li>
</ul>
</div>
<p>Knowledge graphs are specifically designed to represent factual information about the world in a structured format. Each triple <span class="math inline">(h, r, t)</span> encodes a single fact, such as “Paris is the capital of France” or “Albert Einstein developed the theory of relativity.”</p>
<div id="exm-movie-kg" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Movie knowledge graph)</strong></span> Consider a small knowledge graph about movies:</p>
<ul>
<li>Entities: <span class="math inline">E = \{\text{Inception}, \text{Christopher Nolan}, \text{Leonardo DiCaprio}, \text{The Revenant}, \text{Alejandro Iñárritu}, \text{Science Fiction}, \text{Drama}, \text{2010}, \text{2015}\}</span></li>
<li>Relations: <span class="math inline">R = \{\text{directed}, \text{acted_in}, \text{has_genre}, \text{released_in}, \text{won_oscar_for}\}</span></li>
<li>Triples: <span class="math inline">T = \{</span>
<ul>
<li><span class="math inline">(\text{Christopher Nolan}, \text{directed}, \text{Inception})</span>,</li>
<li><span class="math inline">(\text{Leonardo DiCaprio}, \text{acted_in}, \text{Inception})</span>,</li>
<li><span class="math inline">(\text{Leonardo DiCaprio}, \text{acted_in}, \text{The Revenant})</span>,</li>
<li><span class="math inline">(\text{Alejandro Iñárritu}, \text{directed}, \text{The Revenant})</span>,</li>
<li><span class="math inline">(\text{Inception}, \text{has_genre}, \text{Science Fiction})</span>,</li>
<li><span class="math inline">(\text{The Revenant}, \text{has_genre}, \text{Drama})</span>,</li>
<li><span class="math inline">(\text{Inception}, \text{released_in}, \text{2010})</span>,</li>
<li><span class="math inline">(\text{The Revenant}, \text{released_in}, \text{2015})</span>,</li>
<li><span class="math inline">(\text{Leonardo DiCaprio}, \text{won_oscar_for}, \text{The Revenant})</span> <span class="math inline">\}</span></li>
</ul></li>
</ul>
</div>
<p>This example illustrates several key aspects of knowledge graphs:</p>
<ol type="1">
<li><strong>Heterogeneous entities</strong>: Entities can represent diverse concepts (people, movies, genres, years)</li>
<li><strong>Typed relations</strong>: Relations have specific meanings (directed, acted_in, has_genre)</li>
<li><strong>Multi-relational</strong>: The same entities can be connected through different relation types</li>
<li><strong>Factual knowledge</strong>: Each triple represents a specific fact about the world</li>
</ol>
<section id="special-types-of-knowledge-graphs" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="special-types-of-knowledge-graphs"><span class="header-section-number">2.2.1</span> Special types of knowledge graphs</h3>
<p>Some knowledge graphs have additional structure or constraints:</p>
<div id="def-ontology" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 (Ontology)</strong></span> An <strong>ontology</strong> is a knowledge graph that includes class hierarchies, property constraints, and logical axioms. It defines a formal vocabulary and semantic constraints for a domain.</p>
</div>
<div id="def-knowledge-base" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9 (Knowledge base)</strong></span> A <strong>knowledge base</strong> is a knowledge graph that is used for automated reasoning and may include inference rules and axioms in addition to factual triples.</p>
</div>
<p>Large-scale knowledge graphs often combine aspects of both ontologies (providing structure) and knowledge bases (supporting reasoning).</p>
</section>
</section>
<section id="real-world-knowledge-graphs" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="real-world-knowledge-graphs"><span class="header-section-number">2.3</span> Real-world knowledge graphs</h2>
<p>To appreciate the significance of knowledge graphs, it’s helpful to look at some prominent examples:</p>
<section id="public-knowledge-graphs" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="public-knowledge-graphs"><span class="header-section-number">2.3.1</span> Public knowledge graphs</h3>
<p>Several major knowledge graphs are publicly available for research and commercial applications:</p>
<ol type="1">
<li><p><strong>DBpedia</strong>: Extracts structured information from Wikipedia, containing information about persons, places, organizations, and more.</p></li>
<li><p><strong>Wikidata</strong>: A collaborative knowledge base maintained by the Wikimedia Foundation, containing structured data to support Wikipedia and other projects.</p></li>
<li><p><strong>YAGO (Yet Another Great Ontology)</strong>: Combines information from Wikipedia, WordNet, and GeoNames, with a focus on temporal and spatial knowledge.</p></li>
<li><p><strong>WordNet</strong>: A lexical database that groups English words into sets of synonyms (synsets) and records various semantic relations between these synonym sets.</p></li>
<li><p><strong>Freebase</strong>: A large collaborative knowledge base that was acquired by Google and partially integrated into Google’s Knowledge Graph before being shut down. Datasets derived from Freebase, such as FB15k, are still widely used in research.</p></li>
</ol>
</section>
<section id="commercial-knowledge-graphs" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="commercial-knowledge-graphs"><span class="header-section-number">2.3.2</span> Commercial knowledge graphs</h3>
<p>Major technology companies have developed proprietary knowledge graphs to power their products:</p>
<ol type="1">
<li><p><strong>Google Knowledge Graph</strong>: Powers Google’s search engine to provide direct answers to queries and enhances search results with relevant information.</p></li>
<li><p><strong>Facebook Graph API</strong>: Represents information about Facebook users, their connections, and activities.</p></li>
<li><p><strong>Microsoft Academic Graph</strong>: Contains scientific publication records, citations, authors, and fields of study.</p></li>
<li><p><strong>Amazon Product Graph</strong>: Contains information about products, their attributes, and relationships, supporting Amazon’s product recommendations.</p></li>
</ol>
<div id="exm-google-kg" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Google Knowledge Graph in action)</strong></span> When you search for “Albert Einstein” on Google, you receive not just links to websites but also a knowledge panel displaying key facts about Einstein: his birth date (March 14, 1879), death date (April 18, 1955), spouse (Elsa Einstein), children, notable awards (Nobel Prize in Physics), and much more. This information comes from Google’s Knowledge Graph, which has organized facts about Einstein and can present them directly to users.</p>
</div>
</section>
</section>
<section id="applications-of-knowledge-graphs" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="applications-of-knowledge-graphs"><span class="header-section-number">2.4</span> Applications of knowledge graphs</h2>
<p>Knowledge graphs enable numerous applications across various domains:</p>
<section id="search-and-question-answering" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="search-and-question-answering"><span class="header-section-number">2.4.1</span> Search and question answering</h3>
<p>Knowledge graphs significantly enhance search capabilities by understanding the meaning behind search queries and providing direct answers rather than just links.</p>
<div id="exm-semantic-search" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Semantic search with knowledge graphs)</strong></span> Consider the query “Who directed Inception?”</p>
<p>A traditional keyword-based search might return documents containing both “directed” and “Inception,” requiring the user to read through results to find the answer.</p>
<p>A knowledge graph-powered search can directly return “Christopher Nolan” by querying the triple <span class="math inline">(?, \text{directed}, \text{Inception})</span> and returning the matching head entity.</p>
</div>
</section>
<section id="recommendation-systems" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="recommendation-systems"><span class="header-section-number">2.4.2</span> Recommendation systems</h3>
<p>Knowledge graphs enable more sophisticated recommendation systems by capturing complex relationships between users and items.</p>
<div id="exm-movie-recommendations" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Knowledge graph-based movie recommendations)</strong></span> Instead of simply recommending movies liked by similar users, a knowledge graph-based system can use paths like:</p>
<p>User liked Inception → Inception directed_by Christopher Nolan → Christopher Nolan directed Interstellar → Recommend Interstellar</p>
<p>This allows for explainable recommendations: “We recommend Interstellar because you liked Inception, which was also directed by Christopher Nolan.”</p>
</div>
</section>
<section id="virtual-assistants-and-conversational-ai" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="virtual-assistants-and-conversational-ai"><span class="header-section-number">2.4.3</span> Virtual assistants and conversational AI</h3>
<p>Knowledge graphs provide the factual knowledge that enables virtual assistants to answer questions and maintain context in conversations.</p>
<div id="exm-conversational-context" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Conversational context with knowledge graphs)</strong></span> User: “Who wrote Romeo and Juliet?” Assistant: “William Shakespeare wrote Romeo and Juliet.” User: “When was he born?”</p>
<p>The knowledge graph helps the assistant understand that “he” refers to William Shakespeare and can retrieve his birth date (April 1564).</p>
</div>
</section>
<section id="data-integration" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="data-integration"><span class="header-section-number">2.4.4</span> Data integration</h3>
<p>Knowledge graphs serve as a unifying framework for integrating heterogeneous data sources, aligning different vocabularies and data models.</p>
<div id="exm-data-integration" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Enterprise data integration)</strong></span> A company might have customer data in a CRM system, product data in an inventory management system, and transaction data in a financial system. A knowledge graph can integrate these sources by mapping entities across systems (e.g., recognizing that Customer#1234 in the CRM system and User#5678 in the transaction system refer to the same person).</p>
</div>
</section>
</section>
<section id="the-challenge-of-incomplete-knowledge-graphs" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="the-challenge-of-incomplete-knowledge-graphs"><span class="header-section-number">2.5</span> The challenge of incomplete knowledge graphs</h2>
<p>Despite their utility, real-world knowledge graphs face a significant challenge: incompleteness. Building comprehensive knowledge graphs is difficult for several reasons:</p>
<ol type="1">
<li><p><strong>Scale</strong>: The real world contains an enormous number of entities and relationships. Even large knowledge graphs like Freebase or Wikidata capture only a fraction of all possible facts.</p></li>
<li><p><strong>Data acquisition</strong>: Extracting structured knowledge from unstructured sources (text, images, etc.) is challenging and error-prone.</p></li>
<li><p><strong>Evolving knowledge</strong>: The world constantly changes, with new entities emerging and relationships evolving over time.</p></li>
<li><p><strong>Long tail phenomena</strong>: Many entities and relationships occur rarely, making them difficult to collect systematically.</p></li>
</ol>
<div id="def-kg-completion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.10 (Knowledge graph completion)</strong></span> <strong>Knowledge graph completion</strong> is the task of adding missing triples to a knowledge graph. Given an incomplete knowledge graph <span class="math inline">G = (E, R, T)</span>, the goal is to discover additional valid triples <span class="math inline">T' \subseteq E \times R \times E</span> such that <span class="math inline">T' \not\subseteq T</span>.</p>
</div>
<p>Knowledge graph completion is typically approached as a prediction problem: given the existing triples in a knowledge graph, can we predict which additional triples are likely to be true?</p>
</section>
<section id="link-prediction-in-knowledge-graphs" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="link-prediction-in-knowledge-graphs"><span class="header-section-number">2.6</span> Link prediction in knowledge graphs</h2>
<p>The most common formulation of knowledge graph completion is link prediction:</p>
<div id="def-link-prediction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.11 (Link prediction)</strong></span> <strong>Link prediction</strong> in knowledge graphs is the task of predicting missing links (relationships) between entities. Formally, given a triple where either the head or tail entity is missing, <span class="math inline">(?, r, t)</span> or <span class="math inline">(h, r, ?)</span>, the goal is to rank all possible entities from the knowledge graph based on their likelihood of completing the triple correctly.</p>
</div>
<p>Link prediction can be viewed as answering questions like:</p>
<ul>
<li>“Which city is the capital of France?” — predicting <span class="math inline">(?, \text{capital_of}, \text{France})</span></li>
<li>“What genre does the movie Inception belong to?” — predicting <span class="math inline">(\text{Inception}, \text{has_genre}, ?)</span></li>
</ul>
<div id="exm-link-prediction" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Link prediction example)</strong></span> Consider our movie knowledge graph example with a missing triple: <span class="math inline">(\text{Leonardo DiCaprio}, \text{won_oscar_for}, ?)</span></p>
<p>A link prediction model would rank potential tail entities:</p>
<ol type="1">
<li>The Revenant (correct)</li>
<li>Inception (incorrect, but plausible)</li>
<li>Christopher Nolan (implausible) …</li>
</ol>
<p>The model’s quality is assessed by how highly it ranks the correct answer.</p>
</div>
</section>
<section id="approaches-to-knowledge-graph-completion" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="approaches-to-knowledge-graph-completion"><span class="header-section-number">2.7</span> Approaches to knowledge graph completion</h2>
<p>Researchers have developed various approaches to address the knowledge graph completion problem:</p>
<section id="rule-based-methods" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="rule-based-methods"><span class="header-section-number">2.7.1</span> Rule-based methods</h3>
<p>Rule-based methods use logical rules to infer new facts from existing ones. For example, the rule:</p>
<p><span class="math display">\text{born_in}(X, Y) \land \text{located_in}(Y, Z) \Rightarrow \text{nationality}(X, Z)</span></p>
<p>could infer that someone born in Paris has French nationality, based on the fact that Paris is located in France.</p>
<p>Rule-based approaches have the advantage of being interpretable but may struggle with noise and exceptions.</p>
</section>
<section id="graph-based-methods" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="graph-based-methods"><span class="header-section-number">2.7.2</span> Graph-based methods</h3>
<p>Graph-based methods leverage the graph structure to predict missing links. These approaches may use metrics like common neighbors, path features, or random walk statistics to assess the likelihood of links between entities.</p>
<div id="exm-path-features" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (Path features for link prediction)</strong></span> Consider predicting whether Leonardo DiCaprio has worked with Christopher Nolan. A path-based feature might be:</p>
<p><span class="math inline">(\text{Leonardo DiCaprio}, \text{acted_in}, \text{Inception}) \land (\text{Inception}, \text{directed_by}, \text{Christopher Nolan})</span></p>
<p>This path suggests a professional relationship between DiCaprio and Nolan.</p>
</div>
</section>
<section id="embedding-based-methods" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="embedding-based-methods"><span class="header-section-number">2.7.3</span> Embedding-based methods</h3>
<p>Embedding-based methods represent entities and relations as vectors in a continuous space, where the geometric relationships between vectors capture semantic relationships.</p>
<div id="def-knowledge-graph-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.12 (Knowledge graph embedding)</strong></span> <strong>Knowledge graph embedding (KGE)</strong> is the process of mapping entities and relations in a knowledge graph to continuous vector spaces while preserving the graph’s structural and semantic properties. Formally:</p>
<ul>
<li>Each entity <span class="math inline">e \in E</span> is mapped to a vector <span class="math inline">\mathbf{e} \in \mathbb{R}^d</span> (or in some models, <span class="math inline">\mathbf{e} \in \mathbb{C}^d</span>)</li>
<li>Each relation <span class="math inline">r \in R</span> is mapped to a vector, matrix, or tensor representation, depending on the specific model</li>
</ul>
</div>
<p>The embedding approach has several advantages:</p>
<ol type="1">
<li><strong>Computational efficiency</strong>: Vector operations are fast and parallelizable</li>
<li><strong>Generalization</strong>: Embeddings can capture patterns that enable inferences beyond observed facts</li>
<li><strong>Integration with machine learning</strong>: Embeddings can be used as input features for various machine learning tasks</li>
</ol>
<div id="exm-translation-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (Translational embedding intuition)</strong></span> In a translational embedding model like TransE, relationships are represented as translations in the vector space. For example, if we have:</p>
<p><span class="math inline">\mathbf{France} + \mathbf{capital\_of} \approx \mathbf{Paris}</span></p>
<p>Then we might also expect:</p>
<p><span class="math inline">\mathbf{Germany} + \mathbf{capital\_of} \approx \mathbf{Berlin}</span></p>
<p>This geometric structure enables the model to generalize from observed facts to new, similar facts.</p>
</div>
<p>Embedding-based methods have become the dominant approach for knowledge graph completion, and the remainder of this book focuses on various embedding techniques, their properties, and applications.</p>
</section>
</section>
<section id="representing-knowledge-symbolic-vs.-distributed-approaches" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="representing-knowledge-symbolic-vs.-distributed-approaches"><span class="header-section-number">2.8</span> Representing knowledge: symbolic vs.&nbsp;distributed approaches</h2>
<p>To better understand the significance of knowledge graph embeddings, it’s helpful to contrast symbolic and distributed representations of knowledge.</p>
<section id="symbolic-representations" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="symbolic-representations"><span class="header-section-number">2.8.1</span> Symbolic representations</h3>
<p>Symbolic representations store knowledge as discrete symbols and logical statements. Knowledge graphs in their raw form (as collections of triples) are symbolic representations.</p>
<p>Advantages of symbolic representations:</p>
<ul>
<li>Precise and interpretable</li>
<li>Support formal reasoning through logic</li>
<li>Accommodate structured queries</li>
</ul>
<p>Limitations of symbolic representations:</p>
<ul>
<li>Struggle with uncertainty and noise</li>
<li>Limited generalization capabilities</li>
<li>Computationally expensive for large-scale reasoning</li>
<li>Difficult to integrate with neural models</li>
</ul>
</section>
<section id="distributed-representations" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="distributed-representations"><span class="header-section-number">2.8.2</span> Distributed representations</h3>
<p>Distributed representations encode knowledge as patterns of activation across many units (e.g., vectors of real numbers). Knowledge graph embeddings are distributed representations.</p>
<p>Advantages of distributed representations:</p>
<ul>
<li>Robust to noise and uncertainty</li>
<li>Enable similarity-based generalization</li>
<li>Computationally efficient</li>
<li>Compatible with neural networks and deep learning</li>
</ul>
<p>Limitations of distributed representations:</p>
<ul>
<li>Less interpretable</li>
<li>May struggle with precise logical reasoning</li>
<li>Difficult to incorporate hard constraints</li>
</ul>
<div id="exm-representation-comparison" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 (Comparison of representations)</strong></span> Consider the fact “Paris is the capital of France”:</p>
<p><strong>Symbolic representation</strong>:</p>
<pre><code>(Paris, capital_of, France)</code></pre>
<p><strong>Distributed representation (vectors)</strong>:</p>
<pre><code>Paris = [0.1, -0.2, 0.5, 0.3, ...]
capital_of = [0.7, 0.1, -0.4, 0.2, ...]
France = [0.4, -0.1, 0.9, 0.5, ...]</code></pre>
<p>In the distributed representation, the relationship between entities might be captured by a mathematical operation like vector addition:</p>
<pre><code>Paris ≈ France + capital_of</code></pre>
</div>
<p>Knowledge graph embeddings aim to combine the best of both worlds: they start with symbolic knowledge in the form of triples but convert it to distributed representations that enable efficient computation and generalization.</p>
</section>
</section>
<section id="embedding-space-properties" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="embedding-space-properties"><span class="header-section-number">2.9</span> Embedding space properties</h2>
<p>When embedding knowledge graphs into vector spaces, certain properties of the embedding space become important:</p>
<section id="dimensionality" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="dimensionality"><span class="header-section-number">2.9.1</span> Dimensionality</h3>
<p>The dimension <span class="math inline">d</span> of the embedding space significantly impacts the model’s capacity and performance:</p>
<ul>
<li><strong>Low dimensionality</strong> (e.g., <span class="math inline">d = 50</span>) leads to more compact representations that may generalize better but might lose important details</li>
<li><strong>High dimensionality</strong> (e.g., <span class="math inline">d = 500</span>) allows for more expressive representations but may require more data to train effectively and could lead to overfitting</li>
</ul>
</section>
<section id="distance-and-similarity-measures" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="distance-and-similarity-measures"><span class="header-section-number">2.9.2</span> Distance and similarity measures</h3>
<p>Different embedding models use different notions of distance or similarity in the embedding space:</p>
<div id="def-distance-measures" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.13 (Common distance and similarity measures)</strong></span> For vectors <span class="math inline">\mathbf{x}, \mathbf{y} \in \mathbb{R}^d</span>:</p>
<ol type="1">
<li><p><strong>Euclidean distance</strong>: <span class="math inline">d_{\text{Euclidean}}(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}</span></p></li>
<li><p><strong>Manhattan distance</strong>: <span class="math inline">d_{\text{Manhattan}}(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_{i=1}^d |x_i - y_i|</span></p></li>
<li><p><strong>Cosine similarity</strong>: <span class="math inline">\text{sim}_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2} = \frac{\sum_{i=1}^d x_i y_i}{\sqrt{\sum_{i=1}^d x_i^2} \sqrt{\sum_{i=1}^d y_i^2}}</span></p></li>
<li><p><strong>Dot product</strong>: <span class="math inline">\text{sim}_{\text{dot}}(\mathbf{x}, \mathbf{y}) = \mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^d x_i y_i</span></p></li>
</ol>
</div>
<p>Different knowledge graph embedding models use different measures depending on how they conceptualize relationships in the embedding space.</p>
</section>
<section id="geometric-transformations" class="level3" data-number="2.9.3">
<h3 data-number="2.9.3" class="anchored" data-anchor-id="geometric-transformations"><span class="header-section-number">2.9.3</span> Geometric transformations</h3>
<p>Knowledge graph embedding models often interpret relations as geometric transformations in the embedding space:</p>
<ul>
<li><strong>Translations</strong>: Relation vectors shift entity vectors (e.g., in TransE)</li>
<li><strong>Rotations</strong>: Relation matrices rotate entity vectors (e.g., in RotatE)</li>
<li><strong>Projections</strong>: Relation matrices project entities onto subspaces (e.g., in TransH)</li>
<li><strong>Scaling</strong>: Relation vectors scale entity dimensions (e.g., in DistMult)</li>
</ul>
<p>The choice of transformation affects what types of relationship patterns the model can capture.</p>
</section>
</section>
<section id="scoring-functions-for-link-prediction" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="scoring-functions-for-link-prediction"><span class="header-section-number">2.10</span> Scoring functions for link prediction</h2>
<p>At the heart of embedding-based link prediction is the scoring function, which assesses how likely a triple is to be true.</p>
<div id="def-scoring-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.14 (Scoring function)</strong></span> A <strong>scoring function</strong> <span class="math inline">f_r(h, t)</span> in knowledge graph embedding models measures the plausibility of a triple <span class="math inline">(h, r, t)</span>. Higher scores (or lower distances/energies) indicate that a triple is more likely to be valid. The specific form of the scoring function varies across different embedding models.</p>
</div>
<p>Different models define different scoring functions. For example:</p>
<ul>
<li><strong>TransE</strong>: <span class="math inline">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</span> (distance-based, lower is better)</li>
<li><strong>DistMult</strong>: <span class="math inline">f_r(h, t) = \mathbf{h}^\top \text{diag}(\mathbf{r}) \mathbf{t}</span> (similarity-based, higher is better)</li>
</ul>
<p>The scoring function is crucial because it defines the geometry of the embedding space and determines what patterns the model can learn.</p>
</section>
<section id="the-link-prediction-task-formal-definition" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="the-link-prediction-task-formal-definition"><span class="header-section-number">2.11</span> The link prediction task: formal definition</h2>
<p>Now that we understand the components of knowledge graph embeddings, let’s formally define the link prediction task:</p>
<div id="def-link-prediction-formal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.15 (Formal link prediction task)</strong></span> Given:</p>
<ul>
<li>A knowledge graph <span class="math inline">G = (E, R, T)</span></li>
<li>A query in the form <span class="math inline">(h, r, ?)</span> or <span class="math inline">(?, r, t)</span>, where <span class="math inline">?</span> indicates a missing entity</li>
</ul>
<p>The task is to rank all entities <span class="math inline">e \in E</span> by the score <span class="math inline">f_r(h, e)</span> or <span class="math inline">f_r(e, t)</span>, respectively, where <span class="math inline">f_r</span> is a scoring function.</p>
<p>Performance is evaluated by the rank of the correct entity among all possible entities.</p>
</div>
<p>In practice, link prediction involves these steps:</p>
<ol type="1">
<li>Learn entity and relation embeddings from observed triples</li>
<li>For each test query, compute scores for all candidate entities</li>
<li>Rank candidates by score</li>
<li>Evaluate using metrics like Mean Rank, Mean Reciprocal Rank, or Hits@k</li>
</ol>
<div id="exm-link-prediction-ranking" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (Link prediction ranking example)</strong></span> Consider a query <span class="math inline">(?, \text{capital_of}, \text{France})</span> to find the capital of France.</p>
<p>A model might produce these scores for candidate entities:</p>
<ul>
<li>Paris: 0.95</li>
<li>Lyon: 0.65</li>
<li>Berlin: 0.30</li>
<li>London: 0.25</li>
<li>Rome: 0.20</li>
</ul>
<p>The correct answer (Paris) is ranked first, resulting in a perfect prediction in this case.</p>
</div>
</section>
<section id="summary" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.12</span> Summary</h2>
<p>In this chapter, we’ve established the foundations for understanding knowledge graphs and embedding approaches to link prediction:</p>
<ul>
<li>Knowledge graphs represent factual information as triples of the form (head entity, relation, tail entity)</li>
<li>Real-world knowledge graphs like Freebase, DBpedia, and Google’s Knowledge Graph power applications from search to recommendation systems</li>
<li>Knowledge graphs are inevitably incomplete, creating a need for techniques to predict missing links</li>
<li>Knowledge graph embeddings map entities and relations to continuous vector spaces, enabling efficient computation and generalization</li>
<li>Link prediction, the task of predicting missing entities in triples, is the primary application of knowledge graph embeddings</li>
</ul>
<p>The next chapter will delve into the fundamentals of vector space representations, which form the mathematical foundation for all knowledge graph embedding approaches.</p>
</section>
<section id="further-reading" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">2.13</span> Further reading</h2>
<section id="books-and-surveys" class="level3" data-number="2.13.1">
<h3 data-number="2.13.1" class="anchored" data-anchor-id="books-and-surveys"><span class="header-section-number">2.13.1</span> Books and surveys</h3>
<ul>
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
<li>Hogan, A., Blomqvist, E., Cochez, M., d’Amato, C., Melo, G. D., Gutierrez, C., … &amp; Zimmermann, A. (2021). Knowledge graphs. ACM Computing Surveys, 54(4), 1-37.</li>
<li>Getoor, L., &amp; Taskar, B. (Eds.). (2007). Introduction to statistical relational learning. MIT press.</li>
</ul>
</section>
<section id="knowledge-graphs-and-applications" class="level3" data-number="2.13.2">
<h3 data-number="2.13.2" class="anchored" data-anchor-id="knowledge-graphs-and-applications"><span class="header-section-number">2.13.2</span> Knowledge graphs and applications</h3>
<ul>
<li>Singhal, A. (2012). Introducing the Knowledge Graph: things, not strings. Google Official Blog.</li>
<li>Nickel, M., Murphy, K., Tresp, V., &amp; Gabrilovich, E. (2016). A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1), 11-33.</li>
<li>Paulheim, H. (2017). Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic Web, 8(3), 489-508.</li>
</ul>
</section>
<section id="embedding-models-introductions" class="level3" data-number="2.13.3">
<h3 data-number="2.13.3" class="anchored" data-anchor-id="embedding-models-introductions"><span class="header-section-number">2.13.3</span> Embedding models introductions</h3>
<ul>
<li>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (pp.&nbsp;2787-2795).</li>
<li>Yang, B., Yih, W. T., He, X., Gao, J., &amp; Deng, L. (2015). Embedding entities and relations for learning and inference in knowledge bases. In International Conference on Learning Representations.</li>
<li>Trouillon, T., Welbl, J., Riedel, S., Gaussier, É., &amp; Bouchard, G. (2016). Complex embeddings for simple link prediction. In International Conference on Machine Learning (pp.&nbsp;2071-2080).</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/outline.html" class="pagination-link" aria-label="Outline">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Outline</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/translation-based.html" class="pagination-link" aria-label="Translation-Based Embedding Models">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>