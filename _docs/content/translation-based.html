<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Translation-Based Embedding Models – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-4379b0ccadffce622b03caf4c46266b3.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-be379db77a6992e36199e6cba1e88714.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-4104e206323135730aa08c3113d84ebc.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/translation-based.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-translation-principle" id="toc-the-translation-principle" class="nav-link active" data-scroll-target="#the-translation-principle"><span class="header-section-number">3.1</span> The translation principle</a></li>
  <li><a href="#transe-the-pioneering-translation-model" id="toc-transe-the-pioneering-translation-model" class="nav-link" data-scroll-target="#transe-the-pioneering-translation-model"><span class="header-section-number">3.2</span> TransE: the pioneering translation model</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation"><span class="header-section-number">3.2.1</span> Geometric interpretation</a></li>
  <li><a href="#learning-algorithm" id="toc-learning-algorithm" class="nav-link" data-scroll-target="#learning-algorithm"><span class="header-section-number">3.2.2</span> Learning algorithm</a></li>
  <li><a href="#transe-algorithm" id="toc-transe-algorithm" class="nav-link" data-scroll-target="#transe-algorithm"><span class="header-section-number">3.2.3</span> TransE algorithm</a></li>
  <li><a href="#strengths-and-limitations-of-transe" id="toc-strengths-and-limitations-of-transe" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transe"><span class="header-section-number">3.2.4</span> Strengths and limitations of TransE</a></li>
  </ul></li>
  <li><a href="#transh-modeling-relations-as-hyperplanes" id="toc-transh-modeling-relations-as-hyperplanes" class="nav-link" data-scroll-target="#transh-modeling-relations-as-hyperplanes"><span class="header-section-number">3.3</span> TransH: modeling relations as hyperplanes</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-1" id="toc-geometric-interpretation-1" class="nav-link" data-scroll-target="#geometric-interpretation-1"><span class="header-section-number">3.3.1</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-transh" id="toc-strengths-and-limitations-of-transh" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transh"><span class="header-section-number">3.3.2</span> Strengths and limitations of TransH</a></li>
  </ul></li>
  <li><a href="#transr-modeling-relations-in-relation-specific-spaces" id="toc-transr-modeling-relations-in-relation-specific-spaces" class="nav-link" data-scroll-target="#transr-modeling-relations-in-relation-specific-spaces"><span class="header-section-number">3.4</span> TransR: modeling relations in relation-specific spaces</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-2" id="toc-geometric-interpretation-2" class="nav-link" data-scroll-target="#geometric-interpretation-2"><span class="header-section-number">3.4.1</span> Geometric interpretation</a></li>
  <li><a href="#cluster-based-transr-ctransr" id="toc-cluster-based-transr-ctransr" class="nav-link" data-scroll-target="#cluster-based-transr-ctransr"><span class="header-section-number">3.4.2</span> Cluster-based TransR (CTransR)</a></li>
  <li><a href="#strengths-and-limitations-of-transr" id="toc-strengths-and-limitations-of-transr" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transr"><span class="header-section-number">3.4.3</span> Strengths and limitations of TransR</a></li>
  </ul></li>
  <li><a href="#transd-dynamic-mapping-matrices" id="toc-transd-dynamic-mapping-matrices" class="nav-link" data-scroll-target="#transd-dynamic-mapping-matrices"><span class="header-section-number">3.5</span> TransD: dynamic mapping matrices</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-3" id="toc-geometric-interpretation-3" class="nav-link" data-scroll-target="#geometric-interpretation-3"><span class="header-section-number">3.5.1</span> Geometric interpretation</a></li>
  <li><a href="#strengths-and-limitations-of-transd" id="toc-strengths-and-limitations-of-transd" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transd"><span class="header-section-number">3.5.2</span> Strengths and limitations of TransD</a></li>
  </ul></li>
  <li><a href="#transparse-adaptive-sparse-matrices" id="toc-transparse-adaptive-sparse-matrices" class="nav-link" data-scroll-target="#transparse-adaptive-sparse-matrices"><span class="header-section-number">3.6</span> TranSparse: adaptive sparse matrices</a>
  <ul class="collapse">
  <li><a href="#adaptive-sparsity" id="toc-adaptive-sparsity" class="nav-link" data-scroll-target="#adaptive-sparsity"><span class="header-section-number">3.6.1</span> Adaptive sparsity</a></li>
  <li><a href="#strengths-and-limitations-of-transparse" id="toc-strengths-and-limitations-of-transparse" class="nav-link" data-scroll-target="#strengths-and-limitations-of-transparse"><span class="header-section-number">3.6.2</span> Strengths and limitations of TranSparse</a></li>
  </ul></li>
  <li><a href="#rotate-rotation-in-complex-space" id="toc-rotate-rotation-in-complex-space" class="nav-link" data-scroll-target="#rotate-rotation-in-complex-space"><span class="header-section-number">3.7</span> RotatE: rotation in complex space</a>
  <ul class="collapse">
  <li><a href="#geometric-interpretation-4" id="toc-geometric-interpretation-4" class="nav-link" data-scroll-target="#geometric-interpretation-4"><span class="header-section-number">3.7.1</span> Geometric interpretation</a></li>
  <li><a href="#modeling-relation-patterns-with-rotate" id="toc-modeling-relation-patterns-with-rotate" class="nav-link" data-scroll-target="#modeling-relation-patterns-with-rotate"><span class="header-section-number">3.7.2</span> Modeling relation patterns with RotatE</a></li>
  <li><a href="#self-adversarial-negative-sampling" id="toc-self-adversarial-negative-sampling" class="nav-link" data-scroll-target="#self-adversarial-negative-sampling"><span class="header-section-number">3.7.3</span> Self-adversarial negative sampling</a></li>
  <li><a href="#strengths-and-limitations-of-rotate" id="toc-strengths-and-limitations-of-rotate" class="nav-link" data-scroll-target="#strengths-and-limitations-of-rotate"><span class="header-section-number">3.7.4</span> Strengths and limitations of RotatE</a></li>
  </ul></li>
  <li><a href="#comparing-translation-based-models" id="toc-comparing-translation-based-models" class="nav-link" data-scroll-target="#comparing-translation-based-models"><span class="header-section-number">3.8</span> Comparing translation-based models</a></li>
  <li><a href="#relation-patterns-and-model-capabilities" id="toc-relation-patterns-and-model-capabilities" class="nav-link" data-scroll-target="#relation-patterns-and-model-capabilities"><span class="header-section-number">3.9</span> Relation patterns and model capabilities</a></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations"><span class="header-section-number">3.10</span> Implementation considerations</a>
  <ul class="collapse">
  <li><a href="#normalization-constraints" id="toc-normalization-constraints" class="nav-link" data-scroll-target="#normalization-constraints"><span class="header-section-number">3.10.1</span> Normalization constraints</a></li>
  <li><a href="#initialization-strategies" id="toc-initialization-strategies" class="nav-link" data-scroll-target="#initialization-strategies"><span class="header-section-number">3.10.2</span> Initialization strategies</a></li>
  <li><a href="#negative-sampling-strategies" id="toc-negative-sampling-strategies" class="nav-link" data-scroll-target="#negative-sampling-strategies"><span class="header-section-number">3.10.3</span> Negative sampling strategies</a></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">3.10.4</span> Optimization algorithms</a></li>
  </ul></li>
  <li><a href="#performance-analysis" id="toc-performance-analysis" class="nav-link" data-scroll-target="#performance-analysis"><span class="header-section-number">3.11</span> Performance analysis</a>
  <ul class="collapse">
  <li><a href="#benchmark-datasets" id="toc-benchmark-datasets" class="nav-link" data-scroll-target="#benchmark-datasets"><span class="header-section-number">3.11.1</span> Benchmark datasets</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics"><span class="header-section-number">3.11.2</span> Evaluation metrics</a></li>
  <li><a href="#comparative-results" id="toc-comparative-results" class="nav-link" data-scroll-target="#comparative-results"><span class="header-section-number">3.11.3</span> Comparative results</a></li>
  </ul></li>
  <li><a href="#advanced-topics" id="toc-advanced-topics" class="nav-link" data-scroll-target="#advanced-topics"><span class="header-section-number">3.12</span> Advanced topics</a>
  <ul class="collapse">
  <li><a href="#entity-type-constraints" id="toc-entity-type-constraints" class="nav-link" data-scroll-target="#entity-type-constraints"><span class="header-section-number">3.12.1</span> Entity type constraints</a></li>
  <li><a href="#temporal-knowledge-graphs" id="toc-temporal-knowledge-graphs" class="nav-link" data-scroll-target="#temporal-knowledge-graphs"><span class="header-section-number">3.12.2</span> Temporal knowledge graphs</a></li>
  <li><a href="#multi-modal-knowledge-graphs" id="toc-multi-modal-knowledge-graphs" class="nav-link" data-scroll-target="#multi-modal-knowledge-graphs"><span class="header-section-number">3.12.3</span> Multi-modal knowledge graphs</a></li>
  <li><a href="#uncertainty-modeling" id="toc-uncertainty-modeling" class="nav-link" data-scroll-target="#uncertainty-modeling"><span class="header-section-number">3.12.4</span> Uncertainty modeling</a></li>
  </ul></li>
  <li><a href="#applications-of-translation-based-models" id="toc-applications-of-translation-based-models" class="nav-link" data-scroll-target="#applications-of-translation-based-models"><span class="header-section-number">3.13</span> Applications of translation-based models</a>
  <ul class="collapse">
  <li><a href="#recommendation-systems" id="toc-recommendation-systems" class="nav-link" data-scroll-target="#recommendation-systems"><span class="header-section-number">3.13.1</span> Recommendation systems</a></li>
  <li><a href="#question-answering" id="toc-question-answering" class="nav-link" data-scroll-target="#question-answering"><span class="header-section-number">3.13.2</span> Question answering</a></li>
  <li><a href="#information-extraction" id="toc-information-extraction" class="nav-link" data-scroll-target="#information-extraction"><span class="header-section-number">3.13.3</span> Information extraction</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions"><span class="header-section-number">3.14</span> Future directions</a>
  <ul class="collapse">
  <li><a href="#inductive-learning" id="toc-inductive-learning" class="nav-link" data-scroll-target="#inductive-learning"><span class="header-section-number">3.14.1</span> Inductive learning</a></li>
  <li><a href="#neural-architecture-integration" id="toc-neural-architecture-integration" class="nav-link" data-scroll-target="#neural-architecture-integration"><span class="header-section-number">3.14.2</span> Neural architecture integration</a></li>
  <li><a href="#explainable-embeddings" id="toc-explainable-embeddings" class="nav-link" data-scroll-target="#explainable-embeddings"><span class="header-section-number">3.14.3</span> Explainable embeddings</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.15</span> Summary</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">3.16</span> Further reading</a>
  <ul class="collapse">
  <li><a href="#original-papers" id="toc-original-papers" class="nav-link" data-scroll-target="#original-papers"><span class="header-section-number">3.16.1</span> Original papers</a></li>
  <li><a href="#surveys-and-comparative-analyses" id="toc-surveys-and-comparative-analyses" class="nav-link" data-scroll-target="#surveys-and-comparative-analyses"><span class="header-section-number">3.16.2</span> Surveys and comparative analyses</a></li>
  <li><a href="#applications-and-extensions" id="toc-applications-and-extensions" class="nav-link" data-scroll-target="#applications-and-extensions"><span class="header-section-number">3.16.3</span> Applications and extensions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>Translation-based models represent one of the most intuitive and influential approaches to knowledge graph embeddings. These models interpret relations as translations in the embedding space: if a relation connects a head entity to a tail entity, then adding the relation vector to the head entity vector should approximate the tail entity vector. This simple geometric interpretation has proven remarkably effective for capturing many types of relationships in knowledge graphs.</p>
<p>This chapter explores the family of translation-based models, starting with the pioneering TransE model and progressing through various extensions that address its limitations. We’ll examine how these models work, their geometric interpretations, their strengths and limitations, and how they perform on knowledge graph completion tasks. By understanding translation-based models, you’ll gain insight into the core principles that underlie many knowledge graph embedding approaches.</p>
<section id="the-translation-principle" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-translation-principle"><span class="header-section-number">3.1</span> The translation principle</h2>
<p>The fundamental idea behind translation-based models is to interpret relations as translations (displacement vectors) in the embedding space:</p>
<div id="def-translation-principle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Translation principle)</strong></span> In translation-based models, entities and relations are embedded in a shared vector space <span class="math inline">\mathbb{R}^d</span>, where relations are interpreted as translations from head entities to tail entities. For a triple <span class="math inline">(h, r, t)</span>, the model aims to enforce:</p>
<p><span class="math display">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</span></p>
<p>where <span class="math inline">\mathbf{h}, \mathbf{r}, \mathbf{t} \in \mathbb{R}^d</span> are the vector embeddings of the head entity, relation, and tail entity, respectively.</p>
</div>
<p>This principle draws inspiration from word embeddings, where semantic relationships are often captured as consistent vector offsets. For example, in word embeddings, we might observe that <span class="math inline">\mathbf{king} - \mathbf{man} + \mathbf{woman} \approx \mathbf{queen}</span>, indicating that the gender relationship is captured as a consistent offset.</p>
<p>Similarly, in a knowledge graph about geography, we might expect: <span class="math inline">\mathbf{France} + \mathbf{has\_capital} \approx \mathbf{Paris}</span> <span class="math inline">\mathbf{Germany} + \mathbf{has\_capital} \approx \mathbf{Berlin}</span> <span class="math inline">\mathbf{Italy} + \mathbf{has\_capital} \approx \mathbf{Rome}</span></p>
<p>This implies that the “has_capital” relation is represented as a consistent translation vector that, when added to a country’s embedding, approximates the embedding of its capital city.</p>
<div id="exm-translation-intuition" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Translation intuition)</strong></span> Consider a simple knowledge graph about family relationships:</p>
<ul>
<li>(John, is_father_of, Mary)</li>
<li>(Bob, is_father_of, Alice)</li>
<li>(Tom, is_father_of, James)</li>
</ul>
<p>In a translation-based model, the “is*father_of” relation would be represented as a vector <span class="math inline">\mathbf{r}*{\text{is_father_of}}</span> such that:</p>
<ul>
<li><span class="math inline">\mathbf{John} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{Mary}</span></li>
<li><span class="math inline">\mathbf{Bob} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{Alice}</span></li>
<li><span class="math inline">\mathbf{Tom} + \mathbf{r}_{\text{is\_father\_of}} \approx \mathbf{James}</span></li>
</ul>
<p>This means that the father-child relationship is captured as a consistent displacement in the embedding space, allowing the model to generalize to new entity pairs.</p>
</div>
</section>
<section id="transe-the-pioneering-translation-model" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="transe-the-pioneering-translation-model"><span class="header-section-number">3.2</span> TransE: the pioneering translation model</h2>
<p>TransE, introduced by Bordes et al.&nbsp;(2013), was the first translation-based model for knowledge graph embeddings and remains one of the most influential models in the field.</p>
<div id="def-transe" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (TransE model)</strong></span> In the <strong>TransE</strong> model, entities and relations are embedded in the same vector space <span class="math inline">\mathbb{R}^d</span>. For a triple <span class="math inline">(h, r, t)</span>, the model enforces: <span class="math display">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</span></p>
<p>The scoring function, which measures the plausibility of a triple, is defined as: <span class="math display">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</span> where <span class="math inline">\|\cdot\|</span> can be either the L1 or L2 norm.</p>
<p>Lower scores (smaller distances) indicate more plausible triples.</p>
</div>
<section id="geometric-interpretation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="geometric-interpretation"><span class="header-section-number">3.2.1</span> Geometric interpretation</h3>
<p>TransE has a clear geometric interpretation: relations are translations in the embedding space. For a valid triple <span class="math inline">(h, r, t)</span>, the tail entity <span class="math inline">t</span> should be close to the point reached by starting at head entity <span class="math inline">h</span> and moving along the relation vector <span class="math inline">\mathbf{r}</span>.</p>
<div id="exm-transe-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (TransE geometric example)</strong></span> Consider a knowledge graph about countries and their capitals. In a well-trained TransE model, the embeddings might have the following properties:</p>
<ul>
<li><span class="math inline">\mathbf{France} + \mathbf{has\_capital} \approx \mathbf{Paris}</span></li>
<li><span class="math inline">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| \approx 0</span> (low distance, high plausibility)</li>
<li><span class="math inline">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\| \gg 0</span> (high distance, low plausibility)</li>
</ul>
<p>For a query about the capital of France, TransE would rank all entities by their proximity to <span class="math inline">\mathbf{France} + \mathbf{has\_capital}</span>. Paris would be ranked highly (ideally first) because it’s close to this point.</p>
</div>
</section>
<section id="learning-algorithm" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="learning-algorithm"><span class="header-section-number">3.2.2</span> Learning algorithm</h3>
<p>TransE is trained using a margin-based ranking loss:</p>
<div id="def-transe-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (TransE loss function)</strong></span> The TransE model is trained by minimizing the following margin-based ranking loss:</p>
<p><span class="math display">L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma + f_r(h, t) - f_r(h', t')]_+</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">S</span> is the set of valid triples in the training set</li>
<li><span class="math inline">S'_{(h,r,t)}</span> is the set of corrupted triples created by replacing either <span class="math inline">h</span> or <span class="math inline">t</span> with a random entity</li>
<li><span class="math inline">\gamma &gt; 0</span> is a margin hyperparameter</li>
<li><span class="math inline">[x]_+ = \max(0, x)</span> denotes the positive part of <span class="math inline">x</span></li>
<li><span class="math inline">f_r(h, t) = -\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|</span> is the scoring function</li>
</ul>
</div>
<p>This loss function encourages valid triples to have lower scores (smaller distances) than corrupted ones by at least a margin of <span class="math inline">\gamma</span>.</p>
<div id="exm-transe-learning" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (TransE learning example)</strong></span> Consider a valid triple (France, has_capital, Paris) and a corrupted triple (France, has_capital, London).</p>
<p>The loss term for this pair would be: <span class="math inline">[\gamma - \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| + \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\|]_+</span></p>
<p>For effective training, we want: <span class="math inline">\|\mathbf{France} + \mathbf{has\_capital} - \mathbf{Paris}\| &lt; \|\mathbf{France} + \mathbf{has\_capital} - \mathbf{London}\| - \gamma</span></p>
<p>This means the distance for the valid triple should be smaller than the distance for the corrupted triple by at least the margin <span class="math inline">\gamma</span>.</p>
</div>
</section>
<section id="transe-algorithm" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="transe-algorithm"><span class="header-section-number">3.2.3</span> TransE algorithm</h3>
<p>Here’s the algorithm for training a TransE model:</p>
<div id="def-transe-algorithm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 (TransE training algorithm)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Initialize</strong> entity and relation embeddings randomly</li>
<li><strong>Normalize</strong> entity embeddings to have unit L2 norm: <span class="math inline">\|\mathbf{e}\|_2 = 1</span> for all entities <span class="math inline">e</span></li>
<li><strong>For</strong> each training iteration: a. Sample a mini-batch of valid triples <span class="math inline">B \subset S</span> b. For each triple <span class="math inline">(h, r, t) \in B</span>, create a corrupted triple <span class="math inline">(h', r, t')</span> by replacing either <span class="math inline">h</span> or <span class="math inline">t</span> c.&nbsp;Compute the loss function for the mini-batch d.&nbsp;Update embeddings using stochastic gradient descent e. Re-normalize entity embeddings</li>
<li><strong>Return</strong> the learned entity and relation embeddings</li>
</ol>
</div>
<p>The normalization of entity embeddings is crucial for TransE. Without this constraint, the model could “cheat” by making the norms of entities very large while keeping relation norms small, satisfying the translation constraint without learning meaningful representations.</p>
</section>
<section id="strengths-and-limitations-of-transe" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="strengths-and-limitations-of-transe"><span class="header-section-number">3.2.4</span> Strengths and limitations of TransE</h3>
<p>TransE has several strengths that contributed to its popularity:</p>
<ol type="1">
<li><strong>Simplicity</strong>: The model is conceptually simple and easy to implement</li>
<li><strong>Efficiency</strong>: With only <span class="math inline">O(|E|d + |R|d)</span> parameters, TransE is computationally efficient</li>
<li><strong>Effectiveness</strong>: Despite its simplicity, TransE performs well on many knowledge graph completion tasks</li>
</ol>
<p>However, TransE also has important limitations:</p>
<ol type="1">
<li><p><strong>One-to-many, many-to-one, and many-to-many relations</strong>: TransE struggles with these relation types because it enforces a single translation vector for each relation</p></li>
<li><p><strong>Symmetric relations</strong>: TransE cannot model symmetric relations well because if <span class="math inline">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</span> and <span class="math inline">\mathbf{t} + \mathbf{r} \approx \mathbf{h}</span> (for a symmetric relation), then <span class="math inline">\mathbf{r} \approx \mathbf{0}</span></p></li>
<li><p><strong>Reflexive relations</strong>: Relations where an entity relates to itself (e.g., “is_similar_to”) are challenging because they would require <span class="math inline">\mathbf{r} \approx \mathbf{0}</span></p></li>
</ol>
<div id="exm-transe-limitations" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (TransE limitations example)</strong></span> Consider these triples with one-to-many relations:</p>
<ul>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
<li>(Barack_Obama, has_child, Sasha_Obama)</li>
</ul>
<p>TransE would try to enforce:</p>
<ul>
<li><span class="math inline">\mathbf{Barack\_Obama} + \mathbf{has\_child} \approx \mathbf{Malia\_Obama}</span></li>
<li><span class="math inline">\mathbf{Barack\_Obama} + \mathbf{has\_child} \approx \mathbf{Sasha\_Obama}</span></li>
</ul>
<p>This implies <span class="math inline">\mathbf{Malia\_Obama} \approx \mathbf{Sasha\_Obama}</span>, which is incorrect. The model cannot place the two children at different positions while maintaining the translation property for a single relation vector.</p>
</div>
<p>These limitations motivated researchers to develop extensions to the TransE model, which we’ll explore next.</p>
</section>
</section>
<section id="transh-modeling-relations-as-hyperplanes" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="transh-modeling-relations-as-hyperplanes"><span class="header-section-number">3.3</span> TransH: modeling relations as hyperplanes</h2>
<p>TransH (Wang et al., 2014) addresses the limitations of TransE by modeling each relation as a hyperplane, allowing entities to have relation-specific representations.</p>
<div id="def-transh" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.5 (TransH model)</strong></span> In the <strong>TransH</strong> model, each relation <span class="math inline">r</span> is associated with:</p>
<ol type="1">
<li>A normal vector <span class="math inline">\mathbf{w}_r</span> defining a hyperplane</li>
<li>A translation vector <span class="math inline">\mathbf{d}_r</span> on the hyperplane</li>
</ol>
<p>For a triple <span class="math inline">(h, r, t)</span>, the model:</p>
<ol type="1">
<li>Projects the head and tail entities onto the relation-specific hyperplane:
<ul>
<li><span class="math inline">\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^T \mathbf{h} \mathbf{w}_r</span></li>
<li><span class="math inline">\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^T \mathbf{t} \mathbf{w}_r</span></li>
</ul></li>
<li>Applies the translation on the hyperplane:
<ul>
<li><span class="math inline">\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}</span></li>
</ul></li>
</ol>
<p>The scoring function is: <span class="math display">f_r(h, t) = -\|\mathbf{h}_{\perp} + \mathbf{d}_r - \mathbf{t}_{\perp}\|_2^2</span></p>
</div>
<section id="geometric-interpretation-1" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="geometric-interpretation-1"><span class="header-section-number">3.3.1</span> Geometric interpretation</h3>
<p>In TransH, each relation corresponds to a different hyperplane in the embedding space. Entities are projected onto these hyperplanes before the translation is applied. This allows the same entity to have different representations when involved in different relations.</p>
<div id="exm-transh-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 (TransH geometric example)</strong></span> Consider triples involving a one-to-many relation:</p>
<ul>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
<li>(Barack_Obama, has_child, Sasha_Obama)</li>
</ul>
<p>In TransH, Barack_Obama would be projected onto the “has_child” hyperplane, resulting in a specific representation for the “has_child” relation. Then:</p>
<ul>
<li><span class="math inline">\mathbf{Barack\_Obama}_{\perp} + \mathbf{d}_{\text{has\_child}} \approx \mathbf{Malia\_Obama}_{\perp}</span></li>
<li><span class="math inline">\mathbf{Barack\_Obama}_{\perp} + \mathbf{d}_{\text{has\_child}} \approx \mathbf{Sasha\_Obama}_{\perp}</span></li>
</ul>
<p>Since the projections <span class="math inline">\mathbf{Malia\_Obama}_{\perp}</span> and <span class="math inline">\mathbf{Sasha\_Obama}_{\perp}</span> can be different from the original embeddings, TransH can model this one-to-many relation effectively.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transh" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transh"><span class="header-section-number">3.3.2</span> Strengths and limitations of TransH</h3>
<p>TransH offers several advantages over TransE:</p>
<ol type="1">
<li><p><strong>Better handling of complex relations</strong>: TransH can model one-to-many, many-to-one, and many-to-many relations by using relation-specific entity representations</p></li>
<li><p><strong>Symmetric relations</strong>: TransH can model symmetric relations because the projections of head and tail entities can be the same while their original embeddings differ</p></li>
</ol>
<p>However, TransH also has limitations:</p>
<ol type="1">
<li><p><strong>Increased complexity</strong>: TransH has more parameters (<span class="math inline">O(|E|d + 2|R|d)</span>) and is more complex than TransE</p></li>
<li><p><strong>Limited expressiveness</strong>: While TransH improves on TransE, it still has limitations in modeling certain relation patterns like composition</p></li>
</ol>
</section>
</section>
<section id="transr-modeling-relations-in-relation-specific-spaces" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="transr-modeling-relations-in-relation-specific-spaces"><span class="header-section-number">3.4</span> TransR: modeling relations in relation-specific spaces</h2>
<p>TransR (Lin et al., 2015) takes the idea of relation-specific representations further by projecting entities into entirely different vector spaces for each relation.</p>
<div id="def-transr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.6 (TransR model)</strong></span> In the <strong>TransR</strong> model:</p>
<ol type="1">
<li>Entities are embedded in an entity space <span class="math inline">\mathbb{R}^d</span></li>
<li>Relations are embedded in relation-specific spaces <span class="math inline">\mathbb{R}^k</span></li>
<li>Each relation <span class="math inline">r</span> has:
<ul>
<li>A projection matrix <span class="math inline">\mathbf{M}_r \in \mathbb{R}^{k \times d}</span> that maps from entity space to relation space</li>
<li>A translation vector <span class="math inline">\mathbf{r} \in \mathbb{R}^k</span> in the relation space</li>
</ul></li>
</ol>
<p>For a triple <span class="math inline">(h, r, t)</span>, the model:</p>
<ol type="1">
<li>Projects entities into the relation space:
<ul>
<li><span class="math inline">\mathbf{h}_r = \mathbf{M}_r \mathbf{h}</span></li>
<li><span class="math inline">\mathbf{t}_r = \mathbf{M}_r \mathbf{t}</span></li>
</ul></li>
<li>Applies the translation in the relation space:
<ul>
<li><span class="math inline">\mathbf{h}_r + \mathbf{r} \approx \mathbf{t}_r</span></li>
</ul></li>
</ol>
<p>The scoring function is: <span class="math display">f_r(h, t) = -\|\mathbf{M}_r \mathbf{h} + \mathbf{r} - \mathbf{M}_r \mathbf{t}\|_2^2</span></p>
</div>
<section id="geometric-interpretation-2" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="geometric-interpretation-2"><span class="header-section-number">3.4.1</span> Geometric interpretation</h3>
<p>TransR uses different vector spaces for different relations. The projection matrices <span class="math inline">\mathbf{M}_r</span> transform entities from the entity space to relation-specific spaces, where translations are then applied.</p>
<div id="exm-transr-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6 (TransR geometric example)</strong></span> Consider the different semantics of relations:</p>
<ul>
<li>“is_capital_of” relates cities to countries</li>
<li>“was_born_in” relates people to locations</li>
</ul>
<p>In TransR, these relations would have different projection matrices that transform entities into relation-specific spaces:</p>
<ul>
<li><span class="math inline">\mathbf{M}_{\text{is\_capital\_of}}</span> projects entities into a space suitable for capital-country relationships</li>
<li><span class="math inline">\mathbf{M}_{\text{was\_born\_in}}</span> projects entities into a space suitable for person-birthplace relationships</li>
</ul>
<p>This allows TransR to capture the different semantic properties of each relation type.</p>
</div>
</section>
<section id="cluster-based-transr-ctransr" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="cluster-based-transr-ctransr"><span class="header-section-number">3.4.2</span> Cluster-based TransR (CTransR)</h3>
<p>To further improve TransR, the authors proposed a cluster-based variant called CTransR, which clusters entity pairs for each relation and learns a distinct projection for each cluster.</p>
<div id="def-ctransr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.7 (CTransR model)</strong></span> The <strong>CTransR</strong> model extends TransR by:</p>
<ol type="1">
<li>For each relation <span class="math inline">r</span>, clustering the entity pairs <span class="math inline">(h, t)</span> into several groups</li>
<li>Learning a distinct translation vector <span class="math inline">\mathbf{r}_c</span> for each cluster <span class="math inline">c</span> of a relation <span class="math inline">r</span></li>
<li>Using the same projection matrix <span class="math inline">\mathbf{M}_r</span> for all clusters of a relation</li>
</ol>
<p>The scoring function becomes: <span class="math display">f_{r,c}(h, t) = -\|\mathbf{M}_r \mathbf{h} + \mathbf{r}_c - \mathbf{M}_r \mathbf{t}\|_2^2</span></p>
<p>where <span class="math inline">c</span> is the cluster assignment for the entity pair <span class="math inline">(h, t)</span> under relation <span class="math inline">r</span>.</p>
</div>
<p>This approach allows for more fine-grained modeling of relations, as different semantic aspects of a relation can be captured by different clusters.</p>
</section>
<section id="strengths-and-limitations-of-transr" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="strengths-and-limitations-of-transr"><span class="header-section-number">3.4.3</span> Strengths and limitations of TransR</h3>
<p>TransR offers several advantages:</p>
<ol type="1">
<li><p><strong>High expressiveness</strong>: By using relation-specific spaces, TransR can model complex relations more effectively than TransE and TransH</p></li>
<li><p><strong>Semantic differentiation</strong>: TransR can capture the different semantic characteristics of relations through their projection matrices</p></li>
</ol>
<p>However, TransR also has significant limitations:</p>
<ol type="1">
<li><p><strong>High computational complexity</strong>: With <span class="math inline">O(|E|d + |R|k + |R|kd)</span> parameters, TransR is much more computationally expensive than TransE and TransH</p></li>
<li><p><strong>Risk of overfitting</strong>: The large number of parameters can lead to overfitting, especially for relations with few training examples</p></li>
</ol>
</section>
</section>
<section id="transd-dynamic-mapping-matrices" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="transd-dynamic-mapping-matrices"><span class="header-section-number">3.5</span> TransD: dynamic mapping matrices</h2>
<p>TransD (Ji et al., 2015) addresses the high computational complexity of TransR by using dynamic mapping matrices constructed from entity and relation vectors.</p>
<div id="def-transd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.8 (TransD model)</strong></span> In the <strong>TransD</strong> model:</p>
<ol type="1">
<li>Each entity <span class="math inline">e</span> has two vector representations:
<ul>
<li>A standard embedding <span class="math inline">\mathbf{e} \in \mathbb{R}^d</span></li>
<li>A projection vector <span class="math inline">\mathbf{e}_p \in \mathbb{R}^m</span></li>
</ul></li>
<li>Each relation <span class="math inline">r</span> has two vector representations:
<ul>
<li>A standard embedding <span class="math inline">\mathbf{r} \in \mathbb{R}^k</span></li>
<li>A projection vector <span class="math inline">\mathbf{r}_p \in \mathbb{R}^n</span></li>
</ul></li>
<li>Mapping matrices are dynamically constructed:
<ul>
<li><span class="math inline">\mathbf{M}_{rh} = \mathbf{r}_p \mathbf{h}_p^T + \mathbf{I}_{m \times n}</span></li>
<li><span class="math inline">\mathbf{M}_{rt} = \mathbf{r}_p \mathbf{t}_p^T + \mathbf{I}_{m \times n}</span> where <span class="math inline">\mathbf{I}_{m \times n}</span> is a matrix with 1s on the diagonal and 0s elsewhere</li>
</ul></li>
</ol>
<p>The scoring function is: <span class="math display">f_r(h, t) = -\|\mathbf{M}_{rh} \mathbf{h} + \mathbf{r} - \mathbf{M}_{rt} \mathbf{t}\|_2^2</span></p>
</div>
<section id="geometric-interpretation-3" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="geometric-interpretation-3"><span class="header-section-number">3.5.1</span> Geometric interpretation</h3>
<p>TransD creates entity-relation-specific projection matrices by combining entity and relation projection vectors. This allows for more flexibility than TransR while maintaining computational efficiency.</p>
<div id="exm-transd-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7 (TransD geometric example)</strong></span> Consider an entity “Barack_Obama” involved in different relations:</p>
<ul>
<li>(Barack_Obama, was_president_of, United_States)</li>
<li>(Barack_Obama, has_child, Malia_Obama)</li>
</ul>
<p>With TransD, “Barack_Obama” would have:</p>
<ul>
<li>A standard embedding <span class="math inline">\mathbf{Barack\_Obama}</span></li>
<li>A projection vector <span class="math inline">\mathbf{Barack\_Obama}_p</span></li>
</ul>
<p>For each relation, different mapping matrices would be constructed:</p>
<ul>
<li><span class="math inline">\mathbf{M}_{\text{was\_president\_of, Barack\_Obama}} = \mathbf{was\_president\_of}_p \mathbf{Barack\_Obama}_p^T + \mathbf{I}</span></li>
<li><span class="math inline">\mathbf{M}_{\text{has\_child, Barack\_Obama}} = \mathbf{has\_child}_p \mathbf{Barack\_Obama}_p^T + \mathbf{I}</span></li>
</ul>
<p>This allows for entity-relation-specific projections with fewer parameters than TransR.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transd" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transd"><span class="header-section-number">3.5.2</span> Strengths and limitations of TransD</h3>
<p>TransD offers several advantages:</p>
<ol type="1">
<li><p><strong>Reduced complexity</strong>: With <span class="math inline">O(|E|(d+m) + |R|(k+n))</span> parameters, TransD is more efficient than TransR while still allowing for entity-relation-specific projections</p></li>
<li><p><strong>Flexibility</strong>: The dynamic mapping matrices can capture both entity-specific and relation-specific features</p></li>
</ol>
<p>However, TransD still has limitations:</p>
<ol type="1">
<li><p><strong>Increased complexity compared to TransE and TransH</strong>: The additional projection vectors add complexity to the model</p></li>
<li><p><strong>Limited projection capacity</strong>: The rank-1 projection matrices may not capture all necessary transformations for complex relations</p></li>
</ol>
</section>
</section>
<section id="transparse-adaptive-sparse-matrices" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="transparse-adaptive-sparse-matrices"><span class="header-section-number">3.6</span> TranSparse: adaptive sparse matrices</h2>
<p>TranSparse (Ji et al., 2016) introduces sparse mapping matrices to further reduce computational complexity while maintaining expressive power.</p>
<div id="def-transparse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.9 (TranSparse model)</strong></span> The <strong>TranSparse</strong> model extends TransR by using sparse projection matrices:</p>
<ol type="1">
<li>Each relation <span class="math inline">r</span> has a sparsity degree <span class="math inline">\theta_r \in [0, 1]</span></li>
<li>The projection matrix <span class="math inline">\mathbf{M}_r</span> has a sparsity of <span class="math inline">\theta_r</span> (proportion of elements set to zero)</li>
<li>Sparsity degrees are determined based on the number of entity pairs involving the relation</li>
</ol>
<p>Two variants exist:</p>
<ol type="1">
<li><strong>TranSparse(share)</strong>: Uses the same sparse pattern for all relations</li>
<li><strong>TranSparse(separate)</strong>: Uses different sparse patterns for each relation</li>
</ol>
<p>The scoring function is similar to TransR: <span class="math display">f_r(h, t) = -\|\mathbf{M}_r(\theta_r) \mathbf{h} + \mathbf{r} - \mathbf{M}_r(\theta_r) \mathbf{t}\|_2^2</span></p>
<p>where <span class="math inline">\mathbf{M}_r(\theta_r)</span> is the sparse projection matrix for relation <span class="math inline">r</span> with sparsity <span class="math inline">\theta_r</span>.</p>
</div>
<section id="adaptive-sparsity" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="adaptive-sparsity"><span class="header-section-number">3.6.1</span> Adaptive sparsity</h3>
<p>A key innovation in TranSparse is the use of adaptive sparsity based on the frequency of relations:</p>
<ol type="1">
<li><p>Frequent relations (with many training examples) can use sparser matrices because they have sufficient data to learn even with fewer parameters</p></li>
<li><p>Rare relations (with few training examples) use denser matrices to maintain expressive power despite limited data</p></li>
</ol>
<div id="exm-transparse" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8 (TranSparse example)</strong></span> Consider two relations:</p>
<ul>
<li>“is_capital_of” (appears in 200 triples)</li>
<li>“won_nobel_prize_in” (appears in 20 triples)</li>
</ul>
<p>TranSparse might assign:</p>
<ul>
<li><span class="math inline">\theta_{\text{is\_capital\_of}} = 0.8</span> (80% sparsity, only 20% of matrix elements are non-zero)</li>
<li><span class="math inline">\theta_{\text{won\_nobel\_prize\_in}} = 0.3</span> (30% sparsity, 70% of matrix elements are non-zero)</li>
</ul>
<p>This balances model complexity with the amount of available training data for each relation.</p>
</div>
</section>
<section id="strengths-and-limitations-of-transparse" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="strengths-and-limitations-of-transparse"><span class="header-section-number">3.6.2</span> Strengths and limitations of TranSparse</h3>
<p>TranSparse offers several advantages:</p>
<ol type="1">
<li><p><strong>Adaptive complexity</strong>: The sparsity of projection matrices adapts to the frequency of relations, reducing overfitting</p></li>
<li><p><strong>Computational efficiency</strong>: Sparse matrices require less memory and computational resources than dense matrices</p></li>
</ol>
<p>However, TranSparse still has limitations:</p>
<ol type="1">
<li><p><strong>Complex implementation</strong>: The management of sparse matrices adds implementation complexity</p></li>
<li><p><strong>Determining sparsity</strong>: Finding optimal sparsity degrees for each relation can be challenging</p></li>
</ol>
</section>
</section>
<section id="rotate-rotation-in-complex-space" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="rotate-rotation-in-complex-space"><span class="header-section-number">3.7</span> RotatE: rotation in complex space</h2>
<p>RotatE (Sun et al., 2019) is a more recent translation-based model that represents relations as rotations in complex vector space, allowing it to model various relation patterns.</p>
<div id="def-rotate" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.10 (RotatE model)</strong></span> In the <strong>RotatE</strong> model:</p>
<ol type="1">
<li>Entities and relations are embedded in complex space <span class="math inline">\mathbb{C}^d</span></li>
<li>Relations are modeled as rotations in the complex plane</li>
<li>For a triple <span class="math inline">(h, r, t)</span>, the model enforces: <span class="math inline">\mathbf{t} = \mathbf{h} \circ \mathbf{r}</span> where <span class="math inline">\circ</span> is the Hadamard (element-wise) product, and <span class="math inline">|\mathbf{r}_i| = 1</span> for all components <span class="math inline">i</span></li>
</ol>
<p>The scoring function is: <span class="math display">f_r(h, t) = -\|\mathbf{h} \circ \mathbf{r} - \mathbf{t}\|^2</span></p>
</div>
<section id="geometric-interpretation-4" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="geometric-interpretation-4"><span class="header-section-number">3.7.1</span> Geometric interpretation</h3>
<p>In RotatE, each relation is a rotation in the complex plane. For each dimension of the embedding, the relation rotates the head entity’s component by a specific angle to approximate the tail entity’s component.</p>
<div id="exm-rotate-geometry" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9 (RotatE geometric example)</strong></span> Consider a single dimension in the complex embedding space:</p>
<ul>
<li>Head entity component: <span class="math inline">h_i = a + bi</span></li>
<li>Relation component: <span class="math inline">r_i = \cos\theta + i\sin\theta</span> (unit modulus complex number)</li>
<li>Tail entity component: <span class="math inline">t_i = c + di</span></li>
</ul>
<p>The rotation is: <span class="math inline">h_i \cdot r_i = (a + bi)(\cos\theta + i\sin\theta) = a\cos\theta - b\sin\theta + i(a\sin\theta + b\cos\theta)</span></p>
<p>This rotates the complex number <span class="math inline">h_i</span> by angle <span class="math inline">\theta</span> in the complex plane.</p>
<p>For symmetric relations (e.g., “is_sibling_of”), the rotation angle would be 0 or π, meaning <span class="math inline">r_i = 1</span> or <span class="math inline">r_i = -1</span>.</p>
<p>For asymmetric relations (e.g., “is_parent_of”), the rotation angle would be neither 0 nor π.</p>
<p>For inverse relations (e.g., “is*parent_of” and “is_child_of”), the rotation angles would be negatives of each other, meaning <span class="math inline">r*{\text{is_child_of}} = \overline{r\_{\text{is_parent_of}}}</span> (complex conjugate).</p>
</div>
</section>
<section id="modeling-relation-patterns-with-rotate" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="modeling-relation-patterns-with-rotate"><span class="header-section-number">3.7.2</span> Modeling relation patterns with RotatE</h3>
<p>RotatE can model various relation patterns:</p>
<ol type="1">
<li><strong>Symmetry</strong>: For a symmetric relation, <span class="math inline">r_i = 1</span> or <span class="math inline">r_i = -1</span> for all dimensions <span class="math inline">i</span></li>
<li><strong>Antisymmetry</strong>: For an antisymmetric relation, <span class="math inline">r_i \neq 1</span> and <span class="math inline">r_i \neq -1</span> for some dimensions <span class="math inline">i</span></li>
<li><strong>Inversion</strong>: For inverse relations <span class="math inline">r_1</span> and <span class="math inline">r_2</span>, <span class="math inline">r_2 = \overline{r_1}</span> (complex conjugate)</li>
<li><strong>Composition</strong>: For relations <span class="math inline">r_1</span>, <span class="math inline">r_2</span>, and <span class="math inline">r_3</span> where <span class="math inline">r_1 \circ r_2 = r_3</span>, the rotation angles add: <span class="math inline">\theta_{r_3} = \theta_{r_1} + \theta_{r_2}</span></li>
</ol>
<div id="exm-rotate-patterns" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10 (RotatE relation patterns example)</strong></span> Consider these relation patterns:</p>
<ol type="1">
<li><p><strong>Symmetry</strong> (e.g., “is_sibling_of”): If <span class="math inline">\mathbf{h} \circ \mathbf{r} = \mathbf{t}</span> and <span class="math inline">\mathbf{t} \circ \mathbf{r} = \mathbf{h}</span>, then <span class="math inline">\mathbf{r} \circ \mathbf{r} = \mathbf{1}</span>, meaning <span class="math inline">r_i = 1</span> or <span class="math inline">r_i = -1</span> for all <span class="math inline">i</span>.</p></li>
<li><p><strong>Inversion</strong> (e.g., “is_parent_of” and “is_child_of”): If <span class="math inline">\mathbf{h} \circ \mathbf{r}_1 = \mathbf{t}</span> and <span class="math inline">\mathbf{t} \circ \mathbf{r}_2 = \mathbf{h}</span>, then <span class="math inline">\mathbf{r}_1 \circ \mathbf{r}_2 = \mathbf{1}</span>, meaning <span class="math inline">r_2 = \overline{r_1}</span>.</p></li>
<li><p><strong>Composition</strong> (e.g., “is_born_in” and “is_located_in” compose to “has_nationality”): If <span class="math inline">\mathbf{h} \circ \mathbf{r}_1 = \mathbf{e}</span> and <span class="math inline">\mathbf{e} \circ \mathbf{r}_2 = \mathbf{t}</span>, then <span class="math inline">\mathbf{h} \circ (\mathbf{r}_1 \circ \mathbf{r}_2) = \mathbf{t}</span>, meaning <span class="math inline">\mathbf{r}_3 = \mathbf{r}_1 \circ \mathbf{r}_2</span>.</p></li>
</ol>
</div>
</section>
<section id="self-adversarial-negative-sampling" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="self-adversarial-negative-sampling"><span class="header-section-number">3.7.3</span> Self-adversarial negative sampling</h3>
<p>RotatE introduces an advanced training technique called self-adversarial negative sampling:</p>
<div id="def-self-adversarial" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.11 (Self-adversarial negative sampling)</strong></span> In <strong>self-adversarial negative sampling</strong>, the weight of a negative sample <span class="math inline">(h', r, t')</span> is determined by its current score according to the model:</p>
<p><span class="math display">p(h', r, t') = \frac{\exp(\alpha f_r(h', t'))}{\sum_{(h_j', r, t_j') \in S'_{(h,r,t)}} \exp(\alpha f_r(h_j', t_j'))}</span></p>
<p>where <span class="math inline">\alpha</span> is a temperature hyperparameter.</p>
<p>The negative sampling loss becomes: <span class="math display">L = -\log\sigma(\gamma - f_r(h, t)) - \sum_{(h', r, t') \in S'_{(h,r,t)}} p(h', r, t') \log\sigma(f_r(h', t') - \gamma)</span></p>
<p>where <span class="math inline">\sigma</span> is the sigmoid function and <span class="math inline">\gamma</span> is the margin.</p>
</div>
<p>This approach weights negative samples based on their current scores, focusing training on “hard” negative samples (those that the model incorrectly scores as plausible).</p>
</section>
<section id="strengths-and-limitations-of-rotate" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="strengths-and-limitations-of-rotate"><span class="header-section-number">3.7.4</span> Strengths and limitations of RotatE</h3>
<p>RotatE offers several advantages:</p>
<ol type="1">
<li><p><strong>Modeling relation patterns</strong>: RotatE can model symmetry, antisymmetry, inversion, and composition relations in a unified framework</p></li>
<li><p><strong>Parameter efficiency</strong>: With <span class="math inline">O(|E|d + |R|d)</span> parameters (where <span class="math inline">d</span> is the dimension of the complex space), RotatE is as efficient as TransE</p></li>
<li><p><strong>State-of-the-art performance</strong>: RotatE achieves strong results on various benchmark datasets</p></li>
</ol>
<p>However, RotatE still has limitations:</p>
<ol type="1">
<li><p><strong>Complex embeddings</strong>: Working with complex numbers adds some computational complexity</p></li>
<li><p><strong>Limited to rotation transformations</strong>: While rotations are powerful, some relation patterns might require more general transformations</p></li>
</ol>
</section>
</section>
<section id="comparing-translation-based-models" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="comparing-translation-based-models"><span class="header-section-number">3.8</span> Comparing translation-based models</h2>
<p>Let’s compare the key characteristics of the translation-based models we’ve discussed:</p>
<div id="def-model-comparison" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.12 (Comparison of translation-based models)</strong></span> &nbsp;</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Entity Space</th>
<th>Relation Space</th>
<th>Transformation</th>
<th>Parameters</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TransE</td>
<td><span class="math inline">\mathbb{R}^d</span></td>
<td><span class="math inline">\mathbb{R}^d</span></td>
<td>Translation: <span class="math inline">\mathbf{h} + \mathbf{r} \approx \mathbf{t}</span></td>
<td><span class="math inline">O( | E | d + | R | d)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>TransH</td>
<td><span class="math inline">\mathbb{R}^d</span></td>
<td>Hyperplanes in <span class="math inline">\mathbb{R}^d</span></td>
<td>Projection + Translation: <span class="math inline">\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}</span></td>
<td><span class="math inline">O( | E | d + 2 | R | d)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>TransR</td>
<td><span class="math inline">\mathbb{R}^d</span></td>
<td><span class="math inline">\mathbb{R}^k</span></td>
<td>Projection + Translation: <span class="math inline">\mathbf{M}_r \mathbf{h} + \mathbf{r} \approx \mathbf{M}_r \mathbf{t}</span></td>
<td><span class="math inline">O( | E | d + | R | k + | R | kd)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>TransD</td>
<td><span class="math inline">\mathbb{R}^d, \mathbb{R}^m</span></td>
<td><span class="math inline">\mathbb{R}^k, \mathbb{R}^n</span></td>
<td>Dynamic Projection + Translation: <span class="math inline">\mathbf{M}_{rh} \mathbf{h} + \mathbf{r} \approx \mathbf{M}_{rt} \mathbf{t}</span></td>
<td><span class="math inline">O( | E | (d+m) + | R | (k+n))</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>TranSparse</td>
<td><span class="math inline">\mathbb{R}^d</span></td>
<td><span class="math inline">\mathbb{R}^k</span></td>
<td>Sparse Projection + Translation: <span class="math inline">\mathbf{M}_r(\theta_r) \mathbf{h} + \mathbf{r} \approx \mathbf{M}_r(\theta_r) \mathbf{t}</span></td>
<td><span class="math inline">O( | E | d + | R | k + (1-\theta) | R | kd)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>RotatE</td>
<td><span class="math inline">\mathbb{C}^d</span></td>
<td>Complex rotations</td>
<td>Rotation: <span class="math inline">\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}</span></td>
<td><span class="math inline">O( | E | d + | R | d)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Each model makes different trade-offs between expressiveness, parameter efficiency, and computational complexity. The evolution of these models reflects a progression toward more flexible representations while trying to maintain computational efficiency.</p>
<div id="exm-model-evolution" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.11 (Model evolution example)</strong></span> Consider modeling a symmetric relation like “is_sibling_of”:</p>
<ol type="1">
<li><p><strong>TransE</strong> would struggle because it would require <span class="math inline">\mathbf{r} \approx \mathbf{0}</span>, which doesn’t distinguish the relation from others.</p></li>
<li><p><strong>TransH</strong> would project entities onto a relation-specific hyperplane, allowing for symmetric relationships through equivalent projections.</p></li>
<li><p><strong>TransR</strong> would project entities into a relation-specific space where the translation could effectively model symmetry.</p></li>
<li><p><strong>RotatE</strong> would model the relation as rotations by 0 or π in the complex plane, naturally capturing symmetry.</p></li>
</ol>
<p>This evolution shows how each model addressed limitations of its predecessors.</p>
</div>
</section>
<section id="relation-patterns-and-model-capabilities" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="relation-patterns-and-model-capabilities"><span class="header-section-number">3.9</span> Relation patterns and model capabilities</h2>
<p>Different translation-based models have different capabilities for modeling relation patterns:</p>
<div id="def-pattern-capabilities" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.13 (Model capabilities for relation patterns)</strong></span> &nbsp;</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Relation Pattern</th>
<th>TransE</th>
<th>TransH</th>
<th>TransR</th>
<th>TransD</th>
<th>RotatE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-to-1</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>1-to-many</td>
<td>Weak</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>Many-to-1</td>
<td>Weak</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="even">
<td>Many-to-many</td>
<td>Weak</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>Symmetry</td>
<td>Weak</td>
<td>Medium</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Antisymmetry</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
<td>Strong</td>
</tr>
<tr class="odd">
<td>Inversion</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
</tr>
<tr class="even">
<td>Composition</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
<td>Strong</td>
</tr>
</tbody>
</table>
</div>
<p>Understanding these capabilities helps in selecting the appropriate model for specific knowledge graphs based on the prevalent relation patterns.</p>
<div id="exm-pattern-selection" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.12 (Model selection example)</strong></span> Consider different knowledge graphs:</p>
<ol type="1">
<li><p><strong>Geographic knowledge graph</strong> (mostly 1-to-1 relations like “is_capital_of”):</p>
<ul>
<li>TransE might be sufficient and computationally efficient</li>
</ul></li>
<li><p><strong>Family relationship knowledge graph</strong> (many symmetric relations like “is_sibling_of” and inverse relations like “is_parent_of”/“is_child_of”):</p>
<ul>
<li>RotatE would be a good choice for capturing these patterns</li>
</ul></li>
<li><p><strong>Academic knowledge graph</strong> (many-to-many relations like “author_of” where an author can write multiple papers and a paper can have multiple authors):</p>
<ul>
<li>TransR or TransD might be more appropriate to handle the complex relationships</li>
</ul></li>
</ol>
</div>
</section>
<section id="implementation-considerations" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="implementation-considerations"><span class="header-section-number">3.10</span> Implementation considerations</h2>
<p>When implementing translation-based models, several practical considerations are important:</p>
<section id="normalization-constraints" class="level3" data-number="3.10.1">
<h3 data-number="3.10.1" class="anchored" data-anchor-id="normalization-constraints"><span class="header-section-number">3.10.1</span> Normalization constraints</h3>
<p>Many translation-based models apply normalization constraints to entity and relation embeddings:</p>
<div id="def-normalization-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.14 (Common normalization constraints)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>TransE</strong>: Entity embeddings are often constrained to unit L2 norm: <span class="math inline">\|\mathbf{e}\|_2 = 1</span></p></li>
<li><p><strong>TransH</strong>: Both entity embeddings and hyperplane normal vectors are normalized: <span class="math inline">\|\mathbf{e}\|_2 = 1</span> and <span class="math inline">\|\mathbf{w}_r\|_2 = 1</span></p></li>
<li><p><strong>TransR/TransD</strong>: Entity embeddings are normalized, and sometimes additional orthogonality constraints are applied to projection matrices</p></li>
<li><p><strong>RotatE</strong>: Relation embeddings are constrained to have unit modulus: <span class="math inline">|r_i| = 1</span> for all components <span class="math inline">i</span></p></li>
</ol>
</div>
<p>These constraints prevent the model from “cheating” by scaling embeddings arbitrarily and ensure that the geometric interpretations remain valid.</p>
</section>
<section id="initialization-strategies" class="level3" data-number="3.10.2">
<h3 data-number="3.10.2" class="anchored" data-anchor-id="initialization-strategies"><span class="header-section-number">3.10.2</span> Initialization strategies</h3>
<p>Proper initialization of embeddings is crucial for effective training:</p>
<div id="def-initialization-strategies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.15 (Initialization strategies)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Uniform initialization</strong>: Sample from a uniform distribution, e.g., <span class="math inline">U(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})</span></p></li>
<li><p><strong>Normal initialization</strong>: Sample from a normal distribution, e.g., <span class="math inline">N(0, \frac{1}{d})</span></p></li>
<li><p><strong>Xavier/Glorot initialization</strong>: Scale based on input and output dimensions</p></li>
<li><p><strong>Complex initialization</strong>: For RotatE, initialize relation embeddings with random phases but unit modulus</p></li>
</ol>
</div>
<div id="exm-initialization" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.13 (Initialization example)</strong></span> For TransE with embedding dimension <span class="math inline">d = 100</span>:</p>
<ol type="1">
<li>Initialize entity embeddings from <span class="math inline">U(-0.1, 0.1)</span></li>
<li>Initialize relation embeddings from <span class="math inline">U(-0.1, 0.1)</span></li>
<li>Normalize entity embeddings to have unit L2 norm</li>
<li>Begin training with these normalized initializations</li>
</ol>
</div>
</section>
<section id="negative-sampling-strategies" class="level3" data-number="3.10.3">
<h3 data-number="3.10.3" class="anchored" data-anchor-id="negative-sampling-strategies"><span class="header-section-number">3.10.3</span> Negative sampling strategies</h3>
<p>The selection of negative samples significantly impacts training effectiveness:</p>
<div id="def-negative-sampling" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.16 (Negative sampling strategies)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Random sampling</strong>: Replace head or tail entity with a random entity from the knowledge graph</p></li>
<li><p><strong>Bernoulli sampling</strong>: Choose whether to corrupt the head or tail based on the relation’s mapping properties (implemented in TransH, TransR, etc.)</p></li>
<li><p><strong>Self-adversarial sampling</strong>: Weight negative samples based on their current scores (implemented in RotatE)</p></li>
</ol>
</div>
<div id="exm-bernoulli-sampling" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.14 (Bernoulli sampling example)</strong></span> For a 1-to-many relation like “has_child”:</p>
<ul>
<li>The head entity (parent) has fewer valid tails than a tail entity (child) has valid heads</li>
<li>We should corrupt the head entity more often than the tail entity</li>
<li>Bernoulli sampling might use a head corruption probability of 0.8 and tail corruption probability of 0.2</li>
</ul>
<p>For a many-to-1 relation like “born_in”:</p>
<ul>
<li>The head entity (person) has fewer valid tails than a tail entity (location) has valid heads</li>
<li>We should corrupt the tail entity more often than the head entity</li>
<li>Bernoulli sampling might use a head corruption probability of 0.2 and tail corruption probability of 0.8</li>
</ul>
</div>
</section>
<section id="optimization-algorithms" class="level3" data-number="3.10.4">
<h3 data-number="3.10.4" class="anchored" data-anchor-id="optimization-algorithms"><span class="header-section-number">3.10.4</span> Optimization algorithms</h3>
<p>Various optimization algorithms can be used to train translation-based models:</p>
<div id="def-optimization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.17 (Optimization algorithms)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: Simple and widely used, but may require careful learning rate tuning</p></li>
<li><p><strong>AdaGrad</strong>: Adapts learning rates for each parameter based on historical gradients</p></li>
<li><p><strong>Adam</strong>: Combines adaptive learning rates with momentum for faster convergence</p></li>
<li><p><strong>L-BFGS</strong>: A second-order method that can work well for smaller datasets</p></li>
</ol>
</div>
<p>The choice of optimization algorithm depends on the specific model, dataset size, and computational resources.</p>
</section>
</section>
<section id="performance-analysis" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="performance-analysis"><span class="header-section-number">3.11</span> Performance analysis</h2>
<p>Let’s analyze the empirical performance of translation-based models on standard benchmark datasets:</p>
<section id="benchmark-datasets" class="level3" data-number="3.11.1">
<h3 data-number="3.11.1" class="anchored" data-anchor-id="benchmark-datasets"><span class="header-section-number">3.11.1</span> Benchmark datasets</h3>
<p>Several benchmark datasets are commonly used to evaluate knowledge graph embedding models:</p>
<div id="def-benchmarks" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.18 (Common benchmark datasets)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>FB15k</strong>: A subset of Freebase with 14,951 entities, 1,345 relations, and 592,213 triples</p></li>
<li><p><strong>FB15k-237</strong>: A modified version of FB15k with inverse relations removed, containing 14,541 entities, 237 relations, and 310,116 triples</p></li>
<li><p><strong>WN18</strong>: A subset of WordNet with 40,943 entities, 18 relations, and 151,442 triples</p></li>
<li><p><strong>WN18RR</strong>: A modified version of WN18 with inverse relations removed, containing 40,943 entities, 11 relations, and 93,003 triples</p></li>
<li><p><strong>YAGO3-10</strong>: A subset of YAGO3 with 123,182 entities, 37 relations, and 1,089,040 triples</p></li>
</ol>
</div>
</section>
<section id="evaluation-metrics" class="level3" data-number="3.11.2">
<h3 data-number="3.11.2" class="anchored" data-anchor-id="evaluation-metrics"><span class="header-section-number">3.11.2</span> Evaluation metrics</h3>
<p>Knowledge graph embedding models are typically evaluated using link prediction metrics:</p>
<div id="def-evaluation-metrics" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.19 (Common evaluation metrics)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Mean Rank (MR)</strong>: The average rank of the correct entity among all entities in the knowledge graph</p></li>
<li><p><strong>Mean Reciprocal Rank (MRR)</strong>: The average of the reciprocal ranks of the correct entities <span class="math display">\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}</span></p></li>
<li><p><strong>Hits@k</strong>: The percentage of test cases where the correct entity appears in the top k predictions <span class="math display">\text{Hits@k} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \mathbf{1}[\text{rank}_i \leq k]</span> where <span class="math inline">\mathbf{1}[\cdot]</span> is the indicator function</p></li>
</ol>
</div>
<p>Lower MR and higher MRR and Hits@k indicate better performance.</p>
<div id="exm-evaluation" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.15 (Evaluation example)</strong></span> Consider a test triple (France, has_capital, Paris) and a model that ranks all entities for the query (France, has_capital, ?):</p>
<p>If the model ranks Paris as the 3rd entity (with Rome and Berlin incorrectly ranked higher):</p>
<ul>
<li>Rank = 3</li>
<li>Reciprocal Rank = 1/3</li>
<li>Hits@1 = 0 (incorrect)</li>
<li>Hits@3 = 1 (correct)</li>
<li>Hits@10 = 1 (correct)</li>
</ul>
</div>
</section>
<section id="comparative-results" class="level3" data-number="3.11.3">
<h3 data-number="3.11.3" class="anchored" data-anchor-id="comparative-results"><span class="header-section-number">3.11.3</span> Comparative results</h3>
<p>Based on published results, here’s a comparison of translation-based models on benchmark datasets:</p>
<div id="def-comparative-results" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.20 (Performance comparison (Hits@10 in %))</strong></span> &nbsp;</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>FB15k</th>
<th>WN18</th>
<th>FB15k-237</th>
<th>WN18RR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TransE</td>
<td>47.1</td>
<td>89.2</td>
<td>42.8</td>
<td>43.3</td>
</tr>
<tr class="even">
<td>TransH</td>
<td>64.4</td>
<td>86.7</td>
<td>41.5</td>
<td>43.0</td>
</tr>
<tr class="odd">
<td>TransR</td>
<td>68.7</td>
<td>92.0</td>
<td>40.8</td>
<td>43.2</td>
</tr>
<tr class="even">
<td>TransD</td>
<td>77.3</td>
<td>92.2</td>
<td>41.2</td>
<td>43.5</td>
</tr>
<tr class="odd">
<td>RotatE</td>
<td>83.1</td>
<td>95.9</td>
<td>47.6</td>
<td>57.1</td>
</tr>
</tbody>
</table>
</div>
<p>These results show a general trend of improved performance with more sophisticated models, with RotatE achieving the best results across datasets.</p>
<div id="exm-performance-analysis" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.16 (Performance analysis example)</strong></span> On FB15k, the progression from TransE (47.1%) to RotatE (83.1%) shows a dramatic improvement in Hits@10, likely due to:</p>
<ol type="1">
<li>TransE’s limitations with complex relations (FB15k contains many 1-to-many, many-to-1, and many-to-many relations)</li>
<li>RotatE’s ability to model various relation patterns through complex rotations</li>
<li>RotatE’s self-adversarial negative sampling technique, which improves training efficiency</li>
</ol>
<p>On simpler datasets like WN18, even basic models like TransE perform well (89.2% Hits@10) because the relations are primarily hierarchical and fit well with the translation approach.</p>
</div>
</section>
</section>
<section id="advanced-topics" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="advanced-topics"><span class="header-section-number">3.12</span> Advanced topics</h2>
<p>Several advanced topics extend the basic translation-based models:</p>
<section id="entity-type-constraints" class="level3" data-number="3.12.1">
<h3 data-number="3.12.1" class="anchored" data-anchor-id="entity-type-constraints"><span class="header-section-number">3.12.1</span> Entity type constraints</h3>
<p>Incorporating entity type information can improve performance by restricting the set of possible entities for a given relation:</p>
<div id="def-type-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.21 (Entity type constraints)</strong></span> <strong>Entity type constraints</strong> limit the possible head and tail entities for a relation based on their types.</p>
<p>For example, the relation “director_of” might constrain:</p>
<ul>
<li>Head entities to be of type “Person”</li>
<li>Tail entities to be of type “Movie” or “TVShow”</li>
</ul>
<p>These constraints can be integrated into translation-based models by:</p>
<ol type="1">
<li>Assigning types to entities in preprocessing</li>
<li>Only considering entities of the correct type during link prediction</li>
<li>Incorporating type information into the embedding space</li>
</ol>
</div>
<p>Type constraints can significantly improve performance by reducing the search space for link prediction.</p>
</section>
<section id="temporal-knowledge-graphs" class="level3" data-number="3.12.2">
<h3 data-number="3.12.2" class="anchored" data-anchor-id="temporal-knowledge-graphs"><span class="header-section-number">3.12.2</span> Temporal knowledge graphs</h3>
<p>Many real-world facts are valid only during specific time periods. Temporal knowledge graph embedding models extend translation-based approaches to handle time:</p>
<div id="def-temporal-kg" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.22 (Temporal knowledge graph embeddings)</strong></span> <strong>Temporal knowledge graph embeddings</strong> represent quadruples <span class="math inline">(h, r, t, \tau)</span> where <span class="math inline">\tau</span> is a time point or interval.</p>
<p>Extensions of translation-based models include:</p>
<ol type="1">
<li><strong>TTransE</strong>: Adds a temporal translation vector</li>
<li><strong>HyTE</strong>: Projects entities and relations onto time-specific hyperplanes</li>
<li><strong>TA-TransE</strong>: Uses temporal attention mechanisms</li>
</ol>
</div>
<div id="exm-temporal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.17 (Temporal knowledge graph example)</strong></span> Consider the fact “Barack Obama was president of the United States from 2009 to 2017.”</p>
<p>In a temporal knowledge graph, this might be represented as: (Barack_Obama, president_of, United_States, [2009, 2017])</p>
<p>A temporal embedding model would learn to predict not only the entities and relations but also when the fact was valid.</p>
</div>
</section>
<section id="multi-modal-knowledge-graphs" class="level3" data-number="3.12.3">
<h3 data-number="3.12.3" class="anchored" data-anchor-id="multi-modal-knowledge-graphs"><span class="header-section-number">3.12.3</span> Multi-modal knowledge graphs</h3>
<p>Some knowledge graphs incorporate multiple modalities, such as text, images, and numerical values:</p>
<div id="def-multimodal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.23 (Multi-modal knowledge graph embeddings)</strong></span> <strong>Multi-modal knowledge graph embeddings</strong> integrate information from different modalities:</p>
<ol type="1">
<li>Text descriptions of entities and relations</li>
<li>Visual information (images) associated with entities</li>
<li>Numerical attributes of entities</li>
</ol>
<p>Extensions of translation-based models include:</p>
<ol type="1">
<li><strong>IKRL</strong>: Incorporates image information into entity embeddings</li>
<li><strong>KDCoE</strong>: Jointly learns from textual descriptions and structured knowledge</li>
<li><strong>TransEA</strong>: Incorporates numerical attributes</li>
</ol>
</div>
<div id="exm-multimodal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.18 (Multi-modal knowledge graph example)</strong></span> For the entity “Eiffel Tower,” a multi-modal knowledge graph might include:</p>
<ul>
<li>Structured facts: (Eiffel_Tower, located_in, Paris)</li>
<li>Textual description: “The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.”</li>
<li>Images: Several photographs of the Eiffel Tower</li>
<li>Numerical attributes: Height: 330m, Built: 1889</li>
</ul>
<p>A multi-modal embedding model would learn to integrate all these sources of information into a unified embedding space.</p>
</div>
</section>
<section id="uncertainty-modeling" class="level3" data-number="3.12.4">
<h3 data-number="3.12.4" class="anchored" data-anchor-id="uncertainty-modeling"><span class="header-section-number">3.12.4</span> Uncertainty modeling</h3>
<p>Real-world knowledge often involves uncertainty. Several extensions of translation-based models incorporate uncertainty:</p>
<div id="def-uncertainty" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.24 (Uncertainty in knowledge graph embeddings)</strong></span> <strong>Uncertainty modeling</strong> in knowledge graph embeddings represents entities and relations as distributions rather than point vectors:</p>
<ol type="1">
<li><strong>KG2E</strong>: Models entities and relations as Gaussian distributions</li>
<li><strong>TransG</strong>: Uses Gaussian mixtures for multi-component relations</li>
<li><strong>UKGE</strong>: Incorporates uncertainty through Bayesian approaches</li>
</ol>
</div>
<div id="exm-uncertainty" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.19 (Uncertainty modeling example)</strong></span> In KG2E, instead of representing the entity “Paris” as a vector <span class="math inline">\mathbf{Paris}</span>, it would be represented as a Gaussian distribution <span class="math inline">\mathcal{N}(\boldsymbol{\mu}_{\text{Paris}}, \boldsymbol{\Sigma}_{\text{Paris}})</span>.</p>
<p>The mean vector <span class="math inline">\boldsymbol{\mu}_{\text{Paris}}</span> represents the central tendency of the entity’s embedding, while the covariance matrix <span class="math inline">\boldsymbol{\Sigma}_{\text{Paris}}</span> represents the uncertainty about this embedding.</p>
<p>Facts with high certainty would have lower variance, while facts with uncertainty would have higher variance.</p>
</div>
</section>
</section>
<section id="applications-of-translation-based-models" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="applications-of-translation-based-models"><span class="header-section-number">3.13</span> Applications of translation-based models</h2>
<p>Translation-based knowledge graph embedding models have been applied to various domains:</p>
<section id="recommendation-systems" class="level3" data-number="3.13.1">
<h3 data-number="3.13.1" class="anchored" data-anchor-id="recommendation-systems"><span class="header-section-number">3.13.1</span> Recommendation systems</h3>
<p>Knowledge graph embeddings can enhance recommendation systems by capturing complex relationships between users, items, and their attributes:</p>
<div id="def-recommendation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.25 (Knowledge graph-based recommendation)</strong></span> <strong>Knowledge graph-based recommendation</strong> systems use embeddings to:</p>
<ol type="1">
<li>Represent users, items, and their attributes in a unified embedding space</li>
<li>Model complex interaction patterns through relation embeddings</li>
<li>Generate recommendations based on embedding similarities or translations</li>
</ol>
</div>
<div id="exm-recommendation" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.20 (Recommendation system example)</strong></span> Consider a movie recommendation system:</p>
<ol type="1">
<li><strong>Entities</strong>: Users, movies, actors, directors, genres</li>
<li><strong>Relations</strong>: watched, acted_in, directed, has_genre</li>
<li><strong>Triples</strong>: (User1, watched, Movie1), (Actor1, acted_in, Movie1), (Movie1, has_genre, Action)</li>
</ol>
<p>A translation-based model might predict: <span class="math inline">\mathbf{User1} + \mathbf{likes} \approx \mathbf{Action}</span></p>
<p>This could be used to recommend action movies that the user hasn’t watched yet.</p>
</div>
</section>
<section id="question-answering" class="level3" data-number="3.13.2">
<h3 data-number="3.13.2" class="anchored" data-anchor-id="question-answering"><span class="header-section-number">3.13.2</span> Question answering</h3>
<p>Knowledge graph embeddings can support question answering systems by enabling semantic matching and inference:</p>
<div id="def-question-answering" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.26 (Knowledge graph-based question answering)</strong></span> <strong>Knowledge graph-based question answering</strong> systems use embeddings to:</p>
<ol type="1">
<li>Map natural language questions to knowledge graph queries</li>
<li>Perform link prediction to find answers</li>
<li>Rank candidate answers based on embedding scores</li>
</ol>
</div>
<div id="exm-question-answering" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.21 (Question answering example)</strong></span> For the question “Who directed Inception?”, a system might:</p>
<ol type="1">
<li>Identify “Inception” as an entity and “directed” as a relation</li>
<li>Formulate a query (?, directed, Inception)</li>
<li>Use a translation-based model to rank entities based on <span class="math inline">\|\mathbf{e} + \mathbf{directed} - \mathbf{Inception}\|</span></li>
<li>Return “Christopher Nolan” as the top-ranked entity</li>
</ol>
</div>
</section>
<section id="information-extraction" class="level3" data-number="3.13.3">
<h3 data-number="3.13.3" class="anchored" data-anchor-id="information-extraction"><span class="header-section-number">3.13.3</span> Information extraction</h3>
<p>Knowledge graph embeddings can assist in information extraction by providing semantic context for entity and relation extraction:</p>
<div id="def-information-extraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.27 (Knowledge graph-based information extraction)</strong></span> <strong>Knowledge graph-based information extraction</strong> uses embeddings to:</p>
<ol type="1">
<li>Identify candidate entities and relations in text</li>
<li>Score candidate triples based on their plausibility in the knowledge graph</li>
<li>Filter or rank extraction results based on embedding scores</li>
</ol>
</div>
<div id="exm-information-extraction" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.22 (Information extraction example)</strong></span> Given the text “Tim Cook is the CEO of Apple, which is headquartered in Cupertino”:</p>
<ol type="1">
<li><p>An information extraction system might identify candidate triples:</p>
<ul>
<li>(Tim_Cook, is_CEO_of, Apple)</li>
<li>(Apple, headquartered_in, Cupertino)</li>
</ul></li>
<li><p>A translation-based model could score these triples based on existing knowledge graph embeddings</p></li>
<li><p>High scores would indicate consistency with existing knowledge, supporting the extraction</p></li>
</ol>
</div>
</section>
</section>
<section id="future-directions" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="future-directions"><span class="header-section-number">3.14</span> Future directions</h2>
<p>Translation-based models continue to evolve, with several promising research directions:</p>
<section id="inductive-learning" class="level3" data-number="3.14.1">
<h3 data-number="3.14.1" class="anchored" data-anchor-id="inductive-learning"><span class="header-section-number">3.14.1</span> Inductive learning</h3>
<p>Most current translation-based models are transductive, meaning they can only make predictions about entities seen during training. Inductive models aim to handle new entities:</p>
<div id="def-inductive-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.28 (Inductive translation-based models)</strong></span> <strong>Inductive translation-based models</strong> can generate embeddings for previously unseen entities based on their relations with known entities or their attributes.</p>
<p>Approaches include:</p>
<ol type="1">
<li><strong>Graph neural network extensions</strong> that generate embeddings based on local graph structure</li>
<li><strong>Textual description incorporation</strong> to generate embeddings from entity names or descriptions</li>
<li><strong>Attribute-based models</strong> that leverage entity attributes for inductive learning</li>
</ol>
</div>
</section>
<section id="neural-architecture-integration" class="level3" data-number="3.14.2">
<h3 data-number="3.14.2" class="anchored" data-anchor-id="neural-architecture-integration"><span class="header-section-number">3.14.2</span> Neural architecture integration</h3>
<p>Integrating translation-based models with neural architectures can enhance their expressiveness:</p>
<div id="def-neural-integration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.29 (Neural-enhanced translation models)</strong></span> <strong>Neural-enhanced translation models</strong> combine the geometric intuition of translation-based models with the expressive power of neural networks:</p>
<ol type="1">
<li><strong>Graph neural networks</strong> for entity representation learning</li>
<li><strong>Attention mechanisms</strong> to dynamically weight different aspects of embeddings</li>
<li><strong>Transformer-based architectures</strong> for context-aware embeddings</li>
</ol>
</div>
</section>
<section id="explainable-embeddings" class="level3" data-number="3.14.3">
<h3 data-number="3.14.3" class="anchored" data-anchor-id="explainable-embeddings"><span class="header-section-number">3.14.3</span> Explainable embeddings</h3>
<p>Enhancing the explainability of translation-based models is another important research direction:</p>
<div id="def-explainable-embeddings" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.30 (Explainable translation-based models)</strong></span> <strong>Explainable translation-based models</strong> provide interpretable justifications for their predictions:</p>
<ol type="1">
<li><strong>Path-based explanations</strong> that identify relevant paths in the knowledge graph</li>
<li><strong>Rule-based interpretations</strong> that extract symbolic rules from embeddings</li>
<li><strong>Attribution methods</strong> that identify the most influential components of embeddings</li>
</ol>
</div>
</section>
</section>
<section id="summary" class="level2" data-number="3.15">
<h2 data-number="3.15" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.15</span> Summary</h2>
<p>In this chapter, we’ve explored translation-based embedding models for knowledge graphs, starting with the foundational TransE model and progressing through various extensions:</p>
<ul>
<li>TransE introduced the basic idea of representing relations as translations in the embedding space</li>
<li>TransH extended this by projecting entities onto relation-specific hyperplanes</li>
<li>TransR further generalized the approach by mapping entities to relation-specific spaces</li>
<li>TransD optimized the projection matrices through dynamic construction</li>
<li>TranSparse introduced adaptive sparse matrices to balance expressiveness and efficiency</li>
<li>RotatE leveraged complex vector spaces to model various relation patterns through rotations</li>
</ul>
<p>These models demonstrate a progression toward more flexible and expressive representations while trying to maintain computational efficiency. Each model makes different trade-offs and has different capabilities for modeling various relation patterns.</p>
<p>Translation-based models provide an intuitive geometric interpretation of knowledge graph relationships and serve as the foundation for many more sophisticated approaches. Despite the development of alternative paradigms, the principles introduced by translation-based models continue to influence the field of knowledge graph embeddings.</p>
</section>
<section id="further-reading" class="level2" data-number="3.16">
<h2 data-number="3.16" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">3.16</span> Further reading</h2>
<section id="original-papers" class="level3" data-number="3.16.1">
<h3 data-number="3.16.1" class="anchored" data-anchor-id="original-papers"><span class="header-section-number">3.16.1</span> Original papers</h3>
<ul>
<li>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems (pp.&nbsp;2787-2795).</li>
<li>Wang, Z., Zhang, J., Feng, J., &amp; Chen, Z. (2014). Knowledge Graph Embedding by Translating on Hyperplanes. In AAAI Conference on Artificial Intelligence (pp.&nbsp;1112-1119).</li>
<li>Lin, Y., Liu, Z., Sun, M., Liu, Y., &amp; Zhu, X. (2015). Learning Entity and Relation Embeddings for Knowledge Graph Completion. In AAAI Conference on Artificial Intelligence (pp.&nbsp;2181-2187).</li>
<li>Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge Graph Embedding via Dynamic Mapping Matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (pp.&nbsp;687-696).</li>
<li>Ji, G., Liu, K., He, S., &amp; Zhao, J. (2016). Knowledge Graph Completion with Adaptive Sparse Transfer Matrix. In AAAI Conference on Artificial Intelligence (pp.&nbsp;985-991).</li>
<li>Sun, Z., Deng, Z. H., Nie, J. Y., &amp; Tang, J. (2019). RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. In International Conference on Learning Representations.</li>
</ul>
</section>
<section id="surveys-and-comparative-analyses" class="level3" data-number="3.16.2">
<h3 data-number="3.16.2" class="anchored" data-anchor-id="surveys-and-comparative-analyses"><span class="header-section-number">3.16.2</span> Surveys and comparative analyses</h3>
<ul>
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge Graph Embedding: A Survey of Approaches and Applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
<li>Rossi, A., Barbosa, D., Firmani, D., Matinata, A., &amp; Merialdo, P. (2021). Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. ACM Transactions on Knowledge Discovery from Data, 15(2), 1-49.</li>
<li>Ali, M., Berrendorf, M., Hoyt, C. T., Vermue, L., Galkin, M., Sharifzadeh, S., … &amp; Lehmann, J. (2021). PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings. Journal of Machine Learning Research, 22(82), 1-6.</li>
</ul>
</section>
<section id="applications-and-extensions" class="level3" data-number="3.16.3">
<h3 data-number="3.16.3" class="anchored" data-anchor-id="applications-and-extensions"><span class="header-section-number">3.16.3</span> Applications and extensions</h3>
<ul>
<li>Zhang, F., Yuan, N. J., Lian, D., Xie, X., &amp; Ma, W. Y. (2016). Collaborative Knowledge Base Embedding for Recommender Systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;353-362).</li>
<li>Xie, R., Liu, Z., &amp; Sun, M. (2016). Representation Learning of Knowledge Graphs with Hierarchical Types. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (pp.&nbsp;2965-2971).</li>
<li>Lacroix, T., Usunier, N., &amp; Obozinski, G. (2018). Canonical Tensor Decomposition for Knowledge Base Completion. In Proceedings of the 35th International Conference on Machine Learning (pp.&nbsp;2863-2872).</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/intro.html" class="pagination-link" aria-label="Introduction to Knowledge Graphs and Representations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>