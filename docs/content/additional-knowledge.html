<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Incorporating Additional Information – Knowledge Graph Embeddings for Link Prediction and Reasoning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/reasoning.html" rel="next">
<link href="../content/evaluation.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bdec3157979715d7154bb60f5aa24e58.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/additional-knowledge.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Knowledge Graph Embeddings for Link Prediction and Reasoning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/outline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Outline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Knowledge Graphs and Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Fundamentals of Vector Space Representations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/translation-based.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Translation-Based Embedding Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/semantic-matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Semantic Matching Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rotation-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Models: Rotations and Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Training and Optimization Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/additional-knowledge.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Practical Applications and Case Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Implementing Knowledge Graph Embedding Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/frontier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Advanced Topics and Research Frontiers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Appendix A: Mathematical Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/appendix-b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Appendix B: Resources and Tools</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-enhanced-knowledge-graph-embeddings" id="toc-introduction-to-enhanced-knowledge-graph-embeddings" class="nav-link active" data-scroll-target="#introduction-to-enhanced-knowledge-graph-embeddings"><span class="header-section-number">10.1</span> Introduction to Enhanced Knowledge Graph Embeddings</a></li>
  <li><a href="#limitations-of-structure-only-embeddings" id="toc-limitations-of-structure-only-embeddings" class="nav-link" data-scroll-target="#limitations-of-structure-only-embeddings"><span class="header-section-number">10.2</span> Limitations of Structure-Only Embeddings</a></li>
  <li><a href="#types-of-additional-information" id="toc-types-of-additional-information" class="nav-link" data-scroll-target="#types-of-additional-information"><span class="header-section-number">10.3</span> Types of Additional Information</a>
  <ul class="collapse">
  <li><a href="#textual-information" id="toc-textual-information" class="nav-link" data-scroll-target="#textual-information"><span class="header-section-number">10.3.1</span> Textual Information</a></li>
  <li><a href="#entity-types-and-hierarchies" id="toc-entity-types-and-hierarchies" class="nav-link" data-scroll-target="#entity-types-and-hierarchies"><span class="header-section-number">10.3.2</span> Entity Types and Hierarchies</a></li>
  <li><a href="#numerical-attributes" id="toc-numerical-attributes" class="nav-link" data-scroll-target="#numerical-attributes"><span class="header-section-number">10.3.3</span> Numerical Attributes</a></li>
  <li><a href="#visual-information" id="toc-visual-information" class="nav-link" data-scroll-target="#visual-information"><span class="header-section-number">10.3.4</span> Visual Information</a></li>
  <li><a href="#temporal-information" id="toc-temporal-information" class="nav-link" data-scroll-target="#temporal-information"><span class="header-section-number">10.3.5</span> Temporal Information</a></li>
  </ul></li>
  <li><a href="#text-enhanced-knowledge-graph-embeddings" id="toc-text-enhanced-knowledge-graph-embeddings" class="nav-link" data-scroll-target="#text-enhanced-knowledge-graph-embeddings"><span class="header-section-number">10.4</span> Text-Enhanced Knowledge Graph Embeddings</a>
  <ul class="collapse">
  <li><a href="#description-based-approaches" id="toc-description-based-approaches" class="nav-link" data-scroll-target="#description-based-approaches"><span class="header-section-number">10.4.1</span> Description-Based Approaches</a></li>
  <li><a href="#joint-text-and-graph-embedding" id="toc-joint-text-and-graph-embedding" class="nav-link" data-scroll-target="#joint-text-and-graph-embedding"><span class="header-section-number">10.4.2</span> Joint Text and Graph Embedding</a></li>
  <li><a href="#pre-trained-language-models-for-kges" id="toc-pre-trained-language-models-for-kges" class="nav-link" data-scroll-target="#pre-trained-language-models-for-kges"><span class="header-section-number">10.4.3</span> Pre-trained Language Models for KGEs</a></li>
  </ul></li>
  <li><a href="#type-constrained-embeddings" id="toc-type-constrained-embeddings" class="nav-link" data-scroll-target="#type-constrained-embeddings"><span class="header-section-number">10.5</span> Type-Constrained Embeddings</a>
  <ul class="collapse">
  <li><a href="#hard-type-constraints" id="toc-hard-type-constraints" class="nav-link" data-scroll-target="#hard-type-constraints"><span class="header-section-number">10.5.1</span> Hard Type Constraints</a></li>
  <li><a href="#soft-type-constraints" id="toc-soft-type-constraints" class="nav-link" data-scroll-target="#soft-type-constraints"><span class="header-section-number">10.5.2</span> Soft Type Constraints</a></li>
  <li><a href="#type-hierarchies-and-inheritance" id="toc-type-hierarchies-and-inheritance" class="nav-link" data-scroll-target="#type-hierarchies-and-inheritance"><span class="header-section-number">10.5.3</span> Type Hierarchies and Inheritance</a></li>
  </ul></li>
  <li><a href="#numerical-attribute-integration" id="toc-numerical-attribute-integration" class="nav-link" data-scroll-target="#numerical-attribute-integration"><span class="header-section-number">10.6</span> Numerical Attribute Integration</a>
  <ul class="collapse">
  <li><a href="#direct-attribute-encoding" id="toc-direct-attribute-encoding" class="nav-link" data-scroll-target="#direct-attribute-encoding"><span class="header-section-number">10.6.1</span> Direct Attribute Encoding</a></li>
  <li><a href="#attribute-based-similarity" id="toc-attribute-based-similarity" class="nav-link" data-scroll-target="#attribute-based-similarity"><span class="header-section-number">10.6.2</span> Attribute-Based Similarity</a></li>
  <li><a href="#translating-numerical-relations" id="toc-translating-numerical-relations" class="nav-link" data-scroll-target="#translating-numerical-relations"><span class="header-section-number">10.6.3</span> Translating Numerical Relations</a></li>
  </ul></li>
  <li><a href="#visual-information-integration" id="toc-visual-information-integration" class="nav-link" data-scroll-target="#visual-information-integration"><span class="header-section-number">10.7</span> Visual Information Integration</a>
  <ul class="collapse">
  <li><a href="#multi-modal-entity-representations" id="toc-multi-modal-entity-representations" class="nav-link" data-scroll-target="#multi-modal-entity-representations"><span class="header-section-number">10.7.1</span> Multi-modal Entity Representations</a></li>
  <li><a href="#visual-relationship-detection" id="toc-visual-relationship-detection" class="nav-link" data-scroll-target="#visual-relationship-detection"><span class="header-section-number">10.7.2</span> Visual Relationship Detection</a></li>
  </ul></li>
  <li><a href="#temporal-information-integration" id="toc-temporal-information-integration" class="nav-link" data-scroll-target="#temporal-information-integration"><span class="header-section-number">10.8</span> Temporal Information Integration</a>
  <ul class="collapse">
  <li><a href="#time-aware-embeddings" id="toc-time-aware-embeddings" class="nav-link" data-scroll-target="#time-aware-embeddings"><span class="header-section-number">10.8.1</span> Time-Aware Embeddings</a></li>
  <li><a href="#temporal-relation-embeddings" id="toc-temporal-relation-embeddings" class="nav-link" data-scroll-target="#temporal-relation-embeddings"><span class="header-section-number">10.8.2</span> Temporal Relation Embeddings</a></li>
  </ul></li>
  <li><a href="#joint-learning-approaches" id="toc-joint-learning-approaches" class="nav-link" data-scroll-target="#joint-learning-approaches"><span class="header-section-number">10.9</span> Joint Learning Approaches</a>
  <ul class="collapse">
  <li><a href="#multi-view-learning" id="toc-multi-view-learning" class="nav-link" data-scroll-target="#multi-view-learning"><span class="header-section-number">10.9.1</span> Multi-view Learning</a></li>
  <li><a href="#attention-based-fusion" id="toc-attention-based-fusion" class="nav-link" data-scroll-target="#attention-based-fusion"><span class="header-section-number">10.9.2</span> Attention-Based Fusion</a></li>
  </ul></li>
  <li><a href="#case-studies-and-performance-improvements" id="toc-case-studies-and-performance-improvements" class="nav-link" data-scroll-target="#case-studies-and-performance-improvements"><span class="header-section-number">10.10</span> Case Studies and Performance Improvements</a>
  <ul class="collapse">
  <li><a href="#text-enhanced-embeddings" id="toc-text-enhanced-embeddings" class="nav-link" data-scroll-target="#text-enhanced-embeddings"><span class="header-section-number">10.10.1</span> Text-Enhanced Embeddings</a></li>
  <li><a href="#type-constrained-embeddings-1" id="toc-type-constrained-embeddings-1" class="nav-link" data-scroll-target="#type-constrained-embeddings-1"><span class="header-section-number">10.10.2</span> Type-Constrained Embeddings</a></li>
  <li><a href="#multi-modal-embeddings" id="toc-multi-modal-embeddings" class="nav-link" data-scroll-target="#multi-modal-embeddings"><span class="header-section-number">10.10.3</span> Multi-modal Embeddings</a></li>
  <li><a href="#temporal-embeddings" id="toc-temporal-embeddings" class="nav-link" data-scroll-target="#temporal-embeddings"><span class="header-section-number">10.10.4</span> Temporal Embeddings</a></li>
  </ul></li>
  <li><a href="#implementation-challenges-and-solutions" id="toc-implementation-challenges-and-solutions" class="nav-link" data-scroll-target="#implementation-challenges-and-solutions"><span class="header-section-number">10.11</span> Implementation Challenges and Solutions</a>
  <ul class="collapse">
  <li><a href="#heterogeneous-data-integration" id="toc-heterogeneous-data-integration" class="nav-link" data-scroll-target="#heterogeneous-data-integration"><span class="header-section-number">10.11.1</span> Heterogeneous Data Integration</a></li>
  <li><a href="#computational-efficiency" id="toc-computational-efficiency" class="nav-link" data-scroll-target="#computational-efficiency"><span class="header-section-number">10.11.2</span> Computational Efficiency</a></li>
  <li><a href="#overfitting-prevention" id="toc-overfitting-prevention" class="nav-link" data-scroll-target="#overfitting-prevention"><span class="header-section-number">10.11.3</span> Overfitting Prevention</a></li>
  </ul></li>
  <li><a href="#practical-recommendations" id="toc-practical-recommendations" class="nav-link" data-scroll-target="#practical-recommendations"><span class="header-section-number">10.12</span> Practical Recommendations</a></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions"><span class="header-section-number">10.13</span> Future Directions</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10.14</span> Conclusion</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">10.15</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Incorporating Additional Information</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="introduction-to-enhanced-knowledge-graph-embeddings" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="introduction-to-enhanced-knowledge-graph-embeddings"><span class="header-section-number">10.1</span> Introduction to Enhanced Knowledge Graph Embeddings</h2>
<p>While the methods we’ve explored in previous chapters can be effective for capturing the structural patterns in knowledge graphs, they often rely solely on the graph structure itself. In practice, knowledge graphs exist in rich information ecosystems where additional data beyond the triple structure is available. This additional information can significantly enhance the quality of knowledge graph embeddings, particularly for entities with few connections or complex relationship patterns.</p>
<p>In this chapter, we explore how to incorporate various types of additional information into knowledge graph embedding models. We’ll discuss different sources of information, methods for integration, and the impact on performance across various tasks. The goal is to provide a comprehensive understanding of how auxiliary data can be leveraged to create more accurate and robust knowledge graph embeddings.</p>
<p>Let’s begin by examining the limitations of structure-only embeddings and the motivations for incorporating additional information.</p>
</section>
<section id="limitations-of-structure-only-embeddings" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="limitations-of-structure-only-embeddings"><span class="header-section-number">10.2</span> Limitations of Structure-Only Embeddings</h2>
<p>Knowledge graph embedding models that rely solely on the graph structure face several challenges:</p>
<div id="def-structure-limitations" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.1 (Limitations of Structure-Only Embeddings)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Cold-start problem</strong>: New entities with few connections have limited structural information to inform their embeddings</li>
<li><strong>Disambiguation challenges</strong>: Entities with similar connection patterns but different meanings may receive similar embeddings</li>
<li><strong>Domain-specific subtleties</strong>: Some relations require domain knowledge beyond graph structure for proper interpretation</li>
<li><strong>Data sparsity</strong>: Real-world knowledge graphs are typically sparse, with most entities having only a few connections</li>
<li><strong>Long-tail entities</strong>: Many entities appear in very few triples, leading to poor representation quality</li>
</ol>
</div>
<p>These limitations can significantly impact the performance of knowledge graph embedding models, particularly for tasks involving less well-connected entities or complex relationships.</p>
<div id="exm-structure-limitations" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.1 (Example of Structure Limitations)</strong></span> Consider two distinct people who both were born in the same city, work in the same profession, and have similar relationships. With structure-only embeddings, these individuals might receive nearly identical representations, making it difficult to distinguish between them in downstream tasks.</p>
<p>Similarly, a newly added entity with only a single connection to the knowledge graph would receive a poor-quality embedding, as the structural information is insufficient to properly position it in the embedding space.</p>
</div>
</section>
<section id="types-of-additional-information" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="types-of-additional-information"><span class="header-section-number">10.3</span> Types of Additional Information</h2>
<p>Knowledge graphs often come with or can be enriched with various types of additional information beyond the standard (head, relation, tail) triples.</p>
<section id="textual-information" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="textual-information"><span class="header-section-number">10.3.1</span> Textual Information</h3>
<p>Textual information includes descriptions, names, and other text associated with entities and relations.</p>
<div id="def-textual-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.2 (Textual Information for Knowledge Graphs)</strong></span> Common forms of textual information include:</p>
<ol type="1">
<li><strong>Entity descriptions</strong>: Paragraphs describing the entity (e.g., Wikipedia articles)</li>
<li><strong>Entity names</strong>: The surface forms or labels of entities (including aliases)</li>
<li><strong>Relation descriptions</strong>: Explanations of what a relation means</li>
<li><strong>Textual patterns</strong>: Common phrases that express relationships in natural language</li>
<li><strong>Entity mentions</strong>: Occurrences of entities in broader text corpora</li>
</ol>
</div>
<p>Textual information provides semantic context that can help disambiguate entities and enrich their representations beyond what is captured by the graph structure alone.</p>
<div id="exm-textual-information" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.2 (Example of Textual Information)</strong></span> For the entity “Apple Inc.,” textual information might include:</p>
<ul>
<li><strong>Name</strong>: “Apple Inc.” (and aliases like “Apple Computer,” “Apple”)</li>
<li><strong>Description</strong>: “Apple Inc.&nbsp;is an American multinational technology company that specializes in consumer electronics, software, and online services…”</li>
<li><strong>Textual patterns</strong>: “Apple makes iPhones,” “Apple was founded by Steve Jobs,” etc.</li>
</ul>
<p>This information helps distinguish Apple Inc.&nbsp;from the fruit and provides rich semantic content not captured in the graph structure.</p>
</div>
</section>
<section id="entity-types-and-hierarchies" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="entity-types-and-hierarchies"><span class="header-section-number">10.3.2</span> Entity Types and Hierarchies</h3>
<p>Many knowledge graphs include type information that classifies entities into categories and organizes these categories into hierarchies.</p>
<div id="def-type-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.3 (Entity Type Information)</strong></span> Entity type information includes:</p>
<ol type="1">
<li><strong>Entity types</strong>: Categories or classes that entities belong to (e.g., Person, Organization, Location)</li>
<li><strong>Type hierarchies</strong>: The organization of types into a taxonomy (e.g., Athlete is a subtype of Person)</li>
<li><strong>Disjoint types</strong>: Information about types that cannot overlap (e.g., a single entity cannot be both a Person and a Location)</li>
<li><strong>Multi-typing</strong>: The assignment of multiple types to a single entity (e.g., an entity can be both a Scientist and an Author)</li>
</ol>
</div>
<p>Type information introduces additional constraints and structure that can guide the learning of entity embeddings.</p>
<div id="exm-type-hierarchy" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.3 (Example of Type Hierarchy)</strong></span> Consider a simple type hierarchy:</p>
<ul>
<li>Thing
<ul>
<li>Person
<ul>
<li>Artist
<ul>
<li>Musician</li>
<li>Painter</li>
</ul></li>
<li>Scientist</li>
</ul></li>
<li>Organization
<ul>
<li>Company</li>
<li>Government</li>
</ul></li>
<li>Location
<ul>
<li>City</li>
<li>Country</li>
</ul></li>
</ul></li>
</ul>
<p>An entity “Leonardo da Vinci” would have types [Person, Artist, Painter, Scientist], providing structured information about the entity’s nature.</p>
</div>
</section>
<section id="numerical-attributes" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="numerical-attributes"><span class="header-section-number">10.3.3</span> Numerical Attributes</h3>
<p>Entities often have numerical attributes that provide quantitative information.</p>
<div id="def-numerical-attributes" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.4 (Numerical Attributes)</strong></span> Numerical attributes include:</p>
<ol type="1">
<li><strong>Measurements</strong>: Quantities associated with entities (e.g., height, weight, population)</li>
<li><strong>Timestamps</strong>: Temporal information about events or entity properties</li>
<li><strong>Coordinates</strong>: Geographical locations expressed as latitude and longitude</li>
<li><strong>Counts</strong>: Quantities that represent occurrences or frequency</li>
<li><strong>Ratings</strong>: Numerical evaluations or scores</li>
</ol>
</div>
<p>Numerical attributes can provide fine-grained information that is difficult to capture through categorical relationships alone.</p>
<div id="exm-numerical-attributes" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.4 (Example of Numerical Attributes)</strong></span> For a city entity like “New York City,” numerical attributes might include:</p>
<ul>
<li>Population: 8,804,190</li>
<li>Area: 783.8 km²</li>
<li>Founded: 1624 (timestamp)</li>
<li>Coordinates: 40.7128° N, 74.0060° W</li>
<li>Average Temperature: 12.1°C</li>
</ul>
</div>
</section>
<section id="visual-information" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="visual-information"><span class="header-section-number">10.3.4</span> Visual Information</h3>
<p>For certain domains, visual information associated with entities can provide valuable additional context.</p>
<div id="def-visual-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.5 (Visual Information)</strong></span> Visual information includes:</p>
<ol type="1">
<li><strong>Entity images</strong>: Pictures or visual representations of entities</li>
<li><strong>Visual relationships</strong>: Spatial or visual relationships between entities</li>
<li><strong>Visual attributes</strong>: Properties that can be observed visually</li>
<li><strong>Diagrams and charts</strong>: Structured visual representations</li>
</ol>
</div>
<p>Visual information is particularly useful for entities whose identity or properties have strong visual components.</p>
<div id="exm-visual-information" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.5 (Example of Visual Information)</strong></span> For an entity representing the “Mona Lisa” painting, visual information would include:</p>
<ul>
<li>The image of the painting itself</li>
<li>Visual attributes like color scheme, composition, style</li>
<li>Spatial relationships between elements in the painting</li>
</ul>
</div>
</section>
<section id="temporal-information" class="level3" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="temporal-information"><span class="header-section-number">10.3.5</span> Temporal Information</h3>
<p>Many relationships and attributes in knowledge graphs have temporal aspects that can be modeled explicitly.</p>
<div id="def-temporal-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.6 (Temporal Information)</strong></span> Temporal information includes:</p>
<ol type="1">
<li><strong>Validity periods</strong>: Time ranges during which triples are valid</li>
<li><strong>Event timestamps</strong>: Points in time when events occurred</li>
<li><strong>Sequential relationships</strong>: Ordering information between events or states</li>
<li><strong>Periodicity</strong>: Patterns of recurrence over time</li>
<li><strong>Evolving attributes</strong>: Properties that change over time</li>
</ol>
</div>
<p>Modeling temporal information allows for more accurate representation of dynamic knowledge and enables temporal reasoning.</p>
<div id="exm-temporal-information" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.6 (Example of Temporal Information)</strong></span> Consider the relationship (Barack Obama, presidentOf, United States):</p>
<ul>
<li>Validity period: January 20, 2009 to January 20, 2017</li>
<li>This triple is only valid during this specific time range</li>
</ul>
<p>Similarly, a person’s place of residence may change over time, making temporal qualification necessary for accuracy.</p>
</div>
</section>
</section>
<section id="text-enhanced-knowledge-graph-embeddings" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="text-enhanced-knowledge-graph-embeddings"><span class="header-section-number">10.4</span> Text-Enhanced Knowledge Graph Embeddings</h2>
<p>One of the most common forms of additional information is textual data. Several approaches have been developed to incorporate textual information into knowledge graph embeddings.</p>
<section id="description-based-approaches" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="description-based-approaches"><span class="header-section-number">10.4.1</span> Description-Based Approaches</h3>
<p>Description-based approaches use textual descriptions of entities to enhance their embeddings.</p>
<div id="def-description-based" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.7 (Description-Based Embedding Enhancement)</strong></span> Description-based approaches typically:</p>
<ol type="1">
<li>Process entity descriptions using text encoders (e.g., LSTM, CNN, Transformer)</li>
<li>Combine the resulting text embeddings with structure-based embeddings</li>
<li>Train the combined model using both graph and text objectives</li>
</ol>
</div>
<p>These approaches are particularly useful for addressing the cold-start problem, as they can generate reasonable embeddings for entities with limited or no structural information.</p>
<div id="thm-description-enhancement" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.1 (Effect of Description Enhancement)</strong></span> When properly integrated, description-based enhancements can lead to:</p>
<ol type="1">
<li>Improved performance on entities with few connections</li>
<li>Better disambiguation of entities with similar structural patterns</li>
<li>More coherent semantic organization of the embedding space</li>
<li>Enhanced transfer learning capabilities to new entities</li>
</ol>
<p>This effect is most pronounced in sparse regions of the knowledge graph where structural information is limited.</p>
</div>
<div id="exm-description-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.7 (Example of Description-Based Embedding)</strong></span> For a model incorporating entity descriptions:</p>
<ol type="1">
<li>The entity “Quantum Mechanics” has a description: “A fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles…”</li>
<li>This description is encoded into a vector using a text encoder</li>
<li>The text embedding is combined with the structure-based embedding</li>
<li>The combined embedding captures both the position in the knowledge graph and the semantic content of the description</li>
</ol>
</div>
</section>
<section id="joint-text-and-graph-embedding" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="joint-text-and-graph-embedding"><span class="header-section-number">10.4.2</span> Joint Text and Graph Embedding</h3>
<p>Joint embedding approaches simultaneously learn from both textual and graph information from the beginning.</p>
<div id="def-joint-embedding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.8 (Joint Text and Graph Embedding)</strong></span> Joint embedding approaches typically:</p>
<ol type="1">
<li>Define a shared embedding space for both textual and structural representations</li>
<li>Develop joint objectives that align text and graph embeddings</li>
<li>Learn embeddings that capture both types of information concurrently</li>
<li>Optimize for performance on combined text and graph tasks</li>
</ol>
</div>
<p>These approaches aim to create a unified semantic space where similar entities are close together, regardless of whether the similarity comes from text or graph structure.</p>
<div id="exm-joint-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.8 (Example of Joint Embedding)</strong></span> A joint embedding model might:</p>
<ol type="1">
<li>Process the triple (Einstein, developedTheory, Relativity) using a graph encoder</li>
<li>Process the text “Einstein developed the theory of relativity” using a text encoder</li>
<li>Train both encoders to produce similar embeddings for these related inputs</li>
<li>Result in an embedding space where the graph representation of “Einstein” is close to textual mentions of “Einstein”</li>
</ol>
</div>
</section>
<section id="pre-trained-language-models-for-kges" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="pre-trained-language-models-for-kges"><span class="header-section-number">10.4.3</span> Pre-trained Language Models for KGEs</h3>
<p>Recent advances in natural language processing have led to the integration of pre-trained language models like BERT, RoBERTa, and GPT into knowledge graph embeddings.</p>
<div id="def-pretrained-lm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.9 (Pre-trained Language Models for KGEs)</strong></span> Approaches using pre-trained language models typically:</p>
<ol type="1">
<li>Leverage large-scale pre-trained models like BERT to encode textual information</li>
<li>Fine-tune the language models for knowledge graph-specific tasks</li>
<li>Combine the contextual embeddings with graph structural embeddings</li>
<li>Benefit from the rich semantic knowledge already captured in the language models</li>
</ol>
</div>
<p>These approaches capitalize on the powerful semantic representations learned by language models from vast text corpora.</p>
<div id="exm-bert-kge" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.9 (Example of Pre-trained LM Integration)</strong></span> A BERT-enhanced knowledge graph embedding model might:</p>
<ol type="1">
<li>Use BERT to encode the names and descriptions of entities</li>
<li>Fine-tune BERT on a dataset of triples expressed in natural language</li>
<li>Combine the resulting embeddings with traditional structure-based embeddings</li>
<li>Use the combined representation for link prediction and other tasks</li>
</ol>
</div>
</section>
</section>
<section id="type-constrained-embeddings" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="type-constrained-embeddings"><span class="header-section-number">10.5</span> Type-Constrained Embeddings</h2>
<p>Entity type information provides valuable constraints and structure that can be incorporated into knowledge graph embeddings.</p>
<section id="hard-type-constraints" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="hard-type-constraints"><span class="header-section-number">10.5.1</span> Hard Type Constraints</h3>
<p>Hard type constraints enforce strict type compatibility rules during training and inference.</p>
<div id="def-hard-type-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.10 (Hard Type Constraints)</strong></span> Hard type constraint approaches typically:</p>
<ol type="1">
<li>Define domain and range constraints for each relation (e.g., bornIn relation requires a Person as head and a Location as tail)</li>
<li>Filter out impossible triples during negative sampling</li>
<li>Restrict predictions to entities of the appropriate type</li>
<li>Enforce strict type compatibility during training and inference</li>
</ol>
</div>
<p>These approaches improve efficiency by reducing the search space and enhance accuracy by eliminating type-incompatible predictions.</p>
<div id="exm-hard-type-constraints" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.10 (Example of Hard Type Constraints)</strong></span> For the relation “capitalOf”:</p>
<ol type="1">
<li>Domain constraint: Country (only countries can have capitals)</li>
<li>Range constraint: City (only cities can be capitals)</li>
</ol>
<p>During negative sampling, only cities would be considered as corrupted tails for (France, capitalOf, ?), and only countries would be considered as corrupted heads for (?, capitalOf, Paris).</p>
</div>
</section>
<section id="soft-type-constraints" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="soft-type-constraints"><span class="header-section-number">10.5.2</span> Soft Type Constraints</h3>
<p>Soft type constraints incorporate type information as preferences rather than strict rules.</p>
<div id="def-soft-type-constraints" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.11 (Soft Type Constraints)</strong></span> Soft type constraint approaches typically:</p>
<ol type="1">
<li>Represent types as regions or distributions in the embedding space</li>
<li>Penalize embeddings that violate type compatibility but don’t strictly prohibit them</li>
<li>Learn type-relation correlations from data</li>
<li>Allow for exceptions and uncertainty in type assignments</li>
</ol>
</div>
<p>These approaches are more flexible than hard constraints and can handle cases where type information is noisy or incomplete.</p>
<div id="exm-soft-type-constraints" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.11 (Example of Soft Type Constraints)</strong></span> A model with soft type constraints might:</p>
<ol type="1">
<li>Learn that entities of type Person tend to cluster in a certain region of the embedding space</li>
<li>Penalize embeddings of Person entities that drift too far from this region</li>
<li>Model the relation “authorOf” as typically connecting a Person to a Book, but allow for exceptions</li>
<li>Use type compatibility as a soft signal rather than a hard rule</li>
</ol>
</div>
</section>
<section id="type-hierarchies-and-inheritance" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="type-hierarchies-and-inheritance"><span class="header-section-number">10.5.3</span> Type Hierarchies and Inheritance</h3>
<p>Type hierarchies provide structured information about the relationships between different types.</p>
<div id="def-type-hierarchies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.12 (Type Hierarchies for Embeddings)</strong></span> Approaches leveraging type hierarchies typically:</p>
<ol type="1">
<li>Model the hierarchy using techniques like hierarchical embedding or order embeddings</li>
<li>Enforce subtype constraints (e.g., if an entity is a Scientist, it is also a Person)</li>
<li>Allow information to propagate along the hierarchy</li>
<li>Learn embeddings that respect the hierarchical structure</li>
</ol>
</div>
<p>These approaches capture the semantic organization of entity types and enable more sophisticated reasoning.</p>
<div id="exm-type-hierarchy-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.12 (Example of Type Hierarchy Embedding)</strong></span> In a type hierarchy model:</p>
<ol type="1">
<li>The type “Scientist” would be embedded as a subtype of “Person”</li>
<li>The embedding space would be organized such that all Scientist entities are also in the Person region</li>
<li>Constraints like “only Persons can have a birthDate” would automatically apply to Scientists</li>
<li>The hierarchy would guide the placement of entities in the embedding space</li>
</ol>
</div>
</section>
</section>
<section id="numerical-attribute-integration" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="numerical-attribute-integration"><span class="header-section-number">10.6</span> Numerical Attribute Integration</h2>
<p>Numerical attributes provide quantitative information that can enhance knowledge graph embeddings.</p>
<section id="direct-attribute-encoding" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="direct-attribute-encoding"><span class="header-section-number">10.6.1</span> Direct Attribute Encoding</h3>
<p>Direct attribute encoding incorporates numerical attributes directly into entity embeddings.</p>
<div id="def-direct-attribute-encoding" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.13 (Direct Attribute Encoding)</strong></span> Direct encoding approaches typically:</p>
<ol type="1">
<li>Normalize numerical attributes to a standard range</li>
<li>Append or integrate attribute values into entity embeddings</li>
<li>Learn transformation functions that map attributes to the embedding space</li>
<li>Use specialized loss functions that account for numerical similarities</li>
</ol>
</div>
<p>These approaches allow the model to capture fine-grained numerical relationships.</p>
<div id="exm-direct-attribute-encoding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.13 (Example of Direct Attribute Encoding)</strong></span> For entities representing cities with population attributes:</p>
<ol type="1">
<li>Population values would be normalized (e.g., using log transformation and scaling)</li>
<li>The normalized values would be integrated into city embeddings</li>
<li>The model would learn to represent cities with similar populations closer in the embedding space</li>
<li>Predictions involving population-related relations would be more accurate</li>
</ol>
</div>
</section>
<section id="attribute-based-similarity" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="attribute-based-similarity"><span class="header-section-number">10.6.2</span> Attribute-Based Similarity</h3>
<p>Attribute-based similarity approaches use numerical attributes to define additional similarity measures.</p>
<div id="def-attribute-similarity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.14 (Attribute-Based Similarity)</strong></span> Attribute similarity approaches typically:</p>
<ol type="1">
<li>Define similarity metrics based on numerical attributes</li>
<li>Incorporate these similarities as additional signals during training</li>
<li>Learn embeddings that respect both structural and attribute-based similarities</li>
<li>Use specialized loss functions that balance different similarity signals</li>
</ol>
</div>
<p>These approaches enhance the embedding space by incorporating multiple dimensions of similarity.</p>
<div id="exm-attribute-similarity" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.14 (Example of Attribute-Based Similarity)</strong></span> For a movie knowledge graph with rating attributes:</p>
<ol type="1">
<li>Movies with similar ratings would have an additional similarity signal</li>
<li>The model would learn to balance this signal with structural similarities</li>
<li>The embedding space would organize movies based on both their connections and their rating profiles</li>
<li>Predictions would benefit from this multi-faceted similarity measure</li>
</ol>
</div>
</section>
<section id="translating-numerical-relations" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="translating-numerical-relations"><span class="header-section-number">10.6.3</span> Translating Numerical Relations</h3>
<p>Some approaches treat numerical attributes as special types of relations with specific translation operations.</p>
<div id="def-numerical-translation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.15 (Translating Numerical Relations)</strong></span> Numerical translation approaches typically:</p>
<ol type="1">
<li>Define specialized scoring functions for numerical relations</li>
<li>Learn relation-specific transformations for numerical values</li>
<li>Model numerical comparisons (greater than, less than) as specific geometric configurations</li>
<li>Preserve numerical ordering in the embedding space</li>
</ol>
</div>
<p>These approaches allow for more accurate modeling of numerical relationships and enable numerical reasoning.</p>
<div id="exm-numerical-translation" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.15 (Example of Numerical Translation)</strong></span> For population relationships between cities:</p>
<ol type="1">
<li>The relation “hasLargerPopulationThan” would be modeled as a specific translation in the embedding space</li>
<li>Cities would be arranged in the embedding space partly based on their population</li>
<li>The model could accurately predict which city has a larger population</li>
<li>The embedding would preserve transitivity (if A &gt; B and B &gt; C, then A &gt; C)</li>
</ol>
</div>
</section>
</section>
<section id="visual-information-integration" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="visual-information-integration"><span class="header-section-number">10.7</span> Visual Information Integration</h2>
<p>For certain domains, visual information can provide valuable additional context for knowledge graph embeddings.</p>
<section id="multi-modal-entity-representations" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="multi-modal-entity-representations"><span class="header-section-number">10.7.1</span> Multi-modal Entity Representations</h3>
<p>Multi-modal approaches combine visual and graph-based information into unified entity representations.</p>
<div id="def-multimodal-representations" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.16 (Multi-modal Entity Representations)</strong></span> Multi-modal representation approaches typically:</p>
<ol type="1">
<li>Encode images using convolutional neural networks (CNNs) or vision transformers</li>
<li>Align visual and graph embedding spaces</li>
<li>Learn joint representations that capture both visual and relational information</li>
<li>Use multi-modal fusion techniques to combine different information sources</li>
</ol>
</div>
<p>These approaches are particularly valuable for entities with strong visual components, such as products, artwork, or physical objects.</p>
<div id="exm-multimodal-entity" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.16 (Example of Multi-modal Entity Representation)</strong></span> For a knowledge graph of artworks:</p>
<ol type="1">
<li>Each painting entity would have an associated image</li>
<li>A CNN would encode the visual features of the painting</li>
<li>These visual features would be combined with the structural embedding</li>
<li>The resulting representation would capture both the painting’s position in the art historical context and its visual characteristics</li>
</ol>
</div>
</section>
<section id="visual-relationship-detection" class="level3" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="visual-relationship-detection"><span class="header-section-number">10.7.2</span> Visual Relationship Detection</h3>
<p>Visual relationship detection approaches use visual information to enhance the representation of relationships.</p>
<div id="def-visual-relationship" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.17 (Visual Relationship Detection)</strong></span> Visual relationship approaches typically:</p>
<ol type="1">
<li>Extract visual features from images containing multiple entities</li>
<li>Identify visual relationships between entities (e.g., spatial arrangements, interactions)</li>
<li>Align these visual relationships with knowledge graph relations</li>
<li>Learn embeddings that are consistent with both visual and graph relationships</li>
</ol>
</div>
<p>These approaches are particularly useful for spatial and physical relationships that have strong visual components.</p>
<div id="exm-visual-relationship" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.17 (Example of Visual Relationship Detection)</strong></span> For a knowledge graph of objects and their relationships:</p>
<ol type="1">
<li>An image might show a person riding a bicycle</li>
<li>The visual relationship detector would identify the “riding” relationship from the image</li>
<li>This visual evidence would reinforce the (person, rides, bicycle) triple in the knowledge graph</li>
<li>The resulting embeddings would benefit from both visual and structural information</li>
</ol>
</div>
</section>
</section>
<section id="temporal-information-integration" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="temporal-information-integration"><span class="header-section-number">10.8</span> Temporal Information Integration</h2>
<p>Many knowledge graphs contain temporal information that can be incorporated into embeddings.</p>
<section id="time-aware-embeddings" class="level3" data-number="10.8.1">
<h3 data-number="10.8.1" class="anchored" data-anchor-id="time-aware-embeddings"><span class="header-section-number">10.8.1</span> Time-Aware Embeddings</h3>
<p>Time-aware embedding approaches explicitly model the temporal dimension.</p>
<div id="def-time-aware-embeddings" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.18 (Time-Aware Embeddings)</strong></span> Time-aware embedding approaches typically:</p>
<ol type="1">
<li>Represent time as points or intervals in a temporal dimension</li>
<li>Model how entity and relation embeddings evolve over time</li>
<li>Learn temporal patterns and periodicity</li>
<li>Enable queries conditioned on specific time points or intervals</li>
</ol>
</div>
<p>These approaches are essential for accurately representing evolving knowledge and enabling temporal reasoning.</p>
<div id="exm-time-aware-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.18 (Example of Time-Aware Embedding)</strong></span> For a historical knowledge graph:</p>
<ol type="1">
<li>The triple (Barack Obama, presidentOf, United States) would be associated with the time interval [2009-01-20, 2017-01-20]</li>
<li>The model would learn how entity representations evolve over time</li>
<li>Queries like “Who was the president of the United States in 2010?” could be accurately answered</li>
<li>The embedding would capture temporal patterns and transitions</li>
</ol>
</div>
</section>
<section id="temporal-relation-embeddings" class="level3" data-number="10.8.2">
<h3 data-number="10.8.2" class="anchored" data-anchor-id="temporal-relation-embeddings"><span class="header-section-number">10.8.2</span> Temporal Relation Embeddings</h3>
<p>Temporal relation embedding approaches specialize in modeling how relationships change over time.</p>
<div id="def-temporal-relations" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.19 (Temporal Relation Embeddings)</strong></span> Temporal relation approaches typically:</p>
<ol type="1">
<li>Assign time-dependent transformations to relations</li>
<li>Model the dynamics of how relationships evolve</li>
<li>Learn patterns of relationship formation and dissolution</li>
<li>Capture temporal dependencies between relations</li>
</ol>
</div>
<p>These approaches are particularly useful for dynamic domains where relationships frequently change.</p>
<div id="exm-temporal-relation" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.19 (Example of Temporal Relation Embedding)</strong></span> In a social network knowledge graph:</p>
<ol type="1">
<li>The “friendOf” relation might evolve over time as friendship patterns change</li>
<li>The model would learn how this relation’s embedding changes over years</li>
<li>It could capture trends like increasing or decreasing friendship formation rates</li>
<li>Predictions would be conditioned on the specific time period of interest</li>
</ol>
</div>
</section>
</section>
<section id="joint-learning-approaches" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="joint-learning-approaches"><span class="header-section-number">10.9</span> Joint Learning Approaches</h2>
<p>Joint learning approaches integrate multiple types of additional information simultaneously.</p>
<section id="multi-view-learning" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="multi-view-learning"><span class="header-section-number">10.9.1</span> Multi-view Learning</h3>
<p>Multi-view learning treats different information sources as complementary views of the same entities and relations.</p>
<div id="def-multi-view-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.20 (Multi-view Learning for KGEs)</strong></span> Multi-view learning approaches typically:</p>
<ol type="1">
<li>Process each information source with specialized encoders</li>
<li>Align the resulting embeddings in a shared space</li>
<li>Use view-specific and cross-view objectives</li>
<li>Learn representations that capture complementary aspects from each view</li>
</ol>
</div>
<p>These approaches provide a flexible framework for integrating diverse information sources.</p>
<div id="exm-multi-view" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.20 (Example of Multi-view Learning)</strong></span> A multi-view model might integrate:</p>
<ol type="1">
<li>Graph structure view (captured via traditional KGE methods)</li>
<li>Textual view (entity descriptions and relation phrases)</li>
<li>Type hierarchy view (ontological information)</li>
<li>Numerical attribute view (quantitative properties)</li>
</ol>
<p>The resulting embeddings would benefit from the complementary information in each view.</p>
</div>
</section>
<section id="attention-based-fusion" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="attention-based-fusion"><span class="header-section-number">10.9.2</span> Attention-Based Fusion</h3>
<p>Attention-based fusion approaches use attention mechanisms to selectively combine information from different sources.</p>
<div id="def-attention-fusion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.21 (Attention-Based Fusion)</strong></span> Attention fusion approaches typically:</p>
<ol type="1">
<li>Compute attention weights for different information sources</li>
<li>Dynamically adjust the importance of each source based on context</li>
<li>Learn which sources are most relevant for different entities and tasks</li>
<li>Produce adaptive representations that focus on the most informative sources</li>
</ol>
</div>
<p>These approaches allow the model to focus on the most relevant information for each entity and prediction task.</p>
<div id="exm-attention-fusion" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.21 (Example of Attention-Based Fusion)</strong></span> An attention-based model might:</p>
<ol type="1">
<li>Assign high attention to textual information for a newly added entity with few connections</li>
<li>Shift attention to structural information for a well-connected entity</li>
<li>Focus on type information when predicting relations with strict type constraints</li>
<li>Adapt its attention patterns based on the specific prediction task</li>
</ol>
</div>
</section>
</section>
<section id="case-studies-and-performance-improvements" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="case-studies-and-performance-improvements"><span class="header-section-number">10.10</span> Case Studies and Performance Improvements</h2>
<p>Let’s examine several case studies that demonstrate the impact of incorporating additional information into knowledge graph embeddings.</p>
<section id="text-enhanced-embeddings" class="level3" data-number="10.10.1">
<h3 data-number="10.10.1" class="anchored" data-anchor-id="text-enhanced-embeddings"><span class="header-section-number">10.10.1</span> Text-Enhanced Embeddings</h3>
<div id="exm-text-enhanced-case" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.22 (Case Study: Text-Enhanced Embeddings)</strong></span> A study on the FB15k-237 dataset showed that:</p>
<ol type="1">
<li>Incorporating entity descriptions improved MRR by 15% overall</li>
<li>For entities with fewer than 5 connections, the improvement was over 30%</li>
<li>The model was able to make reasonable predictions for newly added entities with descriptions but no connections</li>
<li>The qualitative analysis showed that text-enhanced embeddings captured more semantic nuance</li>
</ol>
</div>
</section>
<section id="type-constrained-embeddings-1" class="level3" data-number="10.10.2">
<h3 data-number="10.10.2" class="anchored" data-anchor-id="type-constrained-embeddings-1"><span class="header-section-number">10.10.2</span> Type-Constrained Embeddings</h3>
<div id="exm-type-constrained-case" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.23 (Case Study: Type-Constrained Embeddings)</strong></span> A comparison of models with and without type constraints on the YAGO3-10 dataset revealed:</p>
<ol type="1">
<li>Hard type constraints improved Hits@10 from 54% to 62%</li>
<li>The improvement was particularly significant for relations with clear type restrictions</li>
<li>Training efficiency improved by reducing the negative sampling space</li>
<li>Error analysis showed a substantial reduction in type-incompatible predictions</li>
</ol>
</div>
</section>
<section id="multi-modal-embeddings" class="level3" data-number="10.10.3">
<h3 data-number="10.10.3" class="anchored" data-anchor-id="multi-modal-embeddings"><span class="header-section-number">10.10.3</span> Multi-modal Embeddings</h3>
<div id="exm-multimodal-case" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.24 (Case Study: Multi-modal Embeddings)</strong></span> A study on a product knowledge graph demonstrated that:</p>
<ol type="1">
<li>Incorporating product images improved product recommendation accuracy by 12%</li>
<li>The model could effectively handle cold-start products with images but few connections</li>
<li>Visual similarity complemented structural similarity for certain relation types</li>
<li>The multi-modal approach was particularly effective for visually distinctive product categories</li>
</ol>
</div>
</section>
<section id="temporal-embeddings" class="level3" data-number="10.10.4">
<h3 data-number="10.10.4" class="anchored" data-anchor-id="temporal-embeddings"><span class="header-section-number">10.10.4</span> Temporal Embeddings</h3>
<div id="exm-temporal-case" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.25 (Case Study: Temporal Embeddings)</strong></span> Research on a historical knowledge graph showed that:</p>
<ol type="1">
<li>Time-aware embeddings improved prediction accuracy for time-dependent facts by 25%</li>
<li>The model could correctly predict changes in relationships over time</li>
<li>Temporal patterns like periodicity were successfully captured</li>
<li>The approach enabled answering complex temporal queries that were impossible with static embeddings</li>
</ol>
</div>
</section>
</section>
<section id="implementation-challenges-and-solutions" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="implementation-challenges-and-solutions"><span class="header-section-number">10.11</span> Implementation Challenges and Solutions</h2>
<p>Incorporating additional information into knowledge graph embeddings presents several implementation challenges, along with corresponding solutions.</p>
<section id="heterogeneous-data-integration" class="level3" data-number="10.11.1">
<h3 data-number="10.11.1" class="anchored" data-anchor-id="heterogeneous-data-integration"><span class="header-section-number">10.11.1</span> Heterogeneous Data Integration</h3>
<div id="def-heterogeneous-integration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.22 (Heterogeneous Data Integration)</strong></span> Challenges in integrating heterogeneous data include:</p>
<ol type="1">
<li>Different data types and formats</li>
<li>Varying scales and distributions</li>
<li>Missing or incomplete information</li>
<li>Inconsistencies between information sources</li>
</ol>
<p>Solutions include:</p>
<ol type="1">
<li>Specialized encoders for each data type</li>
<li>Normalization and standardization techniques</li>
<li>Missing data imputation methods</li>
<li>Consistency enforcement mechanisms</li>
</ol>
</div>
</section>
<section id="computational-efficiency" class="level3" data-number="10.11.2">
<h3 data-number="10.11.2" class="anchored" data-anchor-id="computational-efficiency"><span class="header-section-number">10.11.2</span> Computational Efficiency</h3>
<div id="def-computational-efficiency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.23 (Computational Efficiency Challenges)</strong></span> Efficiency challenges include:</p>
<ol type="1">
<li>Increased model complexity with additional information</li>
<li>Higher memory requirements for multi-modal data</li>
<li>Longer training times for joint models</li>
<li>Scalability issues for large knowledge graphs</li>
</ol>
<p>Solutions include:</p>
<ol type="1">
<li>Parameter sharing across components</li>
<li>Progressive training strategies</li>
<li>Dimensionality reduction for high-dimensional features</li>
<li>Efficient data loading and batching techniques</li>
</ol>
</div>
</section>
<section id="overfitting-prevention" class="level3" data-number="10.11.3">
<h3 data-number="10.11.3" class="anchored" data-anchor-id="overfitting-prevention"><span class="header-section-number">10.11.3</span> Overfitting Prevention</h3>
<div id="def-overfitting-prevention" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.24 (Overfitting Prevention)</strong></span> Overfitting challenges include:</p>
<ol type="1">
<li>Increased risk of overfitting with more parameters</li>
<li>Difficulty in balancing different information sources</li>
<li>Potential for memorizing noise in additional data</li>
<li>Challenges in proper regularization</li>
</ol>
<p>Solutions include:</p>
<ol type="1">
<li>Dropout and other regularization techniques</li>
<li>Adversarial training approaches</li>
<li>Multi-task learning with auxiliary objectives</li>
<li>Cross-validation and early stopping</li>
</ol>
</div>
</section>
</section>
<section id="practical-recommendations" class="level2" data-number="10.12">
<h2 data-number="10.12" class="anchored" data-anchor-id="practical-recommendations"><span class="header-section-number">10.12</span> Practical Recommendations</h2>
<p>Based on the research and case studies, here are practical recommendations for incorporating additional information into knowledge graph embeddings:</p>
<div id="def-practical-recommendations" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.25 (Practical Recommendations)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Start with analysis</strong>: Analyze your knowledge graph to identify which types of additional information would be most beneficial</li>
<li><strong>Prioritize by impact</strong>: Incorporate information sources with the highest potential impact first</li>
<li><strong>Balance complexity</strong>: Choose integration methods that balance performance gains with computational costs</li>
<li><strong>Consider the task</strong>: Select additional information sources based on the specific downstream tasks</li>
<li><strong>Evaluate incrementally</strong>: Measure the impact of each information source individually before combining them</li>
<li><strong>Address missing data</strong>: Develop strategies for handling missing additional information</li>
<li><strong>Tune carefully</strong>: Pay special attention to hyperparameters that balance different information sources</li>
<li><strong>Validate thoroughly</strong>: Use cross-validation to ensure that improvements generalize across the graph</li>
</ol>
</div>
</section>
<section id="future-directions" class="level2" data-number="10.13">
<h2 data-number="10.13" class="anchored" data-anchor-id="future-directions"><span class="header-section-number">10.13</span> Future Directions</h2>
<p>The integration of additional information into knowledge graph embeddings continues to evolve. Here are some promising future directions:</p>
<div id="def-future-directions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.26 (Future Directions)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Large language model integration</strong>: Leveraging powerful large language models like GPT-4 or PaLM for knowledge graph completion</li>
<li><strong>Self-supervised learning</strong>: Developing self-supervised approaches that leverage unlabeled additional information</li>
<li><strong>Cross-lingual information</strong>: Incorporating information across multiple languages to create more robust embeddings</li>
<li><strong>Real-time integration</strong>: Methods for dynamically incorporating new information as it becomes available</li>
<li><strong>Multimodal reasoning</strong>: Advanced reasoning capabilities that leverage multiple information modalities</li>
<li><strong>Interpretable integration</strong>: Approaches that make the contribution of additional information sources transparent and interpretable</li>
<li><strong>Few-shot adaptation</strong>: Techniques for rapidly adapting embeddings to new domains with limited data</li>
</ol>
</div>
</section>
<section id="conclusion" class="level2" data-number="10.14">
<h2 data-number="10.14" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.14</span> Conclusion</h2>
<p>Incorporating additional information into knowledge graph embeddings can significantly enhance their quality and utility, particularly for sparse regions of the graph and complex relationships. Different types of information—textual, type-based, numerical, visual, and temporal—provide complementary signals that, when properly integrated, lead to more accurate and robust embeddings.</p>
<p>The approaches discussed in this chapter demonstrate that there is no one-size-fits-all solution for incorporating additional information. The optimal approach depends on the nature of the knowledge graph, the available additional information, the downstream tasks, and the computational constraints. By understanding the strengths and limitations of different integration methods, practitioners can make informed decisions about how to enhance their knowledge graph embeddings with auxiliary data.</p>
<p>As knowledge graphs continue to grow in size and importance across various domains, the ability to effectively leverage all available information sources will become increasingly critical. The techniques presented in this chapter provide a foundation for building enhanced knowledge graph embeddings that capture the rich, multi-faceted nature of real-world knowledge.</p>
</section>
<section id="further-reading" class="level2" data-number="10.15">
<h2 data-number="10.15" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">10.15</span> Further Reading</h2>
<ol type="1">
<li>Wang, Q., Mao, Z., Wang, B., &amp; Guo, L. (2017). Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.</li>
<li>Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL).</li>
<li>Xie, R., Liu, Z., Jia, J., Luan, H., &amp; Sun, M. (2016). Representation learning of knowledge graphs with entity descriptions. In Proceedings of the AAAI Conference on Artificial Intelligence.</li>
<li>Xiao, H., Huang, M., Meng, L., &amp; Zhu, X. (2017). SSP: Semantic space projection for knowledge graph embedding with text descriptions. In Proceedings of the AAAI Conference on Artificial Intelligence.</li>
<li>Moon, C., Jones, P., &amp; Samatova, N. F. (2017). Learning entity type embeddings for knowledge graph completion. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</li>
<li>García-Durán, A., &amp; Niepert, M. (2018). KBlrn: End-to-end learning of knowledge base representations with latent, relational, and numerical features. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI).</li>
<li>Dasgupta, S. S., Ray, S. N., &amp; Talukdar, P. (2018). HyTE: Hyperplane-based temporally aware knowledge graph embedding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</li>
<li>Yasunaga, M., Leskovec, J., &amp; Liang, P. (2022). LinkBERT: Pretraining language models with document links. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL).</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../content/evaluation.html" class="pagination-link" aria-label="Evaluation Methodologies and Benchmarks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Methodologies and Benchmarks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/reasoning.html" class="pagination-link" aria-label="Reasoning with Knowledge Graph Embeddings">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Reasoning with Knowledge Graph Embeddings</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://tuedsci.github.io/">© Tue Nguyen</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>