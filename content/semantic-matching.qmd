# Semantic Matching Models

In the previous chapter, we explored translation-based models that interpret relations as geometric displacements in the embedding space. While these models provide an intuitive geometric interpretation, they have limitations in modeling certain relation patterns. Semantic matching models take a different approach, focusing on measuring the plausibility of facts through similarity functions that capture the semantic relationships between entities.

Semantic matching models represent a diverse family of knowledge graph embedding approaches that evaluate the compatibility between entities based on their representations and relation-specific parameters. Instead of enforcing a rigid geometric constraint like translation, these models learn more flexible similarity functions that can capture complex relation patterns. This paradigm includes influential models like RESCAL, DistMult, ComplEx, and others that have achieved state-of-the-art performance on knowledge graph completion tasks.

In this chapter, we'll delve into the foundations of semantic matching models, exploring their mathematical formulations, geometric interpretations, and practical applications. We'll examine how these models evolved to address the limitations of translation-based approaches and how they capture different relation patterns. By the end of this chapter, you'll understand the principles behind semantic matching models and their strengths and limitations compared to translation-based approaches.

## The semantic matching principle

Unlike translation-based models that enforce a specific geometric relationship between head and tail entities, semantic matching models focus on measuring the compatibility or similarity between entities in the context of a specific relation:

::: {#def-semantic-matching}

## Semantic matching principle

In **semantic matching models**, the plausibility of a triple $(h, r, t)$ is measured using a scoring function that evaluates the compatibility between the head entity $h$ and tail entity $t$ in the context of relation $r$. This compatibility is typically expressed as a similarity or matching function:

$$f_r(h, t) = sim_r(\mathbf{h}, \mathbf{t})$$

where $sim_r$ is a relation-specific similarity function, and $\mathbf{h}, \mathbf{t}$ are the embedding vectors of entities $h$ and $t$, respectively.

Higher scores indicate more plausible triples.

:::

The key differences between semantic matching models and translation-based models are:

1. **Objective**: Semantic matching models focus on matching or similarity rather than geometric transformations
2. **Direction**: Many semantic matching models are inherently symmetric, though asymmetric variants exist
3. **Interpretation**: The scoring function measures compatibility rather than distance

::: {#exm-semantic-matching}

## Semantic matching example

Consider a knowledge graph about movies, with entities like "The Godfather," "Francis Ford Coppola," and "Crime" and relations like "directed_by" and `"has_genre"`

In a semantic matching model:

- The plausibility of (The_Godfather, directed_by, Francis_Ford_Coppola) would be measured by how well the embeddings of "The Godfather" and "Francis Ford Coppola" match in the context of the "directed_by" relation.
- Similarly, the plausibility of (The_Godfather, has_genre, Crime) would be measured by how well the embeddings of "The Godfather" and "Crime" match in the context of the `"has_genre"` relation.

These matchings are learned from data rather than enforced through a geometric constraint like translation.

:::

## RESCAL: a tensor factorization approach

RESCAL, introduced by Nickel et al. (2011), was one of the first semantic matching models for knowledge graph embeddings. It approaches the problem from a tensor factorization perspective, where the knowledge graph is represented as a third-order tensor.

::: {#def-rescal}

## RESCAL model

In the **RESCAL** model, entities are represented as vectors $\mathbf{e} \in \mathbb{R}^d$, and each relation $r$ is represented as a matrix $\mathbf{W}_r \in \mathbb{R}^{d \times d}$.

The scoring function is a bilinear form: $$f_r(h, t) = \mathbf{h}^T \mathbf{W}_r \mathbf{t}$$

This can be viewed as a tensor decomposition of the knowledge graph tensor $\mathcal{X} \in \mathbb{R}^{|E| \times |R| \times |E|}$, where $\mathcal{X}_{hrt} = 1$ if the triple $(h, r, t)$ exists, and 0 otherwise.

:::

### Tensor factorization interpretation

RESCAL can be viewed as a tensor factorization approach:

::: {#def-tensor-factorization}

## Tensor factorization

A **tensor factorization** decomposes a tensor into lower-dimensional factors. For a third-order tensor $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$, the RESCAL decomposition is:

$$\mathcal{X}_{ijk} \approx \sum_{f=1}^{F} a_{if} b_{jf} c_{kf}$$

In the context of knowledge graphs, this becomes: $$\mathcal{X}_{hrt} \approx \mathbf{h}^T \mathbf{W}_r \mathbf{t}$$

where the entity embedding matrix $\mathbf{E} \in \mathbb{R}^{|E| \times d}$ contains the entity embeddings, and the relation matrices $\mathbf{W}_r \in \mathbb{R}^{d \times d}$ capture relation-specific interactions.

:::

### Geometric interpretation

The RESCAL model has a rich geometric interpretation:

::: {#def-rescal-geometry}

## RESCAL geometric interpretation

In RESCAL, the relation matrix $\mathbf{W}_r$ can be decomposed using eigendecomposition: $$\mathbf{W}_r = \mathbf{V}_r \mathbf{\Lambda}_r \mathbf{V}_r^{-1}$$

where $\mathbf{\Lambda}_r$ is a diagonal matrix of eigenvalues and $\mathbf{V}_r$ contains the eigenvectors.

This decomposition reveals that $\mathbf{W}_r$ represents a linear transformation in the embedding space, which can include:

1. **Scaling**: Changing the magnitude along certain directions
2. **Rotation**: Changing the orientation
3. **Reflection**: Flipping across hyperplanes
4. **Shearing**: Deforming the space along specific directions

:::

::: {#exm-rescal-geometry}

## RESCAL geometric example

Consider a relation `"is_parent_of"` with a relation matrix $\mathbf{W}_{\text{is\_parent\_of}}$. This matrix might encode:

1. **Directionality**: Parents and children are in different regions of the embedding space
2. **Age correlation**: The transformation captures the age difference between parents and children
3. **Genetic similarity**: The transformation preserves certain dimensions that represent genetic traits

When we compute $\mathbf{h}^T \mathbf{W}_{\text{is\_parent\_of}} \mathbf{t}$, we're measuring how well the head entity $h$ matches the pattern of being a parent to the tail entity $t$, based on these learned transformations.

:::

### Learning algorithm

RESCAL is typically trained using alternating least squares (ALS) or stochastic gradient descent (SGD) methods:

::: {#def-rescal-learning}

## RESCAL learning

**Alternating Least Squares (ALS) approach**:

1. Fix entity embeddings $\mathbf{E}$ and optimize relation matrices $\mathbf{W}_r$
2. Fix relation matrices $\mathbf{W}_r$ and optimize entity embeddings $\mathbf{E}$
3. Repeat until convergence

**Gradient-based approach**:

1. Initialize entity embeddings $\mathbf{E}$ and relation matrices $\mathbf{W}_r$
2. Define a loss function, typically margin-based ranking loss: $$L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma - f_r(h, t) + f_r(h', t')]_+$$
3. Update parameters using gradient descent
4. Apply regularization to prevent overfitting: $$L_{\text{reg}} = L + \lambda_{\mathbf{E}} \|\mathbf{E}\|_F^2 + \lambda_{\mathbf{W}} \sum_r \|\mathbf{W}_r\|_F^2$$ where $\|\cdot\|_F$ denotes the Frobenius norm

:::

### Strengths and limitations of RESCAL

RESCAL has several strengths:

1. **Expressiveness**: The bilinear form can capture complex interaction patterns between entities
2. **Interpretability**: The relation matrices have a clear geometric interpretation as linear transformations
3. **Flexibility**: RESCAL can model various relation patterns, including symmetry and antisymmetry

However, RESCAL also has important limitations:

1. **Quadratic complexity**: With $O(|E|d + |R|d^2)$ parameters, RESCAL is computationally expensive for large knowledge graphs or high embedding dimensions
2. **Overfitting risk**: The high number of parameters can lead to overfitting, especially for relations with few training examples
3. **Training difficulty**: The optimization problem is more complex than for simpler models like TransE

::: {#exm-rescal-complexity}

## RESCAL complexity example

Consider a knowledge graph with 100,000 entities, 1,000 relations, and an embedding dimension of 100:

- Entity embeddings: 100,000 × 100 = 10,000,000 parameters
- Relation matrices: 1,000 × 100 × 100 = 10,000,000 parameters
- Total: 20,000,000 parameters

This high parameter count can lead to overfitting and computational challenges, especially for large knowledge graphs.

:::

## DistMult: a simplified bilinear model

DistMult, proposed by Yang et al. (2015), simplifies RESCAL by restricting the relation matrices to be diagonal, significantly reducing the number of parameters while maintaining reasonable expressiveness.

::: {#def-distmult}

## DistMult model

In the **DistMult** model, entities are represented as vectors $\mathbf{e} \in \mathbb{R}^d$, and each relation $r$ is represented as a vector $\mathbf{r} \in \mathbb{R}^d$ that corresponds to the diagonal of a diagonal matrix.

The scoring function is: $$f_r(h, t) = \mathbf{h}^T \text{diag}(\mathbf{r}) \mathbf{t} = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i$$

where $\text{diag}(\mathbf{r})$ is a diagonal matrix with the elements of $\mathbf{r}$ on its diagonal.

:::

### Geometric interpretation

DistMult has a simpler geometric interpretation than RESCAL:

::: {#def-distmult-geometry}

## DistMult geometric interpretation

In DistMult, the diagonal relation matrix $\text{diag}(\mathbf{r})$ represents a dimension-wise scaling of the embedding space. Each component $r_i$ determines the importance of dimension $i$ for relation $r$.

The scoring function can be viewed as a weighted element-wise product: $$f_r(h, t) = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i = \langle \mathbf{h} \odot \mathbf{r}, \mathbf{t} \rangle$$

where $\odot$ denotes the element-wise (Hadamard) product, and $\langle \cdot, \cdot \rangle$ is the dot product.

:::

::: {#exm-distmult-geometry}

## DistMult geometric example

Consider a relation `"has_profession"` with relation vector $\mathbf{r}_{\text{has\_profession}} = [0.9, 0.1, 0.7, 0.2, ...]$.

The high values (0.9, 0.7) indicate dimensions that are important for the `"has_profession"` relation, while low values (0.1, 0.2) indicate dimensions that are less relevant.

When computing the score of (Person, has_profession, Doctor), the model emphasizes dimensions where Person and Doctor have similar values and where the relation vector has high values.

:::

### Strengths and limitations of DistMult

DistMult offers several advantages over RESCAL:

1. **Parameter efficiency**: With $O(|E|d + |R|d)$ parameters, DistMult is much more efficient than RESCAL
2. **Simplicity**: The simpler model is easier to train and less prone to overfitting
3. **Interpretability**: The relation vectors directly indicate the importance of each dimension for a relation

However, DistMult has a critical limitation:

1. **Symmetry constraint**: The scoring function is symmetric: $f_r(h, t) = f_r(t, h)$, which means DistMult cannot distinguish between triples $(h, r, t)$ and $(t, r, h)$. This is problematic for antisymmetric relations, which are common in knowledge graphs.

::: {#exm-distmult-symmetry}

## DistMult symmetry limitation example

Consider the relation `"is_capital_of"` and the triple (Paris, is_capital_of, France).

Since DistMult's scoring function is symmetric, it assigns the same score to:

- (Paris, is_capital_of, France)
- (France, is_capital_of, Paris)

However, while the first triple is correct, the second is incorrect. DistMult cannot distinguish between these cases because of its inherent symmetry.

:::

## Canonical Polyadic (CP) decomposition

The Canonical Polyadic (CP) decomposition is another tensor factorization approach that has been applied to knowledge graph embedding.

::: {#def-cp-decomposition}

## CP decomposition model

In the **CP decomposition** model, each entity and relation is represented by multiple vectors:

1. A head entity $h$ is represented by $\mathbf{h}_{\text{head}} \in \mathbb{R}^d$
2. A tail entity $t$ is represented by $\mathbf{t}_{\text{tail}} \in \mathbb{R}^d$
3. A relation $r$ is represented by $\mathbf{r} \in \mathbb{R}^d$

The scoring function is: $$f_r(h, t) = \sum_{i=1}^d \mathbf{h}_{\text{head},i} \mathbf{r}_i \mathbf{t}_{\text{tail},i}$$

This can be viewed as a tensor factorization where the knowledge graph tensor $\mathcal{X}$ is decomposed into three factor matrices.

:::

The key difference between CP decomposition and DistMult is that CP uses different representations for entities depending on whether they appear as head or tail, which allows it to model asymmetric relations.

::: {#exm-cp-decomposition}

## CP decomposition example

Consider the relation `"is_capital_of"` and the triples:

- (Paris, is_capital_of, France)
- (Berlin, is_capital_of, Germany)

In CP decomposition, the entities would have separate head and tail embeddings:

- $\mathbf{Paris}_{\text{head}}$ and $\mathbf{Paris}_{\text{tail}}$
- $\mathbf{France}_{\text{head}}$ and $\mathbf{France}_{\text{tail}}$
- $\mathbf{Berlin}_{\text{head}}$ and $\mathbf{Berlin}_{\text{tail}}$
- $\mathbf{Germany}_{\text{head}}$ and $\mathbf{Germany}_{\text{tail}}$

The scores would be computed as: $f_{\text{is\_capital\_of}}(\text{Paris}, \text{France}) = \sum_{i=1}^d \mathbf{Paris}_{\text{head},i} \cdot \mathbf{is\_capital\_of}_i \cdot \mathbf{France}_{\text{tail},i}$

$f_{\text{is\_capital\_of}}(\text{France}, \text{Paris}) = \sum_{i=1}^d \mathbf{France}_{\text{head},i} \cdot \mathbf{is\_capital\_of}_i \cdot \mathbf{Paris}_{\text{tail},i}$

Since the head and tail embeddings are different, these scores can be different, allowing the model to capture asymmetric relations.

:::

## ComplEx: complex embeddings for asymmetric relations

ComplEx, proposed by Trouillon et al. (2016), addresses the symmetry limitation of DistMult by using complex-valued embeddings.

::: {#def-complex}

## ComplEx model

In the **ComplEx** model, entities and relations are embedded in complex space: $\mathbf{e}, \mathbf{r} \in \mathbb{C}^d$.

The scoring function is: $$f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \text{Re}\left( \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \overline{\mathbf{t}_i} \right)$$

where $\overline{\mathbf{t}}$ denotes the complex conjugate of $\mathbf{t}$, and $\text{Re}(z)$ is the real part of the complex number $z$.

:::

### Complex number representation

In ComplEx, each embedding component is a complex number with real and imaginary parts:

::: {#def-complex-numbers}

## Complex numbers

A **complex number** $z = a + bi$ consists of:

- A real part $a \in \mathbb{R}$
- An imaginary part $b \in \mathbb{R}$
- The imaginary unit $i = \sqrt{-1}$

Key operations include:

1. **Complex conjugate**: $\overline{z} = a - bi$
2. **Modulus**: $|z| = \sqrt{a^2 + b^2}$
3. **Multiplication**: $(a + bi)(c + di) = (ac - bd) + (ad + bc)i$

:::

### Geometric interpretation

ComplEx has a rich geometric interpretation in terms of rotations and reflections in the complex plane:

::: {#def-complex-geometry}

## ComplEx geometric interpretation

In ComplEx, the scoring function can be rewritten as: $$f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \langle \text{Re}(\mathbf{h} \odot \mathbf{r}), \text{Re}(\mathbf{t}) \rangle + \langle \text{Im}(\mathbf{h} \odot \mathbf{r}), \text{Im}(\mathbf{t}) \rangle$$

This reveals that ComplEx can be viewed as two separate bilinear forms in the real and imaginary parts, allowing it to model both symmetric and antisymmetric components of relations.

The asymmetry in ComplEx comes from the use of the complex conjugate of the tail entity, which introduces an asymmetry in the scoring function: $f_r(h, t) \neq f_r(t, h)$ in general.

:::

::: {#exm-complex-geometry}

## ComplEx geometric example

Consider a relation `"is_part_of"` with a complex embedding $\mathbf{r}_{\text{is\_part\_of}} = [0.7 + 0.3i, 0.1 - 0.9i, ...]$.

For symmetric parts of the relation, the imaginary components would be close to zero, while for antisymmetric parts, the imaginary components would have larger magnitudes.

When computing the score of `(Hand, is_part_of, Body)`, the complex conjugate of the tail entity introduces asymmetry:

$$
f_{\text{is_part_of}}(\text{Hand}, \text{Body}) = \text{Re}(\langle \mathbf{Hand}, \mathbf{is_part_of}, \overline{\mathbf{Body}} \rangle)
$$

This score would be different from:

$$
f_{\text{is\_part\_of}}(\text{Body}, \text{Hand}) = \text{Re}(\langle \mathbf{Body}, \mathbf{is\_part\_of}, \overline{\mathbf{Hand}} \rangle)
$$

allowing the model to distinguish between these two cases.

:::

### Connection to DistMult

ComplEx can be viewed as a generalization of DistMult that addresses its symmetry limitation:

::: {#def-complex-distmult}

## ComplEx as an extension of DistMult

If all embeddings in ComplEx are restricted to have only real parts (zero imaginary components), then: $$f_r(h, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle) = \langle \mathbf{h}, \mathbf{r}, \mathbf{t} \rangle = \sum_{i=1}^d \mathbf{h}_i \mathbf{r}_i \mathbf{t}_i$$

This is exactly the DistMult scoring function. Therefore, DistMult is a special case of ComplEx when all embeddings are real-valued.

The addition of imaginary components in ComplEx allows it to model asymmetric relations, addressing DistMult's key limitation.

:::

### Strengths and limitations of ComplEx

ComplEx offers several advantages:

1. **Expressiveness for asymmetric relations**: ComplEx can model both symmetric and antisymmetric relations effectively
2. **Parameter efficiency**: With $O(2|E|d + 2|R|d)$ parameters (considering real and imaginary parts separately), ComplEx is still relatively efficient
3. **Strong empirical performance**: ComplEx achieves state-of-the-art results on various knowledge graph completion benchmarks

However, ComplEx also has some limitations:

1. **Complex-valued computations**: Working with complex numbers adds some computational overhead
2. **Interpretability**: The complex-valued embeddings can be less intuitive to interpret than real-valued ones
3. **Limited expressiveness for certain relation patterns**: While ComplEx handles symmetry and antisymmetry well, it may struggle with other relation patterns like composition

## HolE: holographic embeddings

Holographic Embeddings (HolE), introduced by Nickel et al. (2016), use circular correlation to capture interactions between entities, inspired by holographic models of associative memory.

::: {#def-hole}

## HolE model

In the **HolE** model, entities and relations are represented as real-valued vectors $\mathbf{e}, \mathbf{r} \in \mathbb{R}^d$.

The scoring function uses circular correlation: $$f_r(h, t) = \mathbf{r}^T (\mathbf{h} \star \mathbf{t}) = \sum_{i=1}^d \mathbf{r}_i [\mathbf{h} \star \mathbf{t}]_i$$

where $\star$ is the circular correlation operation defined as: $$[\mathbf{h} \star \mathbf{t}]_k = \sum_{i=0}^{d-1} \mathbf{h}_i \mathbf{t}_{(k+i) \mod d}$$

:::

### Circular correlation

Circular correlation is a key operation in HolE:

::: {#def-circular-correlation}

## Circular correlation

The **circular correlation** between vectors $\mathbf{a}$ and $\mathbf{b}$ of dimension $d$ is defined as: $$[\mathbf{a} \star \mathbf{b}]_k = \sum_{i=0}^{d-1} \mathbf{a}_i \mathbf{b}_{(k+i) \mod d}$$

This operation can be efficiently computed using the Fast Fourier Transform (FFT): $$\mathbf{a} \star \mathbf{b} = \mathcal{F}^{-1}( \overline{\mathcal{F}(\mathbf{a})} \odot \mathcal{F}(\mathbf{b}))$$

where $\mathcal{F}$ is the Fourier transform, $\mathcal{F}^{-1}$ is the inverse Fourier transform, $\overline{\mathcal{F}(\mathbf{a})}$ is the complex conjugate of $\mathcal{F}(\mathbf{a})$, and $\odot$ is the element-wise product.

:::

### Connection to ComplEx

Interestingly, HolE and ComplEx have been shown to be mathematically equivalent under certain conditions:

::: {#def-hole-complex}

## HolE-ComplEx equivalence

Hayashi and Shimbo (2017) proved that HolE and ComplEx are mathematically equivalent when:

1. The embedding dimension in HolE is $d$
2. The embedding dimension in ComplEx is $d/2$
3. Appropriate constraints are placed on the embeddings

This means that despite their different formulations and motivations, HolE and ComplEx have the same expressive power.

:::

::: {#exm-hole}

## HolE example

Consider entity embeddings $\mathbf{h} = [0.5, 0.8, 0.1, 0.3]$ and $\mathbf{t} = [0.2, 0.7, 0.4, 0.6]$, and relation embedding $\mathbf{r} = [0.9, 0.2, 0.5, 0.3]$.

The circular correlation $\mathbf{h} \star \mathbf{t}$ would be computed as:

- $[\mathbf{h} \star \mathbf{t}]_0 = 0.5 \cdot 0.2 + 0.8 \cdot 0.7 + 0.1 \cdot 0.4 + 0.3 \cdot 0.6 = 0.1 + 0.56 + 0.04 + 0.18 = 0.88$
- $[\mathbf{h} \star \mathbf{t}]_1 = 0.5 \cdot 0.6 + 0.8 \cdot 0.2 + 0.1 \cdot 0.7 + 0.3 \cdot 0.4 = 0.3 + 0.16 + 0.07 + 0.12 = 0.65$
- $[\mathbf{h} \star \mathbf{t}]_2 = 0.5 \cdot 0.4 + 0.8 \cdot 0.6 + 0.1 \cdot 0.2 + 0.3 \cdot 0.7 = 0.2 + 0.48 + 0.02 + 0.21 = 0.91$
- $[\mathbf{h} \star \mathbf{t}]_3 = 0.5 \cdot 0.7 + 0.8 \cdot 0.4 + 0.1 \cdot 0.6 + 0.3 \cdot 0.2 = 0.35 + 0.32 + 0.06 + 0.06 = 0.79$

The final score would be: $f_r(h, t) = \mathbf{r}^T (\mathbf{h} \star \mathbf{t}) = 0.9 \cdot 0.88 + 0.2 \cdot 0.65 + 0.5 \cdot 0.91 + 0.3 \cdot 0.79 = 0.792 + 0.13 + 0.455 + 0.237 = 1.614$

:::

### Strengths and limitations of HolE

HolE offers several advantages:

1. **Asymmetric scoring function**: HolE can model both symmetric and antisymmetric relations
2. **Parameter efficiency**: HolE has the same parameter complexity as DistMult: $O(|E|d + |R|d)$
3. **Compositional representations**: The circular correlation operation captures compositional aspects of entity relationships

However, HolE also has limitations:

1. **Computational complexity**: Computing circular correlation is more expensive than simple operations like dot products
2. **Interpretability**: The circular correlation operation is less intuitive than other scoring functions
3. **Equivalence to ComplEx**: Given the mathematical equivalence to ComplEx, HolE doesn't offer additional expressiveness

## SimplE: a canonical tensor decomposition approach

SimplE, proposed by Kazemi and Poole (2018), is based on Canonical Polyadic (CP) decomposition but addresses its limitations through parameter sharing and an enhanced scoring function.

::: {#def-simple}

## SimplE model

In the **SimplE** model, each entity $e$ has two embeddings:

1. A head embedding $\mathbf{e}_h \in \mathbb{R}^d$
2. A tail embedding $\mathbf{e}_t \in \mathbb{R}^d$

Each relation $r$ also has two embeddings:

1. A forward embedding $\mathbf{r} \in \mathbb{R}^d$
2. A reverse embedding $\mathbf{r}^{-1} \in \mathbb{R}^d$

The scoring function is the average of two CP-based scores: $$f_r(h, t) = \frac{1}{2} \left( \sum_{i=1}^d \mathbf{h}_h[i] \cdot \mathbf{r}[i] \cdot \mathbf{t}_t[i] + \sum_{i=1}^d \mathbf{t}_h[i] \cdot \mathbf{r}^{-1}[i] \cdot \mathbf{h}_t[i] \right)$$

:::

### Parameter sharing and inversion relations

A key innovation in SimplE is its parameter sharing mechanism and the explicit modeling of inverse relations:

::: {#def-simple-inversion}

## SimplE parameter sharing and inversion

SimplE leverages two key ideas:

1. **Parameter sharing**: An entity's head embedding in one relation can be related to its tail embedding in another relation
2. **Explicit inverse relations**: Each relation $r$ has an explicit inverse relation $r^{-1}$

For a triple $(h, r, t)$, the model computes both:

- Forward score: $\sum_{i=1}^d \mathbf{h}_h[i] \cdot \mathbf{r}[i] \cdot \mathbf{t}_t[i]$
- Inverse score: $\sum_{i=1}^d \mathbf{t}_h[i] \cdot \mathbf{r}^{-1}[i] \cdot \mathbf{h}_t[i]$

The final score is the average of these two scores, encouraging consistency between forward and inverse relations.

:::

::: {#exm-simple}

## SimplE example

Consider the triple (France, has_capital, Paris) and its inverse (Paris, is_capital_of, France).

SimplE would compute:

1. Forward score: $\sum_{i=1}^d \mathbf{France}_h[i] \cdot \mathbf{has\_capital}[i] \cdot \mathbf{Paris}_t[i]$
2. Inverse score: $\sum_{i=1}^d \mathbf{Paris}_h[i] \cdot \mathbf{is\_capital\_of}[i] \cdot \mathbf{France}_t[i]$

By averaging these scores, SimplE encourages consistency between the forward and inverse relations, while still allowing for asymmetric scoring due to the use of different head and tail embeddings.

:::

### Strengths and limitations of SimplE

SimplE offers several advantages:

1. **Expressiveness**: SimplE can model various relation patterns, including symmetry, antisymmetry, and inversion
2. **Interpretability**: The model has a clear interpretation in terms of forward and inverse relations
3. **Parameter efficiency**: SimplE has $O(2|E|d + 2|R|d)$ parameters, which is reasonable given its expressiveness

However, SimplE also has limitations:

1. **Additional parameters**: SimplE requires more parameters than models like DistMult or TransE, potentially leading to increased overfitting risk
2. **Training complexity**: The need to maintain consistency between forward and inverse relations can complicate the training process
3. **Composition modeling**: Like other CP-based models, SimplE may struggle with modeling compositional patterns in relations

## TuckER: a tensor decomposition model

TuckER, introduced by Balažević et al. (2019), is based on Tucker decomposition, a more general form of tensor factorization than CP decomposition.

::: {#def-tucker}

## TuckER model

In the **TuckER** model:

1. Entities are represented as vectors $\mathbf{e} \in \mathbb{R}^{d_e}$
2. Relations are represented as vectors $\mathbf{r} \in \mathbb{R}^{d_r}$
3. A core tensor $\mathcal{W} \in \mathbb{R}^{d_e \times d_r \times d_e}$ captures interactions between entities and relations

The scoring function is: $$f_r(h, t) = \mathcal{W} \times_1 \mathbf{h} \times_2 \mathbf{r} \times_3 \mathbf{t}$$

where $\times_n$ denotes the n-mode product of a tensor with a vector.

This can be rewritten as: $$f_r(h, t) = \sum_{i=1}^{d_e} \sum_{j=1}^{d_r} \sum_{k=1}^{d_e} \mathcal{W}_{ijk} \cdot \mathbf{h}_i \cdot \mathbf{r}_j \cdot \mathbf{t}_k$$

:::

### Tucker decomposition

TuckER is based on the Tucker decomposition of tensors:

::: {#def-tucker-decomposition}

## Tucker decomposition

The **Tucker decomposition** factorizes a tensor $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$ into:

1. A core tensor $\mathcal{G} \in \mathbb{R}^{P \times Q \times R}$
2. Factor matrices $\mathbf{A} \in \mathbb{R}^{I \times P}$, $\mathbf{B} \in \mathbb{R}^{J \times Q}$, and $\mathbf{C} \in \mathbb{R}^{K \times R}$

The decomposition is: $$\mathcal{X}_{ijk} \approx \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R \mathcal{G}_{pqr} \cdot \mathbf{A}_{ip} \cdot \mathbf{B}_{jq} \cdot \mathbf{C}_{kr}$$

In the context of knowledge graphs, this becomes: $$\mathcal{X}_{hrt} \approx \sum_{i=1}^{d_e} \sum_{j=1}^{d_r} \sum_{k=1}^{d_e} \mathcal{W}_{ijk} \cdot \mathbf{E}_{hi} \cdot \mathbf{R}_{rj} \cdot \mathbf{E}_{tk}$$

where $\mathbf{E} \in \mathbb{R}^{|E| \times d_e}$ contains entity embeddings and $\mathbf{R} \in \mathbb{R}^{|R| \times d_r}$ contains relation embeddings.

:::

### Connections to other models

TuckER provides a unifying framework for several semantic matching models:

::: {#def-tucker-connections}

## TuckER connections to other models

1. **RESCAL** is a special case of TuckER where $d_r = d_e^2$ and the core tensor is reshaped into a set of matrices
2. **DistMult** is a special case of TuckER where $d_r = d_e$ and the core tensor is diagonal (non-zero elements only when $i = j = k$)
3. **ComplEx** is a special case of TuckER with complex-valued embeddings and specific constraints on the core tensor
4. **SimplE** can be viewed as a special case of TuckER with a specific parameterization of the core tensor

:::

::: {#exm-tucker}

## TuckER example

Consider entity embeddings $\mathbf{h} \in \mathbb{R}^3$ and $\mathbf{t} \in \mathbb{R}^3$, relation embedding $\mathbf{r} \in \mathbb{R}^2$, and a core tensor $\mathcal{W} \in \mathbb{R}^{3 \times 2 \times 3}$.

The score would be computed as: $$f_r(h, t) = \sum_{i=1}^3 \sum_{j=1}^2 \sum_{k=1}^3 \mathcal{W}_{ijk} \cdot \mathbf{h}_i \cdot \mathbf{r}_j \cdot \mathbf{t}_k$$

This allows for complex interactions between different dimensions of the entity and relation embeddings, captured by the core tensor $\mathcal{W}$.

:::

### Strengths and limitations of TuckER

TuckER offers several advantages:

1. **Expressiveness**: As a generalization of several successful models, TuckER can model various relation patterns
2. **Theoretical unification**: TuckER provides a unifying framework for understanding different semantic matching models
3. **Strong empirical performance**: TuckER achieves state-of-the-art results on various benchmarks

However, TuckER also has limitations:

1. **Parameter complexity**: The core tensor introduces $O(d_e^2 \cdot d_r)$ parameters, which can be substantial for large embedding dimensions
2. **Training complexity**: The optimization of the core tensor can be challenging
3. **Interpretability**: The core tensor's role is less intuitive than simpler scoring functions

## QuatE: quaternion embeddings

QuatE, proposed by Zhang et al. (2019), extends complex-valued embeddings to quaternions, which offer even greater expressiveness.

::: {#def-quate}

## QuatE model

In the **QuatE** model, entities and relations are represented as quaternions: $$\mathbf{e} = a + b\mathbf{i} + c\mathbf{j} + d\mathbf{k}$$ where $a, b, c, d \in \mathbb{R}$ are real numbers, and $\mathbf{i}, \mathbf{j}, \mathbf{k}$ are imaginary units with: $$\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{i}\mathbf{j}\mathbf{k} = -1$$

The scoring function is: $$f_r(h, t) = \langle \mathbf{h} \otimes \mathbf{r}, \mathbf{t} \rangle$$ where $\otimes$ is quaternion multiplication and $\langle \cdot, \cdot \rangle$ is the quaternion inner product.

:::

### Quaternion algebra

Quaternions extend complex numbers to four dimensions, providing additional expressiveness:

::: {#def-quaternions}

## Quaternion algebra

A **quaternion** $q = a + b\mathbf{i} + c\mathbf{j} + d\mathbf{k}$ consists of:

- A scalar (real) part $a \in \mathbb{R}$
- Three imaginary parts $b, c, d \in \mathbb{R}$ with imaginary units $\mathbf{i}, \mathbf{j}, \mathbf{k}$

Key operations include:

1. **Conjugate**: $\overline{q} = a - b\mathbf{i} - c\mathbf{j} - d\mathbf{k}$
2. **Norm**: $|q| = \sqrt{a^2 + b^2 + c^2 + d^2}$
3. **Multiplication**: $(a_1 + b_1\mathbf{i} + c_1\mathbf{j} + d_1\mathbf{k})(a_2 + b_2\mathbf{i} + c_2\mathbf{j} + d_2\mathbf{k}) = a_3 + b_3\mathbf{i} + c_3\mathbf{j} + d_3\mathbf{k}$ where:
   - $a_3 = a_1a_2 - b_1b_2 - c_1c_2 - d_1d_2$
   - $b_3 = a_1b_2 + b_1a_2 + c_1d_2 - d_1c_2$
   - $c_3 = a_1c_2 - b_1d_2 + c_1a_2 + d_1b_2$
   - $d_3 = a_1d_2 + b_1c_2 - c_1b_2 + d_1a_2$
4. **Inner product**: $\langle q_1, q_2 \rangle = a_1a_2 + b_1b_2 + c_1c_2 + d_1d_2$

:::

### Geometric interpretation

Quaternions have a rich geometric interpretation related to 3D rotations:

::: {#def-quate-geometry}

## QuatE geometric interpretation

In QuatE:

1. Quaternion multiplication represents rotation in 4D space
2. Pure imaginary quaternions (where the real part is zero) can represent points in 3D space
3. Unit quaternions (with norm 1) can represent 3D rotations

The scoring function measures how well the rotation of the head entity by the relation quaternion aligns with the tail entity.

:::

::: {#exm-quate}

## QuatE example

Consider quaternion embeddings:

- Head entity: $\mathbf{h} = 0.5 + 0.5\mathbf{i} + 0.5\mathbf{j} + 0.5\mathbf{k}$
- Relation: $\mathbf{r} = 0.0 + 1.0\mathbf{i} + 0.0\mathbf{j} + 0.0\mathbf{k}$ (a rotation around the i-axis)
- Tail entity: $\mathbf{t} = 0.5 + 0.5\mathbf{i} - 0.5\mathbf{j} - 0.5\mathbf{k}$

The quaternion product $\mathbf{h} \otimes \mathbf{r}$ would represent the rotation of $\mathbf{h}$ by $\mathbf{r}$, and the inner product with $\mathbf{t}$ would measure how well this rotated vector aligns with the tail entity.

This allows QuatE to model complex relation patterns through rotations in 4D space.

:::

### Strengths and limitations of QuatE

QuatE offers several advantages:

1. **Enhanced expressiveness**: Quaternions provide additional degrees of freedom compared to complex numbers
2. **Rotation modeling**: QuatE naturally models rotations, which can capture various relation patterns
3. **Strong empirical performance**: QuatE achieves state-of-the-art results on several benchmarks

However, QuatE also has limitations:

1. **Increased complexity**: Quaternion operations are more complex than real or complex operations
2. **Additional parameters**: With four components per dimension, QuatE has more parameters than real-valued models
3. **Interpretability**: Quaternion algebra and 4D rotations can be less intuitive to understand

## Extensions and variations

Several extensions and variations of semantic matching models have been proposed to address specific challenges or incorporate additional information:

### Regularization approaches

Various regularization techniques have been proposed to improve the training of semantic matching models:

::: {#def-regularization}

## Regularization techniques

1. **Nuclear 3-norm regularization** (Lacroix et al., 2018): Applies tensor nuclear norm regularization to improve generalization
2. **Adversarial regularization** (Minervini et al., 2017): Adds adversarial perturbations to embeddings during training to improve robustness
3. **Orthogonality constraints** (Sun et al., 2019): Enforces orthogonality between entity and relation embeddings to prevent overfitting

:::

### Multi-modal extensions

Several models incorporate additional modalities beyond the graph structure:

::: {#def-multimodal-extensions}

## Multi-modal extensions

1. **DKRL** (Xie et al., 2016): Incorporates textual descriptions of entities using convolutional neural networks
2. **IKRL** (Xie et al., 2017): Integrates image information for visual entities
3. **KG-BERT** (Yao et al., 2019): Leverages pre-trained language models to encode entities and relations

:::

### Type-aware models

Some models explicitly incorporate entity type information:

::: {#def-type-aware}

## Type-aware models

1. **TKRL** (Xie et al., 2016): Incorporates hierarchical type information for entities
2. **TypeComplex** (Jain et al., 2018): Extends ComplEx with entity type constraints
3. **TRESCAL** (Chang et al., 2014): Incorporates type information into the RESCAL model

:::

::: {#exm-type-aware}

## Type-aware example

Consider the entity "Paris" with type "City" and "France" with type "Country".

A type-aware model would:

1. Learn embeddings for types (City, Country)
2. Incorporate type compatibility for relations (e.g., `"is_capital_of"` connects City to Country)
3. Use type information to constrain predictions (only predict entities of the correct type)

This can significantly improve prediction accuracy by reducing the search space to type-compatible entities.

:::

## Relation patterns and model capabilities

Different semantic matching models have different capabilities for modeling relation patterns:

Model capabilities for relation patterns

| Relation Pattern | RESCAL | DistMult | ComplEx | HolE | SimplE | TuckER | QuatE |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Symmetry | Strong | Strong | Strong | Strong | Strong | Strong | Strong |
| Antisymmetry | Strong | Weak | Strong | Strong | Strong | Strong | Strong |
| Inversion | Medium | Weak | Medium | Medium | Strong | Strong | Strong |
| Composition | Strong | Medium | Medium | Medium | Medium | Strong | Strong |
| Hierarchy | Medium | Medium | Medium | Medium | Medium | Strong | Strong |

Understanding these capabilities helps in selecting the appropriate model for specific knowledge graphs based on the prevalent relation patterns.

::: {#exm-model-selection}

## Model selection example

Consider knowledge graphs with different characteristics:

1. **Biomedical knowledge graph** (many hierarchical relationships like `"is_a"` and "part_of"):

   - TuckER or QuatE might be most appropriate due to their strong hierarchical modeling capabilities

2. **Social network knowledge graph** (many symmetric relationships like `"is_friend_of"`):

   - Even simpler models like DistMult might perform well since they naturally handle symmetry

3. **General knowledge graph** (diverse relation patterns including symmetry, antisymmetry, and composition):
   - ComplEx or SimplE might offer a good balance of expressiveness and efficiency

:::

## Implementation considerations

When implementing semantic matching models, several practical considerations are important:

### Training objectives

Various training objectives can be used for semantic matching models:

::: {#def-training-objectives}

## Training objectives

1. **Margin-based ranking loss**: $$L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'_{(h,r,t)}} [\gamma + f_r(h, t) - f_r(h', t')]_+$$

2. **Binary cross-entropy loss**: $$L = \sum_{(h,r,t) \in S \cup S'} y_{hrt} \log \sigma(f_r(h, t)) + (1-y_{hrt}) \log (1-\sigma(f_r(h, t)))$$ where $y_{hrt} = 1$ for positive triples and $y_{hrt} = 0$ for negative triples

3. **Self-adversarial negative sampling** (Sun et al., 2019): $$L = -\log \sigma(\gamma - f_r(h, t)) - \sum_{(h',r,t') \in S'_{(h,r,t)}} p(h',r,t') \log \sigma(f_r(h', t') - \gamma)$$ where $p(h',r,t')$ is based on the current model scores

:::

The choice of training objective can significantly impact model performance.

### Initialization strategies

Proper initialization is crucial for effective training:

::: {#def-initialization-strategies}

## Initialization strategies for semantic matching models

1. **Uniform initialization**: Sample from a uniform distribution, e.g., $U(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})$
2. **Xavier/Glorot initialization**: Scale based on input and output dimensions
3. **Complex initialization**: For complex-valued embeddings, initialize real and imaginary parts separately
4. **Quaternion initialization**: For quaternion embeddings, initialize each component with appropriate scaling

:::

For semantic matching models, initialization can be particularly important due to the bilinear nature of many scoring functions.

### Regularization and constraints

Various regularization methods and constraints are commonly used:

::: {#def-regularization-constraints}

## Regularization and constraints

1. **L2 regularization**: Add $\lambda(\|\mathbf{E}\|_F^2 + \|\mathbf{R}\|_F^2)$ to the loss function
2. **Nuclear 3-norm regularization**: Regularize based on the tensor nuclear norm
3. **Dropout**: Apply dropout to entity and relation embeddings during training
4. **Max-norm constraints**: Enforce $\|\mathbf{e}\|_2 \leq C$ for all entity embeddings
5. **Orthogonality constraints**: Enforce orthogonality between certain embeddings

:::

Proper regularization is especially important for models with many parameters, like RESCAL and TuckER.

## Performance analysis

Let's analyze the empirical performance of semantic matching models on standard benchmark datasets:

Performance comparison (Hits@10 in %)

| Model    | FB15k | WN18 | FB15k-237 | WN18RR |
| -------- | ----- | ---- | --------- | ------ |
| RESCAL   | 58.7  | 52.8 | 35.9      | 42.8   |
| DistMult | 82.4  | 94.2 | 41.9      | 49.1   |
| ComplEx  | 84.0  | 94.7 | 42.8      | 51.0   |
| HolE     | 73.1  | 94.9 | 40.2      | 49.1   |
| SimplE   | 83.8  | 94.2 | 42.5      | 49.6   |
| TuckER   | 89.2  | 95.8 | 54.4      | 52.6   |
| QuatE    | 88.0  | 95.9 | 51.6      | 53.3   |

These results show the progression of performance as semantic matching models have evolved to become more expressive while maintaining computational efficiency.

::: {#exm-performance-analysis}

## Performance analysis example

On FB15k, we observe:

1. RESCAL achieves moderate performance (58.7%) despite its expressiveness, likely due to overfitting
2. DistMult performs surprisingly well (82.4%) despite its symmetry constraint, suggesting many relations in FB15k are effectively symmetric or can be approximately modeled as such
3. ComplEx improves over DistMult (84.0%) by addressing the symmetry limitation
4. TuckER and QuatE achieve the best performance (89.2% and 88.0%) through their more expressive formulations

This progression reflects the trade-off between model expressiveness and overfitting risk, with the most recent models striking a better balance.

:::

## Applications of semantic matching models

Semantic matching models have been applied to various domains beyond link prediction:

### Question answering

Knowledge graph embeddings can support question answering systems:

::: {#def-qa-application}

## Knowledge graph-based question answering

Semantic matching models support question answering by:

1. Mapping questions to entities and relations in the knowledge graph
2. Using embedding scores to rank candidate answers
3. Providing confidence scores for different answers

:::

::: {#exm-qa}

## Question answering example

For the question "Who wrote the novel 1984?", a system might:

1. Identify "1984" as an entity and "wrote" as a relation
2. Use a semantic matching model to score all entities $e$ for the triple $(e, \text{wrote}, \text{1984})$
3. Return "George Orwell" as the highest-scoring entity

:::

### Recommendation systems

Semantic matching models can enhance recommendation systems:

::: {#def-recommendation-application}

## Knowledge graph-based recommendation

Semantic matching models support recommendations by:

1. Representing users and items as entities in a knowledge graph
2. Modeling user preferences as relations (e.g., "likes", "purchased")
3. Using embedding scores to generate personalized recommendations

:::

::: {#exm-recommendation}

## Recommendation system example

In a movie recommendation system:

1. Users and movies are represented as entities
2. Relations include "watched", "rated", "liked"
3. A semantic matching model can predict scores for unseen user-movie pairs
4. Movies with high predicted scores for "likes" relation are recommended to users

:::

### Information extraction

Semantic matching models can assist in information extraction:

::: {#def-ie-application}

## Knowledge graph-based information extraction

Semantic matching models support information extraction by:

1. Scoring candidate triples extracted from text
2. Filtering out unlikely triples based on embedding scores
3. Integrating new knowledge into the knowledge graph

:::

::: {#exm-ie}

## Information extraction example

Given the text "Apple released the iPhone 14 in September 2022":

1. An information extraction system extracts the triple (Apple, released, iPhone_14)
2. A semantic matching model scores this triple against existing knowledge
3. If the score is high enough, the triple is added to the knowledge graph

:::

## Future directions

Semantic matching models continue to evolve, with several promising research directions:

### Neural architecture integration

Integrating semantic matching models with neural architectures can enhance their expressiveness:

::: {#def-neural-integration}

## Neural-enhanced semantic matching

**Neural-enhanced semantic matching models** combine bilinear scoring functions with neural network components:

1. **ConvE** (Dettmers et al., 2018): Uses convolutional neural networks over reshaped embeddings
2. **ConvKB** (Nguyen et al., 2018): Applies convolutions over entity-relation-entity triples
3. **CapsE** (Nguyen et al., 2019): Utilizes capsule networks for knowledge graph embedding

:::

These models aim to capture more complex interaction patterns while maintaining computational efficiency.

### Pre-trained language model integration

Leveraging pre-trained language models can enhance semantic matching models:

::: {#def-plm-integration}

## Pre-trained language model integration

Recent approaches integrate pre-trained language models with knowledge graph embeddings:

1. **KG-BERT** (Yao et al., 2019): Encodes triples using BERT for scoring
2. **KEPLER** (Wang et al., 2021): Jointly trains knowledge embeddings and language representations
3. **KnowledgeBART** (Liu et al., 2022): Fine-tunes BART for knowledge-aware generation

:::

These approaches leverage the semantic understanding captured by pre-trained language models to enhance knowledge graph representations.

### Inductive learning

Enabling inductive learning is another important research direction:

::: {#def-inductive-semantic-matching}

## Inductive semantic matching models

**Inductive semantic matching models** can handle previously unseen entities:

1. **DRUM** (Sadeghian et al., 2019): Uses entity descriptions for inductive learning
2. **GraIL** (Teru et al., 2020): Employs local subgraph patterns for inductive reasoning
3. **NBFNet** (Zhu et al., 2021): Leverages neural bellman-ford networks for inductive relation prediction

:::

These models address a key limitation of traditional knowledge graph embeddings, which are transductive in nature.

## Summary

In this chapter, we've explored semantic matching models for knowledge graph embeddings, which focus on measuring the compatibility between entities based on their embeddings and relation-specific parameters.

We started with RESCAL, a tensor factorization approach that represents relations as matrices, capturing complex interactions between entities. We then examined DistMult, which simplifies RESCAL by restricting relation matrices to be diagonal, significantly reducing the number of parameters but introducing a symmetry constraint.

To address the symmetry limitation, we explored ComplEx, which extends DistMult to the complex domain, allowing it to model both symmetric and antisymmetric relations effectively. We also discussed HolE, which uses circular correlation to capture interactions between entities, and SimplE, which leverages parameter sharing and explicit inverse relations.

More recent models like TuckER and QuatE push the boundaries of expressiveness, with TuckER providing a unifying framework for various semantic matching models and QuatE extending embeddings to quaternions for enhanced representation power.

Throughout the chapter, we analyzed the strengths and limitations of each model, examined their capabilities for modeling different relation patterns, and discussed practical implementation considerations. We also explored applications of semantic matching models beyond link prediction, including question answering, recommendation systems, and information extraction.

Semantic matching models represent a powerful approach to knowledge graph embeddings, offering a balance of expressiveness, interpretability, and computational efficiency. By understanding the principles behind these models, you can select the appropriate approach for your specific knowledge graph and application requirements.

## Further reading

### Original papers

- Nickel, M., Tresp, V., & Kriegel, H. P. (2011). A Three-Way Model for Collective Learning on Multi-Relational Data. In Proceedings of the 28th International Conference on Machine Learning (pp. 809-816).
- Yang, B., Yih, W. T., He, X., Gao, J., & Deng, L. (2015). Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In International Conference on Learning Representations.
- Trouillon, T., Welbl, J., Riedel, S., Gaussier, É., & Bouchard, G. (2016). Complex Embeddings for Simple Link Prediction. In Proceedings of the 33rd International Conference on Machine Learning (pp. 2071-2080).
- Nickel, M., Rosasco, L., & Poggio, T. (2016). Holographic Embeddings of Knowledge Graphs. In AAAI Conference on Artificial Intelligence (pp. 1955-1961).
- Kazemi, S. M., & Poole, D. (2018). SimplE Embedding for Link Prediction in Knowledge Graphs. In Advances in Neural Information Processing Systems (pp. 4284-4295).
- Balažević, I., Allen, C., & Hospedales, T. M. (2019). TuckER: Tensor Factorization for Knowledge Graph Completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (pp. 5185-5194).
- Zhang, S., Tay, Y., Yao, L., & Liu, Q. (2019). Quaternion Knowledge Graph Embeddings. In Advances in Neural Information Processing Systems (pp. 2731-2741).

### Surveys and comparative analyses

- Nickel, M., Murphy, K., Tresp, V., & Gabrilovich, E. (2016). A Review of Relational Machine Learning for Knowledge Graphs. Proceedings of the IEEE, 104(1), 11-33.
- Wang, Q., Mao, Z., Wang, B., & Guo, L. (2017). Knowledge Graph Embedding: A Survey of Approaches and Applications. IEEE Transactions on Knowledge and Data Engineering, 29(12), 2724-2743.
- Rossi, A., Barbosa, D., Firmani, D., Matinata, A., & Merialdo, P. (2021). Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. ACM Transactions on Knowledge Discovery from Data, 15(2), 1-49.

### Mathematical foundations

- Kolda, T. G., & Bader, B. W. (2009). Tensor Decompositions and Applications. SIAM Review, 51(3), 455-500.
- de Lacerda, G., Hayashi, K., & Shimbo, M. (2021). On the Equivalence Between Holographic and Complex Embeddings for Link Prediction. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 6, pp. 4580-4588).
- Kadlcik, T., & Frolov, A. A. (2021). Understanding the Theoretical Relationships Between Semantic Knowledge Graph Embedding Models. IEEE Access, 9, 91340-91362.
